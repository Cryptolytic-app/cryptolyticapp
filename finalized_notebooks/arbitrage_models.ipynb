{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==0.21.3 in /anaconda3/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from scikit-learn==0.21.3) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ta==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background on Arbitrage Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrage models were created with the goal of predicting arbitrage 10 min before it happens in an active crypto market. The models are generated by getting all of the combinations of 2 exchanges that support the same trading pair, engineering technical analysis features, merging that data on 'closing_time', engineering more features, and creating a target that signals an arbitrage opportunity. A valid arbitrage signal is when the arbitrage lasts >30 mins because it takes time to move coins from one exchange to the other in order to successfully complete the arbitrage trades.\n",
    "\n",
    "The models predict whether there will be an arbitrage opportunity that starts 10 mins after the prediction time and lasts for at least 30 mins, giving a user enough times to execute trades.\n",
    "\n",
    "More than 6000+ iterations of models were generated in this notebook and the best ones were selected from each possible arbitrage combination based on model selection criteria outlined later in this section. The models were Random Forest Classifier and the best model parameters varied for each dataset. The data was obtained from the respective exchanges via their api, and we did a 70/30 train/test split on 5 min candlestick data that fell anywhere in the range from Jun 2015 - Oct 2019. There was a 2 week gap left between the train and test sets to prevent data leakage. The models return 0 (no arbitrage), 1 (arbitrage from exchange 1 to exchange 2) and -1 (arbitrage from exchange 2 to exchange 1). \n",
    "\n",
    "The profit calculation incorporated fees like in the real world. We used mean percent profit as the profitability metric which represented the average percent profit per arbitrage trade if one were to act on all trades predicted by the model in the testing period, whether those predictions were correct or not.\n",
    "\n",
    "From the 6000+ iterations of models trained, the best models were narrowed down based on the following criteria:\n",
    "- How often the models predicted arbitrage when it didn't exist (False positives)\n",
    "- How many times the models predicted arbitrage correctly (True positives)\n",
    "- How profitable the model was in the real world over the period of the test set. \n",
    "\n",
    "There were 21 models that met the thresholds for model selection critera (details of these models can be found at the end of this nb). The final models were all profitable with gains anywhere from 0.2% - 2.3% within the varied testing time periods (Note: the model with >9% mean percent profit was an outlier). Visualizations for how these models performed can be viewed at https://github.com/Lambda-School-Labs/cryptolytic-ds/blob/master/finalized_notebooks/visualization/arb_performance_visualization.ipynb\n",
    "\n",
    "\\* It is HIGHLY recommended to run this on sagemaker and split the training work onto 4 notebooks. These functions will take over a day to run if not split up. There are 95 total options for models, 75 of those options have enough data to train models, and with different options for parameters around ~7K models will be trained. After selecting for the best models, there were 21 good ones that were included in this project.\n",
    "\n",
    "** Feature engineering takes a LONG time. We export data as csvs along each step to not have to re-engineer features everytime the runtime restarts: export after technical analysis features are added, and export after datasets are merged. When iterating on this work, you should first settle on any new features you want to add, create those datasets, and then move onto modeling. Try not to go back and forth with feature engineering and modeling, it will be too time consuming and inefficient.\n",
    "\n",
    "\\*** There has been some feature selection done in this process where we removed highly correlated features, but not enough. There should be more exploration into whether removing features improves accuracy. \n",
    "\n",
    "\\**** We haven't tried normalizing the dataset to see if it will improve accuracy, but that should be a top priority to anyone continuing this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Folder organization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "├── arbitrage/                        <-- The top-level directory for all arbitrage work\n",
    "│   ├── arbitrage_models.ipynb        <-- notebook for arbitrage models\n",
    "│   ├── all_data/                     <-- Directory with subdirectories containing 5 min candle data\n",
    "│   │      ├──bitfinex_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      ├──coinbase_pro_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      ├──gemini_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      ├──hitbtc_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      └──kraken_300/\n",
    "│   │             └── data.csv\n",
    "│   ├── data/                         <-- Directory for csv files of all 5 min candle data\n",
    "│   │     └── data.csv                \n",
    "│   ├── ta_data/                      <-- Directory for csv files of data after ta features engineered\n",
    "│   │     └── data.csv                \n",
    "│   ├── arb_data/                     <-- Directory for csv files of final arbitrage training data\n",
    "│   │     └── data.csv               \n",
    "│   ├── pickles/                      <-- Directory for all pickle models\n",
    "│   │     └── models.pkl              \n",
    "│   ├── arbitrage_pickles             <-- Directory for final models after model selection\n",
    "│   │     └── models.pkl              \n",
    "│   │\n",
    "│   ├── cm/                           <-- Directory for confusion matrices after training models\n",
    "│   │\n",
    "│   ├── model_perf/                   <-- Directory for performance csvs after training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The folders /all_data and /data technically have the same exact data, the only difference is that one is structured. It doesn't make sense that there are two folders with the same exact data, but a function was written to get the combinations for arbitrage with the subdirectories so it's necessary for now until the function is rewritten ¯\\\\\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of the 5 min candle data filepaths into a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepaths = glob.glob('data/*.csv')\n",
    "len(csv_filepaths) #80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get all combinations of exchanges with the same trading pair\n",
    "\n",
    "Follow instructions on folder organization above in order for this to run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five supported exchanges\n",
    "exchanges = ['bitfinex', 'coinbase_pro', 'gemini', 'hitbtc', 'kraken']\n",
    "\n",
    "# function to create pairs for arbitrage datasets\n",
    "def get_file_pairs(exchanges):\n",
    "    \"\"\"This function takes in a list of exchanges and looks through data\n",
    "        directories to find all possible combinations for 2 exchanges\n",
    "        with the same trading pair. Returns a list of all lists that\n",
    "        include the file pairs\"\"\"\n",
    "    \n",
    "    # list for filenames of ohlcv csvs\n",
    "    filenames = []\n",
    "    \n",
    "    for directory in os.listdir('all_data'):\n",
    "        # .DS_Store files can mess things up, since they aren't directories     \n",
    "        if directory != '.DS_Store':\n",
    "            \n",
    "            # for each of the files in the subdirectory\n",
    "            for filename in os.listdir('all_data/' + directory):\n",
    "                \n",
    "                # add to list of filenames if the file is a csv\n",
    "                if filename.endswith('300.csv'):\n",
    "                    filenames.append(filename)\n",
    "                    \n",
    "    # list for pairs of csvs\n",
    "    file_pairs = []\n",
    "    \n",
    "    # compare filenames to eachother and append them in a list\n",
    "    for filename_1 in filenames:\n",
    "        # filenames we haven't looped through yet\n",
    "        remaining_filenames = filenames[filenames.index(filename_1)+1:]\n",
    "        \n",
    "        # iterate through remaining filenames\n",
    "        for filename_2 in remaining_filenames:\n",
    "            \n",
    "            # iterate through exchanges\n",
    "            for exchange in exchanges:\n",
    "                \n",
    "                # drop the exchange from the first filename and see if the\n",
    "                # remaining string is contained in the second filename\n",
    "                if filename_1.replace(exchange, '') in filename_2:\n",
    "                    \n",
    "                    # add the pair of filenames to the list of pairs\n",
    "                    file_pairs.append([filename_1, filename_2])\n",
    "                    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['bitfinex_eos_usdt_300.csv', 'hitbtc_eos_usdt_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'coinbase_pro_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'kraken_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'gemini_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['bitfinex_etc_usd_300.csv', 'coinbase_pro_etc_usd_300.csv'],\n",
       " ['bitfinex_etc_usd_300.csv', 'kraken_etc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'coinbase_pro_btc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'kraken_btc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'gemini_btc_usd_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'coinbase_pro_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'kraken_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'gemini_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['bitfinex_dash_usd_300.csv', 'coinbase_pro_dash_usd_300.csv'],\n",
       " ['bitfinex_dash_usd_300.csv', 'kraken_dash_usd_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'coinbase_pro_dash_btc_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'kraken_dash_btc_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'coinbase_pro_ltc_usd_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'kraken_ltc_usd_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'gemini_ltc_usd_300.csv'],\n",
       " ['bitfinex_bch_usdt_300.csv', 'hitbtc_bch_usdt_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'coinbase_pro_bch_usd_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'kraken_bch_usd_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'gemini_bch_usd_300.csv'],\n",
       " ['bitfinex_eos_usd_300.csv', 'coinbase_pro_eos_usd_300.csv'],\n",
       " ['bitfinex_eos_usd_300.csv', 'kraken_eos_usd_300.csv'],\n",
       " ['bitfinex_xrp_usd_300.csv', 'coinbase_pro_xrp_usd_300.csv'],\n",
       " ['bitfinex_xrp_usd_300.csv', 'kraken_xrp_usd_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'coinbase_pro_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'kraken_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'gemini_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_usdt_300.csv', 'hitbtc_eth_usdt_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'coinbase_pro_eth_usd_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'kraken_eth_usd_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'gemini_eth_usd_300.csv'],\n",
       " ['bitfinex_ltc_usdt_300.csv', 'hitbtc_ltc_usdt_300.csv'],\n",
       " ['bitfinex_zrx_usd_300.csv', 'coinbase_pro_zrx_usd_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'coinbase_pro_xrp_btc_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'kraken_xrp_btc_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['bitfinex_zec_usd_300.csv', 'kraken_zec_usd_300.csv'],\n",
       " ['bitfinex_zec_usd_300.csv', 'gemini_zec_usd_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'coinbase_pro_eos_btc_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'kraken_eos_btc_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['bitfinex_btc_usdt_300.csv', 'hitbtc_btc_usdt_300.csv'],\n",
       " ['coinbase_pro_eos_usd_300.csv', 'kraken_eos_usd_300.csv'],\n",
       " ['coinbase_pro_dash_btc_300.csv', 'kraken_dash_btc_300.csv'],\n",
       " ['coinbase_pro_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'kraken_eth_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'gemini_eth_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['coinbase_pro_xrp_usd_300.csv', 'kraken_xrp_usd_300.csv'],\n",
       " ['coinbase_pro_xrp_btc_300.csv', 'kraken_xrp_btc_300.csv'],\n",
       " ['coinbase_pro_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['coinbase_pro_eth_usd_300.csv', 'kraken_eth_usd_300.csv'],\n",
       " ['coinbase_pro_eth_usd_300.csv', 'gemini_eth_usd_300.csv'],\n",
       " ['coinbase_pro_dash_usd_300.csv', 'kraken_dash_usd_300.csv'],\n",
       " ['coinbase_pro_eos_btc_300.csv', 'kraken_eos_btc_300.csv'],\n",
       " ['coinbase_pro_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['coinbase_pro_eth_usdc_300.csv', 'hitbtc_eth_usdc_300.csv'],\n",
       " ['coinbase_pro_etc_usd_300.csv', 'kraken_etc_usd_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'kraken_bch_btc_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'gemini_bch_btc_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'kraken_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'gemini_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_btc_usd_300.csv', 'kraken_btc_usd_300.csv'],\n",
       " ['coinbase_pro_btc_usd_300.csv', 'gemini_btc_usd_300.csv'],\n",
       " ['coinbase_pro_btc_usdc_300.csv', 'hitbtc_btc_usdc_300.csv'],\n",
       " ['coinbase_pro_ltc_usd_300.csv', 'kraken_ltc_usd_300.csv'],\n",
       " ['coinbase_pro_ltc_usd_300.csv', 'gemini_ltc_usd_300.csv'],\n",
       " ['coinbase_pro_bch_usd_300.csv', 'kraken_bch_usd_300.csv'],\n",
       " ['coinbase_pro_bch_usd_300.csv', 'gemini_bch_usd_300.csv'],\n",
       " ['kraken_eth_btc_300.csv', 'gemini_eth_btc_300.csv'],\n",
       " ['kraken_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['kraken_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['kraken_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['kraken_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['kraken_zec_usd_300.csv', 'gemini_zec_usd_300.csv'],\n",
       " ['kraken_eth_usd_300.csv', 'gemini_eth_usd_300.csv'],\n",
       " ['kraken_btc_usd_300.csv', 'gemini_btc_usd_300.csv'],\n",
       " ['kraken_ltc_btc_300.csv', 'gemini_ltc_btc_300.csv'],\n",
       " ['kraken_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['kraken_bch_btc_300.csv', 'gemini_bch_btc_300.csv'],\n",
       " ['kraken_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['kraken_bch_usd_300.csv', 'gemini_bch_usd_300.csv'],\n",
       " ['kraken_ltc_usd_300.csv', 'gemini_ltc_usd_300.csv'],\n",
       " ['gemini_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['gemini_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['gemini_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = get_file_pairs(exchanges)\n",
    "print(len(pairs)) # 95\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHLCV Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ohlcv(df, period):\n",
    "    \"\"\" Changes the time period on cryptocurrency ohlcv data.\n",
    "        Period is a string denoted by '{time_in_minutes}T'(ex: '1T', '5T', '60T').\"\"\"\n",
    "\n",
    "    # Set date as the index. This is needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "\n",
    "    # Aggregation function\n",
    "    ohlc_dict = {'open':'first',                                                                                                    \n",
    "                 'high':'max',                                                                                                       \n",
    "                 'low':'min',                                                                                                        \n",
    "                 'close': 'last',                                                                                                    \n",
    "                 'base_volume': 'sum'}\n",
    "\n",
    "    # Apply resampling\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample_ohlcv function will create NaNs in df where there were gaps in the data.\n",
    "# The gaps could be caused by exchanges being down, errors from cryptowatch or the \n",
    "# exchanges themselves\n",
    "\n",
    "def fill_nan(df):\n",
    "    \"\"\"Iterates through a dataframe and fills NaNs with appropriate \n",
    "        open, high, low, close values.\"\"\"\n",
    "\n",
    "    # Forward fill close column.\n",
    "    df['close'] = df['close'].ffill()\n",
    "\n",
    "    # Backward fill the open, high, low rows with the close value.\n",
    "    df = df.bfill(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering - before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, period='5T'):\n",
    "    \"\"\"Takes a df, engineers ta features, and returns a df\n",
    "       default period=['5T']\"\"\"\n",
    "    \n",
    "    # convert unix closing_time to datetime\n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "    \n",
    "    # time resampling to fill gaps in data\n",
    "    df = resample_ohlcv(df, period)\n",
    "    \n",
    "    # move date off the index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # create closing_time\n",
    "    closing_time = df.date.values\n",
    "    df.drop(columns='date', inplace=True)\n",
    "    \n",
    "    # create feature to indicate where rows were gaps in data\n",
    "    df['nan_ohlcv'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    # fill gaps in data\n",
    "    df = fill_nan(df)\n",
    "\n",
    "    # adding all the technical analysis features...\n",
    "    df = add_all_ta_features(df, 'open', 'high', 'low', 'close','base_volume', fillna=True)\n",
    "    \n",
    "    # add closing time column\n",
    "    df['closing_time'] = closing_time\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - after merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_closing_price(df):\n",
    "    \"\"\"returns the exchange with the higher closing price\"\"\"\n",
    "    \n",
    "    # exchange 1 has higher closing price\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 1\n",
    "    \n",
    "    # exchange 2 has higher closing price\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 2\n",
    "    \n",
    "    # closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_pct_higher(df):\n",
    "    \"\"\"returns the percentage of the difference between ex1/ex2 \n",
    "        closing prices\"\"\"\n",
    "    \n",
    "    # if exchange 1 has a higher closing price than exchange 2\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_1'] / \n",
    "                 df['close_exchange_2'])-1)*100\n",
    "    \n",
    "    # if exchange 2 has a higher closing price than exchange 1\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_2'] / \n",
    "                 df['close_exchange_1'])-1)*100\n",
    "    \n",
    "    # if closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_arbitrage_opportunity(df):\n",
    "    \"\"\"function to create column showing available arbitrage opportunities\"\"\"\n",
    "    \n",
    "    # assuming the total fees are 0.55%, if the higher closing price is less\n",
    "    # than 0.55% higher than the lower closing price...\n",
    "    if df['pct_higher'] < .55:\n",
    "        return 0 # no arbitrage\n",
    "    \n",
    "    # if exchange 1 closing price is more than 0.55% higher\n",
    "    # than the exchange 2 closing price\n",
    "    elif df['higher_closing_price'] == 1:\n",
    "        return -1 # arbitrage from exchange 2 to exchange 1\n",
    "    \n",
    "    # if exchange 2 closing price is more than 0.55% higher\n",
    "    # than the exchange 1 closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        return 1 # arbitrage from exchange 1 to exchange 2\n",
    "\n",
    "def get_window_length(df):\n",
    "    \"\"\"function to create column showing how long arbitrage opportunity has lasted\"\"\"\n",
    "    \n",
    "    # convert arbitrage_opportunity column to a list\n",
    "    target_list = df['arbitrage_opportunity'].to_list()\n",
    "    \n",
    "    # set initial window length \n",
    "    window_length = 5 # time in minutes\n",
    "    \n",
    "    # list for window_lengths\n",
    "    window_lengths = []\n",
    "    \n",
    "    # iterate through arbitrage_opportunity column\n",
    "    for i in range(len(target_list)):\n",
    "        \n",
    "        # check if a value in the arbitrage_opportunity column is equal to the\n",
    "        # previous value in the arbitrage_opportunity column and increase\n",
    "        # window length\n",
    "        if target_list[i] == target_list[i-1]:\n",
    "            window_length += 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "        # if a value in the arbitrage_opportunity column is\n",
    "        # not equal to the previous value in the arbitrage_opportunity column\n",
    "        # reset the window length to five minutes\n",
    "        else:\n",
    "            window_length = 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "    # create window length column showing how long an arbitrage opportunity has lasted\n",
    "    df['window_length'] = window_lengths\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    \"\"\"function to merge dataframes and create final features for arbitrage data\"\"\"\n",
    "    \n",
    "    # merging two modified ohlcv dfs on closing time to create arbitrage df\n",
    "    df = pd.merge(df1, df2, on='closing_time',\n",
    "                  suffixes=('_exchange_1', '_exchange_2'))\n",
    "\n",
    "    # convert closing_time to datetime\n",
    "    df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "\n",
    "    # Create additional date features.\n",
    "    df['year'] = df['closing_time'].dt.year\n",
    "    df['month'] = df['closing_time'].dt.month\n",
    "    df['day'] = df['closing_time'].dt.day\n",
    "    \n",
    "    # get higher_closing_price feature to create pct_higher feature\n",
    "    df['higher_closing_price'] = df.apply(get_higher_closing_price, axis=1)\n",
    "    \n",
    "    # get pct_higher feature to create arbitrage_opportunity feature\n",
    "    df['pct_higher'] = df.apply(get_pct_higher, axis=1)\n",
    "    \n",
    "    # create arbitrage_opportunity feature\n",
    "    df['arbitrage_opportunity'] = df.apply(get_arbitrage_opportunity, axis=1)\n",
    "    \n",
    "    # create window_length feature\n",
    "    df = get_window_length(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying arbitrage window length to target, in minutes\n",
    "interval = 30\n",
    "\n",
    "def get_target_value(df, interval=30):\n",
    "    \"\"\"function to get target values; takes df and window length to target\"\"\"\n",
    "    \n",
    "    # if the coming arbitrage window is as long as the targeted interval\n",
    "    if df['window_length_shift'] >= interval:\n",
    "        # if that window is for exchange 1 to 2\n",
    "        if df['arbitrage_opportunity_shift'] == 1:\n",
    "            return 1 # arbitrage from exchange 1 to 2\n",
    "        \n",
    "        # if that window is for exchange 2 to 1\n",
    "        elif df['arbitrage_opportunity_shift'] == -1:\n",
    "            return -1 # arbitrage from exchange 2 to 1\n",
    "        \n",
    "        # if no arbitrage opportunity\n",
    "        elif df['arbitrage_opportunity_shift'] == 0:\n",
    "            return 0 # no arbitrage opportunity\n",
    "        \n",
    "    # if the coming window is less than our targeted interval\n",
    "    else:\n",
    "        return 0 # no arbitrage opportunity\n",
    "    \n",
    "\n",
    "def get_target(df, interval=interval):\n",
    "    \"\"\"function to create target column\"\"\"\n",
    "    \n",
    "    # used to shift rows\n",
    "    # assumes candle length is five minutes, interval is 30 mins\n",
    "    rows_to_shift = int(-1*(interval/5)) # -7\n",
    "    \n",
    "    # arbitrage_opportunity feature, shifted by length of targeted interval\n",
    "    # minus one to predict ten minutes in advance rather than five\n",
    "    df['arbitrage_opportunity_shift'] = df['arbitrage_opportunity'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    \n",
    "    # window_length feature, shifted by length of targeted interval minus one\n",
    "    # to predict ten minutes\n",
    "    df['window_length_shift'] = df['window_length'].shift(rows_to_shift - 1)\n",
    "    \n",
    "    # creating target column; this will indicate if an arbitrage opportunity\n",
    "    # that lasts as long as the targeted interval is forthcoming\n",
    "    df['target'] = df.apply(get_target_value, axis=1)\n",
    "    \n",
    "    # dropping rows where target could not be calculated due to shift\n",
    "    df = df[:rows_to_shift - 1] # -7\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_close_shift(df, interval=interval):\n",
    "    \n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    \n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function to create profit feature\n",
    "def get_profit(df):\n",
    "    \"\"\"function to create profit feature\"\"\"\n",
    "    \n",
    "    # if exchange 1 has the higher closing price\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # return how much money you would make if you bought on exchange 2, sold\n",
    "        # on exchange 1, and took account of 0.55% fees\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    \n",
    "    # if exchange 2 has the higher closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # return how much money you would make if you bought on exchange 1, sold\n",
    "        # on exchange 2, and took account of 0.55% fees\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    \n",
    "    # if the closing prices are the same\n",
    "    else:\n",
    "        return 0 # no arbitrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split names when in the format exchange_trading_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coinbase_pro has an extra underscore so we need a function to split it differently\n",
    "def get_exchange_trading_pair(ex_tp):\n",
    "    \n",
    "    # coinbase_pro\n",
    "    if len(ex_tp.split('_')) == 4:\n",
    "        exchange = ex_tp.split('_')[0] + '_' + ex_tp.split('_')[1]\n",
    "        trading_pair = ex_tp.split('_')[2] + '_' + ex_tp.split('_')[3]\n",
    "    \n",
    "    # all other exchanges\n",
    "    else:\n",
    "        exchange = ex_tp.split('_')[0]\n",
    "        trading_pair = ex_tp.split('_')[1] + '_' + ex_tp.split('_')[2]\n",
    "        \n",
    "    return exchange, trading_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all individual csv's with ta data (~1-2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create /ta_data directory before running this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ta_csvs(csv_filepaths):\n",
    "    \"\"\"Takes a csv filename, creates a dataframe, engineers features,\n",
    "        and saves it as a new csv in /ta_data.\"\"\"\n",
    "    \n",
    "    # counter\n",
    "    n = 1\n",
    "    \n",
    "    for file in csv_filepaths:\n",
    "        \n",
    "        # create df\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # define period\n",
    "        period = '5T'\n",
    "        \n",
    "        # engineer features\n",
    "        df = engineer_features(df, period)\n",
    "        print('features engineered')\n",
    "        \n",
    "        # generate new filename\n",
    "        filename = 'ta_data/' + file.split('/')[1][:-4] + '_ta.csv'\n",
    "        \n",
    "        # export csv\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        # print progress\n",
    "        print(f'csv #{n} saved :)')\n",
    "        \n",
    "        # update counter\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ta_csvs(csv_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all arbitrage training data csv's (~9 hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- create a /arb_data directory before running this function\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~2-3 hours if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arb_csvs(pairs):\n",
    "    \"\"\"Takes a list of possible arbitrage combinations, finds the \n",
    "        appropriate datasets in /ta_data, loads datasets, merges them,\n",
    "        engineers more features, creates a target and exports the new\n",
    "        dataset as a csv\"\"\"\n",
    "    \n",
    "    # counter\n",
    "    n = 0\n",
    "    \n",
    "    # iterate through arbitrage combinations\n",
    "    for pair in pairs:\n",
    "        \n",
    "        # define paths for the csv\n",
    "        csv_1, csv_2 = 'ta_data/' + pair[0][:-4] + '_ta.csv', 'ta_data/' + pair[1][:-4] + '_ta.csv'\n",
    "        \n",
    "        # define exchanges and trading_pairs\n",
    "        ex_tp_1, ex_tp_2 = pair[0][:-8], pair[1][:-8]\n",
    "        exchange_1, trading_pair_1 = get_exchange_trading_pair(ex_tp_1)\n",
    "        exchange_2, trading_pair_2 = get_exchange_trading_pair(ex_tp_2)\n",
    "        print(exchange_1, trading_pair_1,  exchange_2, trading_pair_2)\n",
    "        \n",
    "        # define model_name for the filename\n",
    "        model_name = exchange_1 + '_' + ex_tp_2\n",
    "        print(model_name)\n",
    "          \n",
    "        # create dfs from csv's that already include ta features\n",
    "        df1, df2 = pd.read_csv(csv_1, index_col=0), pd.read_csv(csv_2, index_col=0)       \n",
    "        print('df 1 shape: ', df1.shape, 'df 2 shape: ', df2.shape)\n",
    "        \n",
    "        # merge dfs\n",
    "        df = merge_dfs(df1, df2)\n",
    "        print('dfs merged')\n",
    "        print('merged df shape:' , df.shape)\n",
    "        \n",
    "        # create target \n",
    "        df = get_target(df)\n",
    "        print(model_name, ' ', df.shape)\n",
    "        \n",
    "        # export csv\n",
    "        path = 'arb_data/'\n",
    "        csv_filename = path + model_name + '.csv'\n",
    "        df.to_csv(csv_filename)\n",
    "        \n",
    "        # print progress\n",
    "        print(f'csv #{n} saved :)')\n",
    "        \n",
    "        # update counter\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitfinex eos_usdt hitbtc eos_usdt\n",
      "bitfinex_hitbtc_eos_usdt\n",
      "df 1 shape:  (59409, 69) df 2 shape:  (247395, 69)\n",
      "dfs merged\n",
      "merged df shape: (59409, 144)\n",
      "bitfinex_hitbtc_eos_usdt   (59402, 147)\n",
      "1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_arb_csvs(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get arbitrage data csvs into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "arb_data_paths = glob.glob('arb_data/*.csv')\n",
    "print(len(arb_data_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "- create /pickles and /arbitrage_pickles directories before running this function\n",
    "- test that this function will run to completion before running fully using just one option for each model parameter and one dataset\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~4 hours if split up on 4 notebooks.\n",
    "- this function was written to pick up where it left off in case something goes wrong. It will first check if a specific model exists in the /pickles directory and if not, it will then train the model. If the model does exist, it will just get the performance stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(arb_data_paths):\n",
    "    \"\"\"This function takes in a list of all the arbitrage data paths, \n",
    "        does train/test split, feature selection, trains models, \n",
    "        saves the pickle file, gets performance stats for the model, \n",
    "        and returns a dataframe of performance stats and a dictionary\n",
    "        of confusion matrices for each model\"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    line = '---------------'\n",
    "    performance_list = []\n",
    "    confusion_dict = {}\n",
    "\n",
    "    # this is in case the function stops running you can pick up where you left off\n",
    "    # get all model paths into a variable\n",
    "    model_paths = glob.glob('pickles/*.pkl')\n",
    "    \n",
    "    # iterate through the arbitrage csvs\n",
    "    for file in arb_data_paths:\n",
    "        \n",
    "        # define model name\n",
    "        name = file.split('/')[1][:-8]\n",
    "        \n",
    "        # read csv\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # convert str closing_time to datetime\n",
    "        df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "        \n",
    "        print('\\n' + line*5 + '\\n' + line*2 + name.upper() + line*2 + '\\n' + line*5)\n",
    "\n",
    "        # 70/30 train/test split\n",
    "        test_train_split_row = round(len(df)*.7)\n",
    "        \n",
    "        # get closing_time for t/t split\n",
    "        test_train_split_time = df['closing_time'][test_train_split_row]\n",
    "\n",
    "        # remove 1 week from each end of the t/t datasets to create a \n",
    "        # two week gap between the data - prevents data leakage\n",
    "        train_cutoff_time = test_train_split_time - dt.timedelta(days=7)\n",
    "        test_cutoff_time = test_train_split_time + dt.timedelta(days=7)\n",
    "        print('cutoff time:', train_cutoff_time, test_cutoff_time)\n",
    "        \n",
    "        # train and test subsets\n",
    "        train = df[df['closing_time'] < train_cutoff_time]\n",
    "        test = df[df['closing_time'] > test_cutoff_time]\n",
    "        \n",
    "        # printing shapes to track progress\n",
    "        print('train and test shape: ', train.shape, test.shape)\n",
    "        \n",
    "        # pick features\n",
    "        # not using open, high, or low, which are highly correlated with close \n",
    "        # and do not improve model performance\n",
    "        features = ['close_exchange_1','base_volume_exchange_1', \n",
    "                    'nan_ohlcv_exchange_1','volume_adi_exchange_1', 'volume_obv_exchange_1',\n",
    "                    'volume_cmf_exchange_1', 'volume_fi_exchange_1','volume_em_exchange_1', \n",
    "                    'volume_vpt_exchange_1','volume_nvi_exchange_1', 'volatility_atr_exchange_1',\n",
    "                    'volatility_bbhi_exchange_1','volatility_bbli_exchange_1', \n",
    "                    'volatility_kchi_exchange_1', 'volatility_kcli_exchange_1',\n",
    "                    'volatility_dchi_exchange_1','volatility_dcli_exchange_1',\n",
    "                    'trend_macd_signal_exchange_1', 'trend_macd_diff_exchange_1', \n",
    "                    'trend_adx_exchange_1', 'trend_adx_pos_exchange_1', \n",
    "                    'trend_adx_neg_exchange_1', 'trend_vortex_ind_pos_exchange_1', \n",
    "                    'trend_vortex_ind_neg_exchange_1', 'trend_vortex_diff_exchange_1', \n",
    "                    'trend_trix_exchange_1', 'trend_mass_index_exchange_1', \n",
    "                    'trend_cci_exchange_1', 'trend_dpo_exchange_1', 'trend_kst_sig_exchange_1',\n",
    "                    'trend_kst_diff_exchange_1', 'trend_aroon_up_exchange_1',\n",
    "                    'trend_aroon_down_exchange_1', 'trend_aroon_ind_exchange_1',\n",
    "                    'momentum_rsi_exchange_1', 'momentum_mfi_exchange_1',\n",
    "                    'momentum_tsi_exchange_1', 'momentum_uo_exchange_1',\n",
    "                    'momentum_stoch_signal_exchange_1', 'momentum_wr_exchange_1', \n",
    "                    'momentum_ao_exchange_1', 'others_dr_exchange_1', 'close_exchange_2',\n",
    "                    'base_volume_exchange_2', 'nan_ohlcv_exchange_2',\n",
    "                    'volume_adi_exchange_2', 'volume_obv_exchange_2',\n",
    "                    'volume_cmf_exchange_2', 'volume_fi_exchange_2',\n",
    "                    'volume_em_exchange_2', 'volume_vpt_exchange_2',\n",
    "                    'volume_nvi_exchange_2', 'volatility_atr_exchange_2',\n",
    "                    'volatility_bbhi_exchange_2', 'volatility_bbli_exchange_2',\n",
    "                    'volatility_kchi_exchange_2', 'volatility_kcli_exchange_2',\n",
    "                    'volatility_dchi_exchange_2', 'volatility_dcli_exchange_2',\n",
    "                    'trend_macd_signal_exchange_2',\n",
    "                    'trend_macd_diff_exchange_2', 'trend_adx_exchange_2',\n",
    "                    'trend_adx_pos_exchange_2', 'trend_adx_neg_exchange_2',\n",
    "                    'trend_vortex_ind_pos_exchange_2',\n",
    "                    'trend_vortex_ind_neg_exchange_2',\n",
    "                    'trend_vortex_diff_exchange_2', 'trend_trix_exchange_2',\n",
    "                    'trend_mass_index_exchange_2', 'trend_cci_exchange_2',\n",
    "                    'trend_dpo_exchange_2', 'trend_kst_sig_exchange_2',\n",
    "                    'trend_kst_diff_exchange_2', 'trend_aroon_up_exchange_2',\n",
    "                    'trend_aroon_down_exchange_2',\n",
    "                    'trend_aroon_ind_exchange_2',\n",
    "                    'momentum_rsi_exchange_2', 'momentum_mfi_exchange_2',\n",
    "                    'momentum_tsi_exchange_2', 'momentum_uo_exchange_2',\n",
    "                    'momentum_stoch_signal_exchange_2',\n",
    "                    'momentum_wr_exchange_2', 'momentum_ao_exchange_2',\n",
    "                    'others_dr_exchange_2', 'year', 'month', 'day',\n",
    "                    'higher_closing_price', 'pct_higher', \n",
    "                    'arbitrage_opportunity', 'window_length']\n",
    "        \n",
    "        # pick target\n",
    "        target = 'target'\n",
    "        \n",
    "        # X, y matrix\n",
    "        X_train = train[features]\n",
    "        X_test = test[features]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "        print('train test shapes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        \n",
    "        # filter out datasets that are too small\n",
    "        if (X_train.shape[0] > 1000) and (X_test.shape[0] > 100):\n",
    "            \n",
    "#             max_depth_list = [14] # just for testing to see if function completes\n",
    "            max_depth_list = [14, 15, 17, 18, 21, 25]\n",
    "            for max_depth in max_depth_list:\n",
    "\n",
    "#                 max_features_list = [50] # just for testing to see if function completes\n",
    "                max_features_list = [50, 55, 60, 65, 70, 75, 80]\n",
    "                for max_features in max_features_list:\n",
    "\n",
    "#                     n_estimator_list = [100] # just for testing to see if function completes\n",
    "                    n_estimator_list = [100, 150]\n",
    "                    for n_estimators in n_estimator_list:\n",
    "\n",
    "                        # define model \n",
    "                        model_name = name + '_' + str(max_features) + '_' + str(max_depth) + '_' + str(n_estimators)\n",
    "                        print(line + model_name + line)\n",
    "\n",
    "                        # define model filename to check if it exists\n",
    "                        model_path = f'pickles/{model_name}.pkl'\n",
    "\n",
    "                        # if the model does not exist\n",
    "                        if model_path not in model_paths:\n",
    "                            \n",
    "                            # instantiate model\n",
    "                            model = RandomForestClassifier(max_features=max_features, \n",
    "                                                           max_depth=max_depth, \n",
    "                                                           n_estimators=n_estimators, \n",
    "                                                           n_jobs=-1, \n",
    "                                                           random_state=42)\n",
    "                            \n",
    "                            # there was a weird error caused by two of the datasets which\n",
    "                            # is why this try/except is needed to keep the function running\n",
    "                            try:\n",
    "                                # fit model\n",
    "                                model = model.fit(X_train, y_train)\n",
    "                                print('model fitted!')\n",
    "\n",
    "                                # train accuracy\n",
    "                                train_score = model.score(X_train, y_train)\n",
    "                                print('train accuracy:', train_score)\n",
    "\n",
    "                                # make predictions\n",
    "                                y_preds = model.predict(X_test)\n",
    "                                print('predictions made!')\n",
    "\n",
    "                                # test accuracy\n",
    "                                score = accuracy_score(y_test, y_preds)\n",
    "                                print('test accuracy:', score)\n",
    "\n",
    "                                # save model\n",
    "                                pickle.dump(model, open('pickles/{model_name}.pkl'.format(\n",
    "                                            model_name=model_name), 'wb'))\n",
    "                                print('pickle saved!'.format(model_name=model_name))\n",
    "\n",
    "                            except:\n",
    "                                print(line*3 + '\\n' + line + 'ERROR' + line + '\\n' + line*3)\n",
    "                                break # break out of for loop if there is an error with modeling\n",
    "\n",
    "                        # if the model exists\n",
    "                        else:\n",
    "\n",
    "                            # load model\n",
    "                            model = pickle.load(open(model_path, 'rb'))\n",
    "                            print('model loaded')\n",
    "\n",
    "                            # train accuracy\n",
    "                            train_score = model.score(X_train, y_train)\n",
    "                            print('train accuracy:', train_score)\n",
    "\n",
    "                            # make predictions\n",
    "                            y_preds = model.predict(X_test)\n",
    "                            print('predictions made!')\n",
    "\n",
    "                            # test accuracy\n",
    "                            score = accuracy_score(y_test, y_preds)\n",
    "                            print('test accuracy:', score)\n",
    "                        \n",
    "                        ############## Performance metrics ###############\n",
    "                        # TODO: put this all in a function and just return the \n",
    "                        # metrics we want\n",
    "                        \n",
    "                        # labels for confusion matrix\n",
    "                        unique_y_test = y_test.unique().tolist()\n",
    "                        unique_y_preds = list(set(y_preds))\n",
    "                        labels = list(set(unique_y_test + unique_y_preds))\n",
    "                        labels.sort()\n",
    "                        columns = [f'Predicted {label}' for label in labels]\n",
    "                        index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "                        # create confusion matrix\n",
    "                        confusion = pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "                                                 columns=columns, index=index)\n",
    "                        print(model_name + ' confusion matrix:')\n",
    "                        print(confusion, '\\n')\n",
    "\n",
    "                        # append to confusion list\n",
    "                        confusion_dict[model_name] = confusion\n",
    "\n",
    "                        # creating dataframe from test set to calculate profitability\n",
    "                        test_with_preds = X_test.copy()\n",
    "\n",
    "                        # add column with higher closing price\n",
    "                        test_with_preds['higher_closing_price'] = test_with_preds.apply(\n",
    "                                get_higher_closing_price, axis=1)\n",
    "\n",
    "                        # add column with shifted closing price\n",
    "                        test_with_preds = get_close_shift(test_with_preds)\n",
    "\n",
    "                        # adding column with predictions\n",
    "                        test_with_preds['pred'] = y_preds\n",
    "\n",
    "                        # adding column with profitability of predictions\n",
    "                        test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "                                get_profit, axis=1).shift(-2)\n",
    "\n",
    "                        # filtering out rows where no arbitrage is predicted\n",
    "                        test_with_preds = test_with_preds[test_with_preds['pred'] != 0]\n",
    "\n",
    "                        # calculating mean profit where arbitrage predicted...\n",
    "                        pct_profit_mean = test_with_preds['pct_profit'].mean()\n",
    "\n",
    "                        # calculating median profit where arbitrage predicted...\n",
    "                        pct_profit_median = test_with_preds['pct_profit'].median()\n",
    "                        print('percent profit mean:', pct_profit_mean)\n",
    "                        print('percent profit median:', pct_profit_median, '\\n\\n')\n",
    "\n",
    "                        # save net performance to list\n",
    "                        performance_list.append([name, max_features, max_depth, n_estimators,\n",
    "                                                 pct_profit_mean, pct_profit_median])\n",
    "                        ######################## END OF TODO ###########################\n",
    "                        \n",
    "                        \n",
    "        # if there is not enough data\n",
    "        else:\n",
    "            print('{model_name}: not enough data!'.format(model_name=name))\n",
    "        \n",
    "        # update count\n",
    "        # TODO: make a better counter that is actually useful in \n",
    "        # showing how much is left \n",
    "        counter += 1\n",
    "        print(counter, '\\n')\n",
    "        \n",
    "    # create a dataframe for performace of all models\n",
    "    df = pd.DataFrame(performance_list, columns = ['ex_tp', 'max_features', 'max_depth', \n",
    "                                                   'n_estimators', 'pct_profit_mean','pct_profit_median'])\n",
    "    \n",
    "    return df, confusion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "------------------------------BITFINEX_GEMINI_BCH------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "cutoff time: 2019-10-16 02:50:00 2019-10-30 02:50:00\n",
      "train and test shape:  (6414, 147) (1596, 147)\n",
      "train test shapes: (6414, 91) (1596, 91) (6414,) (1596,)\n",
      "---------------bitfinex_gemini_bch_50_14_100---------------\n",
      "model fitted!\n",
      "train accuracy: 0.9873713751169317\n",
      "predictions made!\n",
      "test accuracy: 0.7142857142857143\n",
      "pickle saved!\n",
      "bitfinex_gemini_bch_50_14_100 confusion matrix:\n",
      "           Predicted -1  Predicted 0  Predicted 1\n",
      "Actual -1            20           53            1\n",
      "Actual 0             27          945          170\n",
      "Actual 1              0          205          175 \n",
      "\n",
      "percent profit mean: 0.4825669023900904\n",
      "percent profit median: 0.24872418149823372 \n",
      "\n",
      "\n",
      "1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df, confusion_dict = create_models(arb_data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export model performance data into csvs and JSON "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need /model_perf and /cm directories to store the performance csv's and JSON if you split up running the models on several notebooks. You will also have to change the name of each of the files being exported in the other notebooks otherwise you will overwrite everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting model performance to csv\n",
    "df.to_csv('model_perf/perf1.csv', index=False) # change the name of this file\n",
    "\n",
    "# exporting confusion matrices to json\n",
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj, 'to_json'):\n",
    "            return obj.to_json(orient='records')\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "with open('cm/confusion1.json', 'w') as fp: # change the name of this file\n",
    "    json.dump(confusion_dict, fp, cls=JSONEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate performance and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_perf_dfs(filepaths):\n",
    "    \n",
    "    df_list = []\n",
    "    for path in filepaths:\n",
    "        df = pd.read_csv(path)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df = pd.concat(df_list)\n",
    "    df = df.sort_values(by='pct_profit_mean', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def concat_dicts(filepaths):\n",
    "    \n",
    "    confusion_dict = {}\n",
    "    \n",
    "    for path in filepaths:\n",
    "        confusion = json.load(open(path))\n",
    "        confusion_dict.update(confusion)\n",
    "    \n",
    "    return confusion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check the number of files you have in each folder\n",
    "# if you ran on 4 notebooks you should have 4\n",
    "\n",
    "perf_csv_paths = glob.glob('model_perf/*.csv')\n",
    "confusion_paths = glob.glob('cm/*.json')\n",
    "\n",
    "print(len(perf_csv_paths))\n",
    "print(len(confusion_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# concatenate all confusion matrices\n",
    "confusion_dict = concat_dicts(confusion_paths)\n",
    "\n",
    "# concatenate all performance dataframes\n",
    "perf_df = concat_perf_dfs(perf_csv_paths)\n",
    "\n",
    "# check the number of entries in performance data\n",
    "# should be the number of models trained\n",
    "# lengths of confusion_dict and perf_df should be same\n",
    "print(len(confusion_dict.keys()))\n",
    "print(len(perf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at performance dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_tp</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>pct_profit_mean</th>\n",
       "      <th>pct_profit_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bitfinex_gemini_bch</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0.482567</td>\n",
       "      <td>0.248724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ex_tp  max_features  max_depth  n_estimators  \\\n",
       "0  bitfinex_gemini_bch            50         14           100   \n",
       "\n",
       "   pct_profit_mean  pct_profit_median  \n",
       "0         0.482567           0.248724  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at confusion dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bitfinex_gemini_bch_50_14_100': '[{\"Predicted -1\":20,\"Predicted 0\":53,\"Predicted 1\":1},{\"Predicted -1\":27,\"Predicted 0\":945,\"Predicted 1\":170},{\"Predicted -1\":0,\"Predicted 0\":205,\"Predicted 1\":175}]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Models were evaluated on several metrics to select the best models that predict arbitrage. The following were taken into consideration:\n",
    "- How often the models predicted arbitrage when it didn't exist (False positives)\n",
    "- How many times the models predicted arbitrage correctly (True positives)\n",
    "- How profitable the model was in the real world over the period of the test set. \n",
    "\n",
    "Model accuracy scores are not a good evaluation of model performance alone, and neither is profitability alone. \n",
    "\n",
    "**Why is accuracy score alone a bad measure?**\n",
    "\n",
    "The model can be making predictions of 0 (no arbitrage) 100% of the time and still achieve high accuracy because the dataset may not have that many arbitrage opportunities. So simply guessing 0 all the time could be more \"accurate\" than a model that makes arbitrage predictions. You want to make sure that the model is actually predicting arbitrage.\n",
    "\n",
    "**Why is profitability alone a bad measure?**\n",
    "\n",
    "Models that are profitable can be making predictions of arbitrage where arbitrage doesn't actually exist. Those predictions may be profitable by chance, but the model is not predicting what you want it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataframe with PNL and Confusion Matrix Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_confusion(df, confusion_dict):\n",
    "    \"\"\"This function takes in the dataframe of performance stats \n",
    "        for all the models, their respective confusion matrices,\n",
    "        creates new features from the confusion matrices, \n",
    "        and returns a dataframe with all of the performance stats\"\"\"\n",
    "    \n",
    "    line = '-------'\n",
    "    feature_dict = {}\n",
    "    model_name_list = []\n",
    "    \n",
    "    # create a copy of df to not overwrite original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # iterate through all models\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        # define model name\n",
    "        model_name = (df.ex_tp.iloc[i] + '_' + str(df.max_features.iloc[i]) \n",
    "                      + '_' + str(df.max_depth.iloc[i]) + '_' + str(df.n_estimators.iloc[i]))\n",
    "        model_name_list.append(model_name)\n",
    "        \n",
    "        # get confusion matrix for specific model\n",
    "        conf_mat = pd.read_json(confusion_dict[model_name])\n",
    "\n",
    "        #########################################################\n",
    "        ############## create confusion features ################\n",
    "        #########################################################\n",
    "        \n",
    "        # Some models never predicted -1, some never predicted 1, and \n",
    "        # some never predicted 1 or -1, meaning that they never predicted\n",
    "        # arbitrage at all. Each case needs to be handled with a conditional.\n",
    "        \n",
    "        # confusion matrix has -1, 0, 1 predictions\n",
    "        if 'Predicted 1' in conf_mat.columns and 'Predicted -1' in conf_mat.columns:\n",
    "\n",
    "            # % incorrect predictions for 0, 1, -1\n",
    "            pct_wrong_0 = (conf_mat['Predicted 0'].loc[0] + \n",
    "                           conf_mat['Predicted 0'].loc[2])/conf_mat['Predicted 0'].sum()\n",
    "            pct_wrong_1 = (conf_mat['Predicted 1'].loc[0] + \n",
    "                           conf_mat['Predicted 1'].loc[1])/conf_mat['Predicted 1'].sum()\n",
    "            pct_wrong_neg1 = (conf_mat['Predicted -1'].loc[1] + \n",
    "                               conf_mat['Predicted -1'].loc[2])/conf_mat['Predicted -1'].sum()\n",
    "\n",
    "            # total number correct arbitrage preds (-1)\n",
    "            correct_arb_neg1 = conf_mat['Predicted -1'].loc[0]\n",
    "\n",
    "            # total number correct arbitrage preds (1)\n",
    "            correct_arb_1 = conf_mat['Predicted 1'].loc[2]\n",
    "            \n",
    "            # total number correct arbitrage preds (-1) + (1)\n",
    "            correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "\n",
    "            # total number correct no arbitrage preds (0)\n",
    "            correct_arb_0 = conf_mat['Predicted 0'].loc[1]\n",
    "\n",
    "        # confusion matrix has 0, 1 predictions\n",
    "        elif 'Predicted 1' in conf_mat.columns:\n",
    "\n",
    "            pct_wrong_0 = conf_mat['Predicted 0'].loc[1] / conf_mat['Predicted 0'].sum()\n",
    "            pct_wrong_1 = conf_mat['Predicted 1'].loc[0] / conf_mat['Predicted 1'].sum()\n",
    "            pct_wrong_neg1 = np.nan\n",
    "\n",
    "            # total number correct arbitrage preds (-1)\n",
    "            correct_arb_neg1 = 0\n",
    "\n",
    "            # total number correct arbitrage preds (1)\n",
    "            correct_arb_1 = conf_mat['Predicted 1'].loc[1]\n",
    "            \n",
    "            # total number correct arbitrage preds (-1) + (1)\n",
    "            correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "\n",
    "            # total number correct no arbitrage preds (0)\n",
    "            correct_arb_0 = conf_mat['Predicted 0'].loc[0]\n",
    "        \n",
    "        # confusion matrix has -1, 0 predictions\n",
    "        elif 'Predicted -1' in conf_mat.columns:\n",
    "\n",
    "            pct_wrong_0 = conf_mat['Predicted 0'].loc[0] / conf_mat['Predicted 0'].sum()\n",
    "            pct_wrong_1 = np.nan\n",
    "            pct_wrong_neg1 = conf_mat['Predicted -1'].loc[1] / conf_mat['Predicted -1'].sum()\n",
    "\n",
    "            # total number correct arbitrage preds (-1)\n",
    "            correct_arb_neg1 = conf_mat['Predicted -1'].loc[0]\n",
    "\n",
    "            # total number correct arbitrage preds (1)\n",
    "            correct_arb_1 = 0\n",
    "            \n",
    "            # total number correct arbitrage preds (-1) + (1)\n",
    "            correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "\n",
    "            # total number correct no arbitrage preds (0)\n",
    "            correct_arb_0 = conf_mat['Predicted 0'].loc[1]\n",
    "        \n",
    "        # confusion matrix has only 0\n",
    "        else:\n",
    "            pct_wrong_0 = 0\n",
    "            pct_wrong_1 = 0\n",
    "            pct_wrong_neg1 = 0\n",
    "            correct_arb = 0\n",
    "            correct_arb_neg1 = 0\n",
    "            correct_arb_1 = 0\n",
    "            correct_arb_0 = 0\n",
    "        \n",
    "        # add confusion features to dict\n",
    "        feature_list = [correct_arb, pct_wrong_0, pct_wrong_1, pct_wrong_neg1, \n",
    "                        correct_arb_neg1, correct_arb_1, correct_arb_0]\n",
    "        feature_dict[model_name] = feature_list\n",
    "\n",
    "    # create a df from the new features\n",
    "    columns = ['correct_arb', 'pct_wrong_0', 'pct_wrong_1', 'pct_wrong_neg1', \n",
    "                'correct_arb_neg1', 'correct_arb_1', 'correct_arb_0']\n",
    "    df2 = pd.DataFrame(feature_dict).transpose().reset_index()\n",
    "    df2 = df2.rename(columns = {'index': 'model_name', 0: 'correct_arb', 1:'pct_wrong_0', \n",
    "                                2: 'pct_wrong_1', 3: 'pct_wrong_neg1', \n",
    "                                4: 'correct_arb_neg1', 5: 'correct_arb_1', \n",
    "                                6: 'correct_arb_0'})\n",
    "    \n",
    "    # merge new features with performance df\n",
    "    df['model_name'] = model_name_list\n",
    "    print(df.shape, df2.shape)\n",
    "    df = df.merge(df2, on='model_name').drop(columns = 'model_name')\n",
    "    print('shape after merge:', df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_tp</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>pct_profit_mean</th>\n",
       "      <th>pct_profit_median</th>\n",
       "      <th>correct_arb</th>\n",
       "      <th>pct_wrong_0</th>\n",
       "      <th>pct_wrong_1</th>\n",
       "      <th>pct_wrong_neg1</th>\n",
       "      <th>correct_arb_neg1</th>\n",
       "      <th>correct_arb_1</th>\n",
       "      <th>correct_arb_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>150</td>\n",
       "      <td>9.692778</td>\n",
       "      <td>11.240564</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>9.583432</td>\n",
       "      <td>11.099366</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>9.572652</td>\n",
       "      <td>11.296690</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>9.275228</td>\n",
       "      <td>10.927262</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>9.252052</td>\n",
       "      <td>11.141901</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6379</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6381</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6382</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6383</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6384 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ex_tp  max_features  max_depth  n_estimators  \\\n",
       "0     coinbase_pro_hitbtc_bch_btc            60         17           150   \n",
       "1     coinbase_pro_hitbtc_bch_btc            60         17           100   \n",
       "2     coinbase_pro_hitbtc_bch_btc            50         14           150   \n",
       "3     coinbase_pro_hitbtc_bch_btc            60         14           150   \n",
       "4     coinbase_pro_hitbtc_bch_btc            60         14           100   \n",
       "...                           ...           ...        ...           ...   \n",
       "6379  coinbase_pro_kraken_ltc_usd            65         25           150   \n",
       "6380  coinbase_pro_kraken_ltc_usd            70         25           100   \n",
       "6381  coinbase_pro_kraken_ltc_usd            70         25           150   \n",
       "6382  coinbase_pro_kraken_ltc_usd            75         25           100   \n",
       "6383  coinbase_pro_kraken_ltc_usd            75         25           150   \n",
       "\n",
       "      pct_profit_mean  pct_profit_median  correct_arb  pct_wrong_0  \\\n",
       "0            9.692778          11.240564         60.0     0.000862   \n",
       "1            9.583432          11.099366         54.0     0.001069   \n",
       "2            9.572652          11.296690         58.0     0.000931   \n",
       "3            9.275228          10.927262         64.0     0.000724   \n",
       "4            9.252052          11.141901         63.0     0.000759   \n",
       "...               ...                ...          ...          ...   \n",
       "6379              NaN                NaN          0.0     0.000000   \n",
       "6380              NaN                NaN          0.0     0.000000   \n",
       "6381              NaN                NaN          0.0     0.000000   \n",
       "6382              NaN                NaN          0.0     0.000000   \n",
       "6383              NaN                NaN          0.0     0.000000   \n",
       "\n",
       "      pct_wrong_1  pct_wrong_neg1  correct_arb_neg1  correct_arb_1  \\\n",
       "0             NaN        0.062500              60.0            0.0   \n",
       "1             NaN        0.052632              54.0            0.0   \n",
       "2             NaN        0.079365              58.0            0.0   \n",
       "3             NaN        0.085714              64.0            0.0   \n",
       "4             NaN        0.100000              63.0            0.0   \n",
       "...           ...             ...               ...            ...   \n",
       "6379          0.0        0.000000               0.0            0.0   \n",
       "6380          0.0        0.000000               0.0            0.0   \n",
       "6381          0.0        0.000000               0.0            0.0   \n",
       "6382          0.0        0.000000               0.0            0.0   \n",
       "6383          0.0        0.000000               0.0            0.0   \n",
       "\n",
       "      correct_arb_0  \n",
       "0           28974.0  \n",
       "1           28975.0  \n",
       "2           28973.0  \n",
       "3           28972.0  \n",
       "4           28971.0  \n",
       "...             ...  \n",
       "6379            0.0  \n",
       "6380            0.0  \n",
       "6381            0.0  \n",
       "6382            0.0  \n",
       "6383            0.0  \n",
       "\n",
       "[6384 rows x 13 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = model_confusion(perf_df, confusion_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after filetering pct_wrong_0: (6121, 13)\n",
      "shape after filtering correct_arb: (1803, 13)\n",
      "shape after filtering pct_profit_mean: (1152, 13)\n",
      "shape after droping duplicates: (21, 13)\n"
     ]
    }
   ],
   "source": [
    "# filter for models that are predicting arb when its not happening < 30% of the time\n",
    "df2 = df[df['pct_wrong_0'] < 0.30]\n",
    "print('shape after filetering pct_wrong_0:', df2.shape)\n",
    "\n",
    "# filter for models that predict > 25 correct arb \n",
    "df2 = df2[df2['correct_arb'] > 25]\n",
    "print('shape after filtering correct_arb:', df2.shape)\n",
    "\n",
    "# filter for models that make > 0.20% profit\n",
    "df2 = df2[df2['pct_profit_mean'] > 0.2]\n",
    "print('shape after filtering pct_profit_mean:', df2.shape)\n",
    "\n",
    "# sort values to have the best model from each option listed first\n",
    "# and drop the rest of the duplicates\n",
    "df2 = df2.sort_values(by=['correct_arb'], ascending=False)\n",
    "df2 = df2.drop_duplicates(subset='ex_tp')\n",
    "\n",
    "print('shape after droping duplicates:', df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect best model stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_tp</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>pct_profit_mean</th>\n",
       "      <th>pct_profit_median</th>\n",
       "      <th>correct_arb</th>\n",
       "      <th>pct_wrong_0</th>\n",
       "      <th>pct_wrong_1</th>\n",
       "      <th>pct_wrong_neg1</th>\n",
       "      <th>correct_arb_neg1</th>\n",
       "      <th>correct_arb_1</th>\n",
       "      <th>correct_arb_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>bitfinex_coinbase_pro_ltc_usd</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>1.530637</td>\n",
       "      <td>1.558647</td>\n",
       "      <td>42154.0</td>\n",
       "      <td>0.073884</td>\n",
       "      <td>0.270370</td>\n",
       "      <td>0.286640</td>\n",
       "      <td>41957.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>37391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>coinbase_pro_hitbtc_eth_usdc</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>0.849957</td>\n",
       "      <td>0.623874</td>\n",
       "      <td>6509.0</td>\n",
       "      <td>0.243025</td>\n",
       "      <td>0.228881</td>\n",
       "      <td>0.225672</td>\n",
       "      <td>4090.0</td>\n",
       "      <td>2419.0</td>\n",
       "      <td>12562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>bitfinex_hitbtc_bch_usdt</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0.655209</td>\n",
       "      <td>0.548990</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>0.224332</td>\n",
       "      <td>0.247346</td>\n",
       "      <td>0.349186</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1418.0</td>\n",
       "      <td>9557.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>bitfinex_hitbtc_ltc_usdt</td>\n",
       "      <td>60</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>0.322593</td>\n",
       "      <td>0.261954</td>\n",
       "      <td>558.0</td>\n",
       "      <td>0.093674</td>\n",
       "      <td>0.428302</td>\n",
       "      <td>0.529520</td>\n",
       "      <td>255.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>13352.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>bitfinex_coinbase_pro_etc_usd</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>1.263888</td>\n",
       "      <td>0.392540</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.253311</td>\n",
       "      <td>0.909375</td>\n",
       "      <td>29.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>35741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1297</td>\n",
       "      <td>coinbase_pro_gemini_ltc_btc</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>0.448320</td>\n",
       "      <td>0.327193</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>0.436508</td>\n",
       "      <td>0.401813</td>\n",
       "      <td>198.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>9739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>bitfinex_gemini_bch_btc</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>0.670892</td>\n",
       "      <td>0.343057</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.174439</td>\n",
       "      <td>0.407792</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>22.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>gemini_hitbtc_bch_btc</td>\n",
       "      <td>75</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>0.893036</td>\n",
       "      <td>0.467330</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.149167</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.355482</td>\n",
       "      <td>194.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>bitfinex_gemini_ltc_btc</td>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "      <td>100</td>\n",
       "      <td>0.248251</td>\n",
       "      <td>0.076761</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.136985</td>\n",
       "      <td>0.691860</td>\n",
       "      <td>0.514469</td>\n",
       "      <td>151.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>9595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>coinbase_pro_gemini_bch_btc</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>0.854335</td>\n",
       "      <td>0.499952</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.153239</td>\n",
       "      <td>0.349823</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>15.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1072.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>gemini_hitbtc_ltc_btc</td>\n",
       "      <td>80</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0.438228</td>\n",
       "      <td>0.399027</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.130750</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>27.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>9826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>bitfinex_hitbtc_eos_usdt</td>\n",
       "      <td>75</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0.439146</td>\n",
       "      <td>0.411328</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0.475490</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>17.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>14858.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1813</td>\n",
       "      <td>kraken_gemini_bch_btc</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>0.214579</td>\n",
       "      <td>0.136856</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.191710</td>\n",
       "      <td>0.487437</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>5.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1092.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>kraken_gemini_ltc_btc</td>\n",
       "      <td>65</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>0.313630</td>\n",
       "      <td>0.219231</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.514925</td>\n",
       "      <td>65.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1642</td>\n",
       "      <td>bitfinex_coinbase_pro_bch_usd</td>\n",
       "      <td>55</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>0.250101</td>\n",
       "      <td>-0.190736</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.989290</td>\n",
       "      <td>16.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>27123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>bitfinex_coinbase_pro_bch_btc</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>2.365913</td>\n",
       "      <td>-0.300623</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>28479.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>9.275228</td>\n",
       "      <td>10.927262</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>bitfinex_hitbtc_ltc_btc</td>\n",
       "      <td>80</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>0.600388</td>\n",
       "      <td>0.630759</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>150854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>bitfinex_coinbase_pro_eth_btc</td>\n",
       "      <td>80</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>1.138764</td>\n",
       "      <td>1.428952</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>107207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>bitfinex_coinbase_pro_ltc_btc</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>0.224988</td>\n",
       "      <td>0.122285</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>99303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>bitfinex_hitbtc_eth_btc</td>\n",
       "      <td>55</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>1.262654</td>\n",
       "      <td>1.856009</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>113217.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ex_tp  max_features  max_depth  n_estimators  \\\n",
       "349   bitfinex_coinbase_pro_ltc_usd            50         14           100   \n",
       "783    coinbase_pro_hitbtc_eth_usdc            55         14           150   \n",
       "997        bitfinex_hitbtc_bch_usdt            50         14           100   \n",
       "1490       bitfinex_hitbtc_ltc_usdt            60         15           100   \n",
       "422   bitfinex_coinbase_pro_etc_usd            70         15           100   \n",
       "1297    coinbase_pro_gemini_ltc_btc            80         17           100   \n",
       "975         bitfinex_gemini_bch_btc            50         18           100   \n",
       "695           gemini_hitbtc_bch_btc            75         17           100   \n",
       "1645        bitfinex_gemini_ltc_btc            75         18           100   \n",
       "770     coinbase_pro_gemini_bch_btc            50         25           100   \n",
       "1305          gemini_hitbtc_ltc_btc            80         14           100   \n",
       "1304       bitfinex_hitbtc_eos_usdt            75         14           100   \n",
       "1813          kraken_gemini_bch_btc            55         14           100   \n",
       "1510          kraken_gemini_ltc_btc            65         21           100   \n",
       "1642  bitfinex_coinbase_pro_bch_usd            55         21           100   \n",
       "114   bitfinex_coinbase_pro_bch_btc            50         15           100   \n",
       "3       coinbase_pro_hitbtc_bch_btc            60         14           150   \n",
       "1041        bitfinex_hitbtc_ltc_btc            80         15           150   \n",
       "478   bitfinex_coinbase_pro_eth_btc            80         14           100   \n",
       "1776  bitfinex_coinbase_pro_ltc_btc            65         15           150   \n",
       "424         bitfinex_hitbtc_eth_btc            55         15           150   \n",
       "\n",
       "      pct_profit_mean  pct_profit_median  correct_arb  pct_wrong_0  \\\n",
       "349          1.530637           1.558647      42154.0     0.073884   \n",
       "783          0.849957           0.623874       6509.0     0.243025   \n",
       "997          0.655209           0.548990       2458.0     0.224332   \n",
       "1490         0.322593           0.261954        558.0     0.093674   \n",
       "422          1.263888           0.392540        480.0     0.012379   \n",
       "1297         0.448320           0.327193        269.0     0.126155   \n",
       "975          0.670892           0.343057        250.0     0.174439   \n",
       "695          0.893036           0.467330        244.0     0.149167   \n",
       "1645         0.248251           0.076761        204.0     0.136985   \n",
       "770          0.854335           0.499952        199.0     0.153239   \n",
       "1305         0.438228           0.399027        175.0     0.130750   \n",
       "1304         0.439146           0.411328        124.0     0.043333   \n",
       "1813         0.214579           0.136856        107.0     0.191710   \n",
       "1510         0.313630           0.219231        104.0     0.219626   \n",
       "1642         0.250101          -0.190736         84.0     0.002171   \n",
       "114          2.365913          -0.300623         68.0     0.000947   \n",
       "3            9.275228          10.927262         64.0     0.000724   \n",
       "1041         0.600388           0.630759         41.0     0.000424   \n",
       "478          1.138764           1.428952         40.0     0.000671   \n",
       "1776         0.224988           0.122285         33.0     0.000905   \n",
       "424          1.262654           1.856009         30.0     0.000609   \n",
       "\n",
       "      pct_wrong_1  pct_wrong_neg1  correct_arb_neg1  correct_arb_1  \\\n",
       "349      0.270370        0.286640           41957.0          197.0   \n",
       "783      0.228881        0.225672            4090.0         2419.0   \n",
       "997      0.247346        0.349186            1040.0         1418.0   \n",
       "1490     0.428302        0.529520             255.0          303.0   \n",
       "422      0.253311        0.909375              29.0          451.0   \n",
       "1297     0.436508        0.401813             198.0           71.0   \n",
       "975      0.407792        0.584906              22.0          228.0   \n",
       "695      0.479167        0.355482             194.0           50.0   \n",
       "1645     0.691860        0.514469             151.0           53.0   \n",
       "770      0.349823        0.687500              15.0          184.0   \n",
       "1305     0.408000        0.437500              27.0          148.0   \n",
       "1304     0.475490        0.753623              17.0          107.0   \n",
       "1813     0.487437        0.838710               5.0          102.0   \n",
       "1510     0.220000        0.514925              65.0           39.0   \n",
       "1642     0.209302        0.989290              16.0           68.0   \n",
       "114      0.728000             NaN               0.0           68.0   \n",
       "3             NaN        0.085714              64.0            0.0   \n",
       "1041     0.166667        0.416667              21.0           20.0   \n",
       "478      0.239130        0.500000               5.0           35.0   \n",
       "1776     0.509804        0.500000               8.0           25.0   \n",
       "424      0.129032        0.700000               3.0           27.0   \n",
       "\n",
       "      correct_arb_0  \n",
       "349         37391.0  \n",
       "783         12562.0  \n",
       "997          9557.0  \n",
       "1490        13352.0  \n",
       "422         35741.0  \n",
       "1297         9739.0  \n",
       "975           956.0  \n",
       "695          1021.0  \n",
       "1645         9595.0  \n",
       "770          1072.0  \n",
       "1305         9826.0  \n",
       "1304        14858.0  \n",
       "1813         1092.0  \n",
       "1510         2004.0  \n",
       "1642        27123.0  \n",
       "114         28479.0  \n",
       "3           28972.0  \n",
       "1041       150854.0  \n",
       "478        107207.0  \n",
       "1776        99303.0  \n",
       "424        113217.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move best models into a new folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there were >6K models trained, it is difficult to find the best ones in the folder with all of the models. Moving the models to a new folder makes them easier to find and download.\n",
    "\n",
    "Be super careful when running this next cell because it will rename the models with it's general name without the parameters - that is to abide by naming conventions in lambda functions. If you want to revert this change you will have to manually find the model parameters and rename each one. So do not run this unless you're absolutely sure that the way you filtered the models is final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 21 21 21\n"
     ]
    }
   ],
   "source": [
    "# info for model names\n",
    "models = df2['ex_tp'].values\n",
    "max_features = df2['max_features'].values\n",
    "max_depth = df2['max_depth'].values\n",
    "n_estimators = df2['n_estimators'].values\n",
    "\n",
    "# check that the list lengths are what you expect\n",
    "# the length should be the number of rows in the filtered df\n",
    "print(len(models), len(max_features), len(max_depth), len(n_estimators))\n",
    "\n",
    "#### commented out to prevent accidental run ####\n",
    "# for i in range(len(models)):\n",
    "    \n",
    "#     # define model name\n",
    "#     model_name = models[i] + '_' + str(max_features[i]) + '_' + str(max_depth[i]) + '_' + str(n_estimators[i])\n",
    "    \n",
    "#     # rename the filepath to move\n",
    "#     os.rename(f'pickles/{model_name}.pkl', f'arb_pickles/{models[i]}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the correct number of models were moved\n",
    "\n",
    "model_paths = glob.glob('arbitrage_pickles/*.pkl')\n",
    "print(len(model_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "The data sets for all models were split by a 70/30 train/test split, meaning that the size of the test data varies greatly for each model. The size of the test set is larger or smaller depending on the starting size of the merged dataset for the two exchanges. The % profit mean is the average percentage gained if one were to act on all arbitrage opportunities predicted by that specific model during the period of the test set. With a larger test set, there is a better chance of seeing more arbitrage predictions, and the % profit mean number will be more accurate as it's averaged over more observations. Right now, it's possible that the % profit mean is skewed due to different testing timeframes where more or less arbitrage predictions are observed. \n",
    "\n",
    "When more data is available, it may be more accurate to apply evaluation metrics to the last 100-200 arbitrage predictions by the model so that you can make valid comparisons between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "- More hyperparameter tuning\n",
    "- Neural Networks\n",
    "- Implement auto model retraining\n",
    "    - create a trigger that checks for model decay\n",
    "    - retrain models in cloud if decayed, select best performers, save new version to S3 Buckets\n",
    "    - save train/test data of newest version for reference\n",
    "- Create a bot that will act on arbitrage predictions from models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic",
   "language": "python",
   "name": "cryptolytic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
