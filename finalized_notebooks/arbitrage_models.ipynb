{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==0.21.3 in /anaconda3/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from scikit-learn==0.21.3) (0.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ta==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background on modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrage models are generated by getting the combination of 2 exchanges that support the same trading pair, merging their available data, engineering features, and creating a target that signals an arbitrage opportunity. A valid arbitrage signal is when the arbitrage lasts >30 mins because it takes time to move coins from one exchange to the other in order to successfully complete the arbitrage trades. These functions will take over a day to run if not split up on separate notebooks. There are 95 total options for models, 75 of those options have enough data to train models, and with different options for parameters around ~7K models will be trained. After selecting for the best models there were 21 good ones included in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "├── arbitrage/                        <-- The top-level directory for all arbitrage work\n",
    "│   ├── arbitrage_models.ipynb        <-- notebook for arbitrage models\n",
    "│   ├── all_data/                     <-- Directory with subdirectories containing 5 min candle data\n",
    "│   │      ├──bitfinex_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      ├──coinbase_pro_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      ├──gemini_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      ├──hitbtc_300/\n",
    "│   │      │      └── data.csv\n",
    "│   │      └──kraken_300/\n",
    "│   │             └── data.csv\n",
    "│   ├── data/                         <-- Directory for csv files of 5 min candle data\n",
    "│   │     └── data.csv                \n",
    "│   ├── ta_data/                      <-- Directory for csv files of data after ta features engineered\n",
    "│   │     └── data.csv                \n",
    "│   ├── arb_data/                     <-- Directory for csv files of final arbitrage training data\n",
    "│   │     └── data.csv               \n",
    "│   ├── pickles/                      <-- Directory for all pickle models\n",
    "│   │     └── models.pkl              \n",
    "│   ├── arbitrage_pickles             <-- Directory for final models after model selection\n",
    "│   │     └── models.pkl              \n",
    "│   │\n",
    "│   ├── cm/                           <-- Directory for confusion matrices after training models\n",
    "│   │\n",
    "│   ├── model_perf/                   <-- Directory for performance csvs after training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it doesn't make sense that there are two folders with the same exact data, the only difference being the subdirectories. A function was written to get the combinations for arbitrage with the subdirectories so it's necessary for now until the function is rewritten ¯\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the models predicting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models predict whether there will be an arbitrage opportunity that starts 10 mins from the prediction time and lasts for at least 30 mins, giving a user enough times to execute trades. The model will return 0 (no arbitrage), 1 (arbitrage from exchange 1 to exchange 2) and -1 (arbitrage from exchange 2 to exchange 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of the 5 min candle data filepaths into a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepaths = glob.glob('data/*.csv')\n",
    "len(csv_filepaths) #80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get all combinations of exchanges with the same trading pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 min candle data should be placed in this exact folder structure for the function to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five supported exchanges\n",
    "exchanges = ['bitfinex', 'coinbase_pro', 'gemini', 'hitbtc', 'kraken']\n",
    "\n",
    "# function to create pairs for arbitrage datasets\n",
    "def get_file_pairs(exchanges):\n",
    "    \"\"\"This function takes in a list of exchanges and looks through data\n",
    "        directories to find all possible combinations for 2 exchanges\n",
    "        with the same trading pair\"\"\"\n",
    "    \n",
    "    # list for filenames of ohlcv csvs\n",
    "    filenames = []\n",
    "    \n",
    "    for directory in os.listdir('all_data'):\n",
    "        # .DS_Store files can mess things up, since they aren't directories     \n",
    "        if directory != '.DS_Store':\n",
    "            \n",
    "            # for each of the files in the subdirectory\n",
    "            for filename in os.listdir('all_data/' + directory):\n",
    "                \n",
    "                # add to list of filenames if the file is a csv\n",
    "                if filename.endswith('300.csv'):\n",
    "                    filenames.append(filename)\n",
    "                    \n",
    "    # list for pairs of csvs\n",
    "    file_pairs = []\n",
    "    \n",
    "    # compare filenames to eachother and append them in a list\n",
    "    for filename_1 in filenames:\n",
    "        # filenames we haven't looped through yet\n",
    "        remaining_filenames = filenames[filenames.index(filename_1)+1:]\n",
    "        \n",
    "        # iterate through remaining filenames\n",
    "        for filename_2 in remaining_filenames:\n",
    "            \n",
    "            # iterate through exchanges\n",
    "            for exchange in exchanges:\n",
    "                # drop the exchange from the first filename and see if the\n",
    "                # remaining string is contained in the second filename\n",
    "                if filename_1.replace(exchange, '') in filename_2:\n",
    "                    # add the pair of filenames to the list of pairs\n",
    "                    file_pairs.append([filename_1, filename_2])\n",
    "                    \n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['bitfinex_eos_usdt_300.csv', 'hitbtc_eos_usdt_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'coinbase_pro_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'kraken_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'gemini_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['bitfinex_etc_usd_300.csv', 'coinbase_pro_etc_usd_300.csv'],\n",
       " ['bitfinex_etc_usd_300.csv', 'kraken_etc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'coinbase_pro_btc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'kraken_btc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'gemini_btc_usd_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'coinbase_pro_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'kraken_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'gemini_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['bitfinex_dash_usd_300.csv', 'coinbase_pro_dash_usd_300.csv'],\n",
       " ['bitfinex_dash_usd_300.csv', 'kraken_dash_usd_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'coinbase_pro_dash_btc_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'kraken_dash_btc_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'coinbase_pro_ltc_usd_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'kraken_ltc_usd_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'gemini_ltc_usd_300.csv'],\n",
       " ['bitfinex_bch_usdt_300.csv', 'hitbtc_bch_usdt_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'coinbase_pro_bch_usd_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'kraken_bch_usd_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'gemini_bch_usd_300.csv'],\n",
       " ['bitfinex_eos_usd_300.csv', 'coinbase_pro_eos_usd_300.csv'],\n",
       " ['bitfinex_eos_usd_300.csv', 'kraken_eos_usd_300.csv'],\n",
       " ['bitfinex_xrp_usd_300.csv', 'coinbase_pro_xrp_usd_300.csv'],\n",
       " ['bitfinex_xrp_usd_300.csv', 'kraken_xrp_usd_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'coinbase_pro_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'kraken_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'gemini_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_usdt_300.csv', 'hitbtc_eth_usdt_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'coinbase_pro_eth_usd_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'kraken_eth_usd_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'gemini_eth_usd_300.csv'],\n",
       " ['bitfinex_ltc_usdt_300.csv', 'hitbtc_ltc_usdt_300.csv'],\n",
       " ['bitfinex_zrx_usd_300.csv', 'coinbase_pro_zrx_usd_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'coinbase_pro_xrp_btc_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'kraken_xrp_btc_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['bitfinex_zec_usd_300.csv', 'kraken_zec_usd_300.csv'],\n",
       " ['bitfinex_zec_usd_300.csv', 'gemini_zec_usd_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'coinbase_pro_eos_btc_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'kraken_eos_btc_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['bitfinex_btc_usdt_300.csv', 'hitbtc_btc_usdt_300.csv'],\n",
       " ['coinbase_pro_eos_usd_300.csv', 'kraken_eos_usd_300.csv'],\n",
       " ['coinbase_pro_dash_btc_300.csv', 'kraken_dash_btc_300.csv'],\n",
       " ['coinbase_pro_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'kraken_eth_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'gemini_eth_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['coinbase_pro_xrp_usd_300.csv', 'kraken_xrp_usd_300.csv'],\n",
       " ['coinbase_pro_xrp_btc_300.csv', 'kraken_xrp_btc_300.csv'],\n",
       " ['coinbase_pro_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['coinbase_pro_eth_usd_300.csv', 'kraken_eth_usd_300.csv'],\n",
       " ['coinbase_pro_eth_usd_300.csv', 'gemini_eth_usd_300.csv'],\n",
       " ['coinbase_pro_dash_usd_300.csv', 'kraken_dash_usd_300.csv'],\n",
       " ['coinbase_pro_eos_btc_300.csv', 'kraken_eos_btc_300.csv'],\n",
       " ['coinbase_pro_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['coinbase_pro_eth_usdc_300.csv', 'hitbtc_eth_usdc_300.csv'],\n",
       " ['coinbase_pro_etc_usd_300.csv', 'kraken_etc_usd_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'kraken_bch_btc_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'gemini_bch_btc_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'kraken_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'gemini_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_btc_usd_300.csv', 'kraken_btc_usd_300.csv'],\n",
       " ['coinbase_pro_btc_usd_300.csv', 'gemini_btc_usd_300.csv'],\n",
       " ['coinbase_pro_btc_usdc_300.csv', 'hitbtc_btc_usdc_300.csv'],\n",
       " ['coinbase_pro_ltc_usd_300.csv', 'kraken_ltc_usd_300.csv'],\n",
       " ['coinbase_pro_ltc_usd_300.csv', 'gemini_ltc_usd_300.csv'],\n",
       " ['coinbase_pro_bch_usd_300.csv', 'kraken_bch_usd_300.csv'],\n",
       " ['coinbase_pro_bch_usd_300.csv', 'gemini_bch_usd_300.csv'],\n",
       " ['kraken_eth_btc_300.csv', 'gemini_eth_btc_300.csv'],\n",
       " ['kraken_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['kraken_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['kraken_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['kraken_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['kraken_zec_usd_300.csv', 'gemini_zec_usd_300.csv'],\n",
       " ['kraken_eth_usd_300.csv', 'gemini_eth_usd_300.csv'],\n",
       " ['kraken_btc_usd_300.csv', 'gemini_btc_usd_300.csv'],\n",
       " ['kraken_ltc_btc_300.csv', 'gemini_ltc_btc_300.csv'],\n",
       " ['kraken_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['kraken_bch_btc_300.csv', 'gemini_bch_btc_300.csv'],\n",
       " ['kraken_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['kraken_bch_usd_300.csv', 'gemini_bch_usd_300.csv'],\n",
       " ['kraken_ltc_usd_300.csv', 'gemini_ltc_usd_300.csv'],\n",
       " ['gemini_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['gemini_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['gemini_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = get_file_pairs(exchanges)\n",
    "print(len(pairs)) #95\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHLCV Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ohlcv(df, period):\n",
    "    \"\"\" Changes the time period on cryptocurrency ohlcv data.\n",
    "        Period is a string denoted by '{time_in_minutes}T'(ex: '1T', '5T', '60T').\"\"\"\n",
    "\n",
    "    # Set date as the index. This is needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "\n",
    "    # Aggregation function\n",
    "    ohlc_dict = {'open':'first',                                                                                                    \n",
    "                 'high':'max',                                                                                                       \n",
    "                 'low':'min',                                                                                                        \n",
    "                 'close': 'last',                                                                                                    \n",
    "                 'base_volume': 'sum'}\n",
    "\n",
    "    # Apply resampling\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample_ohlcv function will create NaNs in df where there were gaps in the data.\n",
    "# The gaps could be caused by exchanges being down, errors from cryptowatch or the \n",
    "# exchanges themselves\n",
    "\n",
    "def fill_nan(df):\n",
    "    \"\"\"Iterates through a dataframe and fills NaNs with appropriate \n",
    "        open, high, low, close values.\"\"\"\n",
    "\n",
    "    # Forward fill close column.\n",
    "    df['close'] = df['close'].ffill()\n",
    "\n",
    "    # Backward fill the open, high, low rows with the close value.\n",
    "    df = df.bfill(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering - before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, period='5T'):\n",
    "    \"\"\"Takes a df, engineers ta features, and returns a df\n",
    "       default period=['5T']\"\"\"\n",
    "    \n",
    "    # convert unix closing_time to datetime\n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "    \n",
    "    # time resampling to fill gaps in data\n",
    "    df = resample_ohlcv(df, period)\n",
    "    \n",
    "    # move date off the index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # create closing_time\n",
    "    closing_time = df.date.values\n",
    "    df.drop(columns='date', inplace=True)\n",
    "    \n",
    "    # create feature to indicate where rows were gaps in data\n",
    "    df['nan_ohlcv'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    # fill gaps in data\n",
    "    df = fill_nan(df)\n",
    "\n",
    "    # adding all the technical analysis features...\n",
    "    df = add_all_ta_features(df, 'open', 'high', 'low', 'close','base_volume', fillna=True)\n",
    "    \n",
    "    # add closing time column\n",
    "    df['closing_time'] = closing_time\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - after merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_closing_price(df):\n",
    "    \"\"\"returns the exchange with the higher closing price\"\"\"\n",
    "    \n",
    "    # exchange 1 has higher closing price\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 1\n",
    "    \n",
    "    # exchange 2 has higher closing price\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 2\n",
    "    \n",
    "    # closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_pct_higher(df):\n",
    "    \"\"\"returns the percentage of the difference between ex1/ex2 \n",
    "        closing prices\"\"\"\n",
    "    \n",
    "    # if exchange 1 has a higher closing price than exchange 2\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_1'] / \n",
    "                 df['close_exchange_2'])-1)*100\n",
    "    \n",
    "    # if exchange 2 has a higher closing price than exchange 1\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_2'] / \n",
    "                 df['close_exchange_1'])-1)*100\n",
    "    \n",
    "    # if closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_arbitrage_opportunity(df):\n",
    "    \"\"\"function to create column showing available arbitrage opportunities\"\"\"\n",
    "    \n",
    "    # assuming the total fees are 0.55%, if the higher closing price is less\n",
    "    # than 0.55% higher than the lower closing price...\n",
    "    if df['pct_higher'] < .55:\n",
    "        return 0 # no arbitrage\n",
    "    \n",
    "    # if exchange 1 closing price is more than 0.55% higher\n",
    "    # than the exchange 2 closing price\n",
    "    elif df['higher_closing_price'] == 1:\n",
    "        return -1 # arbitrage from exchange 2 to exchange 1\n",
    "    \n",
    "    # if exchange 2 closing price is more than 0.55% higher\n",
    "    # than the exchange 1 closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        return 1 # arbitrage from exchange 1 to exchange 2\n",
    "\n",
    "def get_window_length(df):\n",
    "    \"\"\"function to create column showing how long arbitrage opportunity has lasted\"\"\"\n",
    "    \n",
    "    # convert arbitrage_opportunity column to a list\n",
    "    target_list = df['arbitrage_opportunity'].to_list()\n",
    "    \n",
    "    # set initial window length \n",
    "    window_length = 5 # time in minutes\n",
    "    \n",
    "    # list for window_lengths\n",
    "    window_lengths = []\n",
    "    \n",
    "    # iterate through arbitrage_opportunity column\n",
    "    for i in range(len(target_list)):\n",
    "        \n",
    "        # check if a value in the arbitrage_opportunity column is equal to the\n",
    "        # previous value in the arbitrage_opportunity column and increase\n",
    "        # window length\n",
    "        if target_list[i] == target_list[i-1]:\n",
    "            window_length += 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "        # if a value in the arbitrage_opportunity column is\n",
    "        # not equal to the previous value in the arbitrage_opportunity column\n",
    "        # reset the window length to five minutes\n",
    "        else:\n",
    "            window_length = 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "    # create window length column showing how long an arbitrage opportunity has lasted\n",
    "    df['window_length'] = window_lengths\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    \"\"\"function to merge dataframes and create final features for arbitrage data\"\"\"\n",
    "    \n",
    "    # merging two modified ohlcv dfs on closing time to create arbitrage df\n",
    "    df = pd.merge(df1, df2, on='closing_time',\n",
    "                  suffixes=('_exchange_1', '_exchange_2'))\n",
    "\n",
    "    # convert closing_time to datetime\n",
    "    df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "\n",
    "    # Create additional date features.\n",
    "    df['year'] = df['closing_time'].dt.year\n",
    "    df['month'] = df['closing_time'].dt.month\n",
    "    df['day'] = df['closing_time'].dt.day\n",
    "    \n",
    "    # get higher_closing_price feature to create pct_higher feature\n",
    "    df['higher_closing_price'] = df.apply(get_higher_closing_price, axis=1)\n",
    "    \n",
    "    # get pct_higher feature to create arbitrage_opportunity feature\n",
    "    df['pct_higher'] = df.apply(get_pct_higher, axis=1)\n",
    "    \n",
    "    # create arbitrage_opportunity feature\n",
    "    df['arbitrage_opportunity'] = df.apply(get_arbitrage_opportunity, axis=1)\n",
    "    \n",
    "    # create window_length feature\n",
    "    df = get_window_length(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying arbitrage window length to target, in minutes\n",
    "interval = 30\n",
    "\n",
    "def get_target_value(df, interval=30):\n",
    "    \"\"\"function to get target values; takes df and window length to target\"\"\"\n",
    "    \n",
    "    # if the coming arbitrage window is as long as the targeted interval\n",
    "    if df['window_length_shift'] >= interval:\n",
    "        # if that window is for exchange 1 to 2\n",
    "        if df['arbitrage_opportunity_shift'] == 1:\n",
    "            return 1 # arbitrage from exchange 1 to 2\n",
    "        \n",
    "        # if that window is for exchange 2 to 1\n",
    "        elif df['arbitrage_opportunity_shift'] == -1:\n",
    "            return -1 # arbitrage from exchange 2 to 1\n",
    "        \n",
    "        # if no arbitrage opportunity\n",
    "        elif df['arbitrage_opportunity_shift'] == 0:\n",
    "            return 0 # no arbitrage opportunity\n",
    "        \n",
    "    # if the coming window is less than our targeted interval\n",
    "    else:\n",
    "        return 0 # no arbitrage opportunity\n",
    "    \n",
    "\n",
    "def get_target(df, interval=interval):\n",
    "    \"\"\"function to create target column\"\"\"\n",
    "    \n",
    "    # used to shift rows\n",
    "    # assumes candle length is five minutes, interval is 30 mins\n",
    "    rows_to_shift = int(-1*(interval/5)) # -7\n",
    "    \n",
    "    # arbitrage_opportunity feature, shifted by length of targeted interval\n",
    "    # minus one to predict ten minutes in advance rather than five\n",
    "    df['arbitrage_opportunity_shift'] = df['arbitrage_opportunity'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    \n",
    "    # window_length feature, shifted by length of targeted interval minus one\n",
    "    # to predict ten minutes\n",
    "    df['window_length_shift'] = df['window_length'].shift(rows_to_shift - 1)\n",
    "    \n",
    "    # creating target column; this will indicate if an arbitrage opportunity\n",
    "    # that lasts as long as the targeted interval is forthcoming\n",
    "    df['target'] = df.apply(get_target_value, axis=1)\n",
    "    \n",
    "    # dropping rows where target could not be calculated due to shift\n",
    "    df = df[:rows_to_shift - 1] # -7\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_close_shift(df, interval=interval):\n",
    "    \n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    return df\n",
    "\n",
    "# function to create profit feature\n",
    "def get_profit(df):\n",
    "    \"\"\"function to create profit feature\"\"\"\n",
    "    \n",
    "    # if exchange 1 has the higher closing price\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        # return how much money you would make if you bought on exchange 2, sold\n",
    "        # on exchange 1, and took account of 0.55% fees\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    \n",
    "    # if exchange 2 has the higher closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        # return how much money you would make if you bought on exchange 1, sold\n",
    "        # on exchange 2, and took account of 0.55% fees\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    \n",
    "    # if the closing prices are the same\n",
    "    else:\n",
    "        return 0 # no trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split names when in the format exchange_trading_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coinbase_pro has an extra underscore so we need a function to split it differently\n",
    "def get_exchange_trading_pair(ex_tp):\n",
    "    \n",
    "    # coinbase_pro\n",
    "    if len(ex_tp.split('_')) == 4:\n",
    "        exchange = ex_tp.split('_')[0] + '_' + ex_tp.split('_')[1]\n",
    "        trading_pair = ex_tp.split('_')[2] + '_' + ex_tp.split('_')[3]\n",
    "    \n",
    "    # all other exchanges\n",
    "    else:\n",
    "        exchange = ex_tp.split('_')[0]\n",
    "        trading_pair = ex_tp.split('_')[1] + '_' + ex_tp.split('_')[2]\n",
    "        \n",
    "    return exchange, trading_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all individual csv's with ta data (~1-2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create /ta_data directory before running this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_arbitrage_csvs(csv_filepaths):\n",
    "    \"\"\"Takes a csv filename, creates a dataframe, engineers features,\n",
    "        and saves it as a new csv in /ta_data.\"\"\"\n",
    "    n = 1\n",
    "    for file in csv_filepaths:\n",
    "        \n",
    "        # create df\n",
    "        df = pd.read_csv(file, index_col=0)[:1000]\n",
    "        \n",
    "        # define period\n",
    "        period = '5T'\n",
    "        \n",
    "        # engineer features\n",
    "        df = engineer_features(df, period)\n",
    "        print('features engineered')\n",
    "        \n",
    "        filename = 'ta_data/' + file.split('/')[1][:-4] + '_ta.csv'\n",
    "        print(filename)\n",
    "        df.to_csv(filename)\n",
    "        print(f'csv #{n} saved :)')\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_arbitrage_csvs(csv_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- create a /arb_data directory before running this function\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~2-3 hours if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arb_csvs(pairs):\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for pair in pairs:\n",
    "        \n",
    "        # define paths for the csv\n",
    "        csv_1, csv_2 = 'ta_data/' + pair[0][:-4] + '_ta.csv', 'ta_data/' + pair[1][:-4] + '_ta.csv'\n",
    "        \n",
    "        # define exchanges and trading_pairs\n",
    "        ex_tp_1, ex_tp_2 = pair[0][:-8], pair[1][:-8]\n",
    "        exchange_1, trading_pair_1 = get_exchange_trading_pair(ex_tp_1)\n",
    "        exchange_2, trading_pair_2 = get_exchange_trading_pair(ex_tp_2)\n",
    "        print(exchange_1, trading_pair_1,  exchange_2, trading_pair_2)\n",
    "        \n",
    "        # define model_name\n",
    "        model_name = exchange_1 + '_' + ex_tp_2\n",
    "        print(model_name)\n",
    "          \n",
    "        # create dfs from csv's that already include ta features\n",
    "        df1, df2 = pd.read_csv(csv_1, index_col=0), pd.read_csv(csv_2, index_col=0)       \n",
    "        print('df 1 shape: ', df1.shape, 'df 2 shape: ', df2.shape)\n",
    "        \n",
    "        # merge dfs\n",
    "        df = merge_dfs(df1, df2)\n",
    "        print('dfs merged')\n",
    "        print('merged df shape:' , df.shape)\n",
    "        \n",
    "        # create target \n",
    "        df = get_target(df)\n",
    "        print(model_name, ' ', df.shape)\n",
    "        \n",
    "        # export csv\n",
    "        path = 'arb_data/'\n",
    "        csv_filename = path + model_name + '.csv'\n",
    "        df.to_csv(csv_filename)\n",
    "        \n",
    "        counter += 1\n",
    "        print(counter, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitfinex eos_usdt hitbtc eos_usdt\n",
      "bitfinex_hitbtc_eos_usdt\n",
      "df 1 shape:  (59409, 69) df 2 shape:  (247395, 69)\n",
      "dfs merged\n",
      "merged df shape: (59409, 144)\n",
      "bitfinex_hitbtc_eos_usdt   (59402, 147)\n",
      "1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_arb_csvs(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get arbitrage data csvs into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "arb_data_paths = glob.glob('arb_data/*.csv')\n",
    "print(len(arb_data_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "- create /pickles and /arbitrage_pickles directories before running this function\n",
    "- test that this function will run to completion before running fully\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~4 hours if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(arb_data_paths):\n",
    "    \n",
    "    counter = 0\n",
    "    line = '---------------'\n",
    "    performance_list = []\n",
    "    confusion_dict = {}\n",
    "\n",
    "    # this is in case the function stops running you can pick up where you left off\n",
    "    # get all model paths into a variable\n",
    "    model_paths = glob.glob('pickles/*.pkl')\n",
    "    \n",
    "    # iterate through the arbitrage csvs\n",
    "    for file in arb_data_paths:\n",
    "        \n",
    "        # define model name\n",
    "        name = file.split('/')[1][:-8]\n",
    "        \n",
    "        # read csv\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # convert str closing_time to datetime\n",
    "        df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "        \n",
    "        print('\\n' + line*5 + '\\n' + line*2 + name.upper() + line*2 + '\\n' + line*5)\n",
    "\n",
    "        # 70/30 train/test split\n",
    "        test_train_split_row = round(len(df)*.7)\n",
    "        \n",
    "        # get closing_time for t/t split\n",
    "        test_train_split_time = df['closing_time'][test_train_split_row]\n",
    "\n",
    "        # remove 1 week from each end of the t/t datasets to create a \n",
    "        # two week gap between the data - prevents data leakage\n",
    "        train_cutoff_time = test_train_split_time - dt.timedelta(days=7)\n",
    "        test_cutoff_time = test_train_split_time + dt.timedelta(days=7)\n",
    "        print('cutoff time:', train_cutoff_time, test_cutoff_time)\n",
    "        \n",
    "        # train and test subsets\n",
    "        train = df[df['closing_time'] < train_cutoff_time]\n",
    "        test = df[df['closing_time'] > test_cutoff_time]\n",
    "        \n",
    "        # printing shapes to track progress\n",
    "        print('train and test shape: ', train.shape, test.shape)\n",
    "        \n",
    "        # pick features\n",
    "        # not using open, high, or low, which are highly correlated with close \n",
    "        # and do not improve model performance\n",
    "        features = ['close_exchange_1','base_volume_exchange_1', \n",
    "                    'nan_ohlcv_exchange_1','volume_adi_exchange_1', 'volume_obv_exchange_1',\n",
    "                    'volume_cmf_exchange_1', 'volume_fi_exchange_1','volume_em_exchange_1', \n",
    "                    'volume_vpt_exchange_1','volume_nvi_exchange_1', 'volatility_atr_exchange_1',\n",
    "                    'volatility_bbhi_exchange_1','volatility_bbli_exchange_1', \n",
    "                    'volatility_kchi_exchange_1', 'volatility_kcli_exchange_1',\n",
    "                    'volatility_dchi_exchange_1','volatility_dcli_exchange_1',\n",
    "                    'trend_macd_signal_exchange_1', 'trend_macd_diff_exchange_1', 'trend_adx_exchange_1',\n",
    "                    'trend_adx_pos_exchange_1', 'trend_adx_neg_exchange_1',\n",
    "                    'trend_vortex_ind_pos_exchange_1', 'trend_vortex_ind_neg_exchange_1', \n",
    "                    'trend_vortex_diff_exchange_1', 'trend_trix_exchange_1',\n",
    "                    'trend_mass_index_exchange_1', 'trend_cci_exchange_1',\n",
    "                    'trend_dpo_exchange_1', 'trend_kst_sig_exchange_1',\n",
    "                    'trend_kst_diff_exchange_1', 'trend_aroon_up_exchange_1',\n",
    "                    'trend_aroon_down_exchange_1',\n",
    "                    'trend_aroon_ind_exchange_1',\n",
    "                    'momentum_rsi_exchange_1', 'momentum_mfi_exchange_1',\n",
    "                    'momentum_tsi_exchange_1', 'momentum_uo_exchange_1',\n",
    "                    'momentum_stoch_signal_exchange_1',\n",
    "                    'momentum_wr_exchange_1', 'momentum_ao_exchange_1',\n",
    "                    'others_dr_exchange_1', 'close_exchange_2',\n",
    "                    'base_volume_exchange_2', 'nan_ohlcv_exchange_2',\n",
    "                    'volume_adi_exchange_2', 'volume_obv_exchange_2',\n",
    "                    'volume_cmf_exchange_2', 'volume_fi_exchange_2',\n",
    "                    'volume_em_exchange_2', 'volume_vpt_exchange_2',\n",
    "                    'volume_nvi_exchange_2', 'volatility_atr_exchange_2',\n",
    "                    'volatility_bbhi_exchange_2', \n",
    "                    'volatility_bbli_exchange_2',\n",
    "                    'volatility_kchi_exchange_2',\n",
    "                    'volatility_kcli_exchange_2',\n",
    "                    'volatility_dchi_exchange_2',\n",
    "                    'volatility_dcli_exchange_2',\n",
    "                    'trend_macd_signal_exchange_2',\n",
    "                    'trend_macd_diff_exchange_2', 'trend_adx_exchange_2',\n",
    "                    'trend_adx_pos_exchange_2', 'trend_adx_neg_exchange_2',\n",
    "                    'trend_vortex_ind_pos_exchange_2',\n",
    "                    'trend_vortex_ind_neg_exchange_2',\n",
    "                    'trend_vortex_diff_exchange_2', 'trend_trix_exchange_2',\n",
    "                    'trend_mass_index_exchange_2', 'trend_cci_exchange_2',\n",
    "                    'trend_dpo_exchange_2', 'trend_kst_sig_exchange_2',\n",
    "                    'trend_kst_diff_exchange_2', 'trend_aroon_up_exchange_2',\n",
    "                    'trend_aroon_down_exchange_2',\n",
    "                    'trend_aroon_ind_exchange_2',\n",
    "                    'momentum_rsi_exchange_2', 'momentum_mfi_exchange_2',\n",
    "                    'momentum_tsi_exchange_2', 'momentum_uo_exchange_2',\n",
    "                    'momentum_stoch_signal_exchange_2',\n",
    "                    'momentum_wr_exchange_2', 'momentum_ao_exchange_2',\n",
    "                    'others_dr_exchange_2', 'year', 'month', 'day',\n",
    "                    'higher_closing_price', 'pct_higher', \n",
    "                    'arbitrage_opportunity', 'window_length']\n",
    "        \n",
    "        # pick target\n",
    "        target = 'target'\n",
    "        \n",
    "        # X, y matrix\n",
    "        X_train = train[features]\n",
    "        X_test = test[features]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "        print('train test shapes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        \n",
    "        # filter out datasets that are too small\n",
    "        if (X_train.shape[0] > 1000) and (X_test.shape[0] > 0):\n",
    "            \n",
    "#             max_depth_list = [14] # just for testing to see if function completes\n",
    "            max_depth_list = [14, 15, 17, 18, 21, 25]\n",
    "            for max_depth in max_depth_list:\n",
    "\n",
    "#                 max_features_list = [50] # just for testing to see if function completes\n",
    "                max_features_list = [50, 55, 60, 65, 70, 75]\n",
    "                for max_features in max_features_list:\n",
    "\n",
    "#                     n_estimator_list = [100] # just for testing to see if function completes\n",
    "                    n_estimator_list = [100, 150]\n",
    "                    for n_estimators in n_estimator_list:\n",
    "\n",
    "                        # define model \n",
    "                        model_name = name + '_' + str(max_features) + '_' + str(max_depth) + '_' + str(n_estimators)\n",
    "                        print(line + model_name + line)\n",
    "\n",
    "                        # define model filename to check if it exists\n",
    "                        model_path = f'pickles/{model_name}.pkl'\n",
    "\n",
    "                        # check if the model is in the folder of models\n",
    "                        if model_path not in model_paths:\n",
    "                            \n",
    "                            # instantiate model\n",
    "                            model = RandomForestClassifier(max_features=max_features, \n",
    "                                                           max_depth=max_depth, \n",
    "                                                           n_estimators=n_estimators, \n",
    "                                                           n_jobs=-1, \n",
    "                                                           random_state=42)\n",
    "\n",
    "                            try:\n",
    "                                # fit model\n",
    "                                model = model.fit(X_train, y_train)\n",
    "                                print('model fitted!')\n",
    "\n",
    "                                # train accuracy\n",
    "                                train_score = model.score(X_train, y_train)\n",
    "                                print('train accuracy:', train_score)\n",
    "\n",
    "                                # make predictions\n",
    "                                y_preds = model.predict(X_test)\n",
    "                                print('predictions made!')\n",
    "\n",
    "                                # test accuracy\n",
    "                                score = accuracy_score(y_test, y_preds)\n",
    "                                print('test accuracy:', score)\n",
    "\n",
    "                                # save model\n",
    "                                pickle.dump(model, open('pickles/{model_name}.pkl'.format(\n",
    "                                            model_name=model_name), 'wb'))\n",
    "                                print('pickle saved!'.format(model_name=model_name))\n",
    "\n",
    "                            except:\n",
    "                                print(line*3 + '\\n' + line + 'ERROR' + line + '\\n' + line*3)\n",
    "                                break # break out of for loop if there is an error with modeling\n",
    "\n",
    "                        else: # if the model exists\n",
    "\n",
    "                            # load model\n",
    "                            model = pickle.load(open(model_path, 'rb'))\n",
    "                            print('model loaded')\n",
    "\n",
    "                            # train accuracy\n",
    "                            train_score = model.score(X_train, y_train)\n",
    "                            print('train accuracy:', train_score)\n",
    "\n",
    "                            # make predictions\n",
    "                            y_preds = model.predict(X_test)\n",
    "                            print('predictions made!')\n",
    "\n",
    "                            # test accuracy\n",
    "                            score = accuracy_score(y_test, y_preds)\n",
    "                            print('test accuracy:', score)\n",
    "                        \n",
    "                        ######## Performance metrics ########\n",
    "                        # labels for confusion matrix\n",
    "                        unique_y_test = y_test.unique().tolist()\n",
    "                        unique_y_preds = list(set(y_preds))\n",
    "                        labels = list(set(unique_y_test + unique_y_preds))\n",
    "                        labels.sort()\n",
    "                        columns = [f'Predicted {label}' for label in labels]\n",
    "                        index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "                        # create confusion matrix\n",
    "                        confusion = pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "                                                 columns=columns, index=index)\n",
    "                        print(model_name + ' confusion matrix:')\n",
    "                        print(confusion, '\\n')\n",
    "\n",
    "                        # append to confusion list\n",
    "                        confusion_dict[model_name] = confusion\n",
    "\n",
    "                        # creating dataframe from test set to calculate profitability\n",
    "                        test_with_preds = X_test.copy()\n",
    "\n",
    "                        # add column with higher closing price\n",
    "                        test_with_preds['higher_closing_price'] = test_with_preds.apply(\n",
    "                                get_higher_closing_price, axis=1)\n",
    "\n",
    "                        # add column with shifted closing price\n",
    "                        test_with_preds = get_close_shift(test_with_preds)\n",
    "\n",
    "                        # adding column with predictions\n",
    "                        test_with_preds['pred'] = y_preds\n",
    "\n",
    "                        # adding column with profitability of predictions\n",
    "                        test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "                                get_profit, axis=1).shift(-2)\n",
    "\n",
    "                        # filtering out rows where no arbitrage is predicted\n",
    "                        test_with_preds = test_with_preds[test_with_preds['pred'] != 0]\n",
    "\n",
    "                        # calculating mean profit where arbitrage predicted...\n",
    "                        pct_profit_mean = test_with_preds['pct_profit'].mean()\n",
    "\n",
    "                        # calculating median profit where arbitrage predicted...\n",
    "                        pct_profit_median = test_with_preds['pct_profit'].median()\n",
    "                        print('percent profit mean:', pct_profit_mean)\n",
    "                        print('percent profit median:', pct_profit_median, '\\n\\n')\n",
    "\n",
    "                        # save net performance to list\n",
    "                        performance_list.append([name, max_features, max_depth, n_estimators,\n",
    "                                                 pct_profit_mean, pct_profit_median])\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "        # i.e., if there are less than 1000 rows on which to train...\n",
    "        else:\n",
    "            print('{model_name}: not enough data!'.format(model_name=name))\n",
    "        \n",
    "        # update count\n",
    "        counter += 1\n",
    "        print(counter, '\\n')\n",
    "        \n",
    "    # create a dataframe for performace of all models\n",
    "    df = pd.DataFrame(performance_list, columns = ['ex_tp', 'max_features', 'max_depth', \n",
    "                                                   'n_estimators', 'pct_profit_mean','pct_profit_median'])\n",
    "    \n",
    "\n",
    "    return df, confusion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "------------------------------BITFINEX_GEMINI_BCH------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "cutoff time: 2019-10-16 02:50:00 2019-10-30 02:50:00\n",
      "train and test shape:  (6414, 147) (1596, 147)\n",
      "train test shapes: (6414, 91) (1596, 91) (6414,) (1596,)\n",
      "---------------bitfinex_gemini_bch_50_14_100---------------\n",
      "model fitted!\n",
      "train accuracy: 0.9873713751169317\n",
      "predictions made!\n",
      "test accuracy: 0.7142857142857143\n",
      "pickle saved!\n",
      "bitfinex_gemini_bch_50_14_100 confusion matrix:\n",
      "           Predicted -1  Predicted 0  Predicted 1\n",
      "Actual -1            20           53            1\n",
      "Actual 0             27          945          170\n",
      "Actual 1              0          205          175 \n",
      "\n",
      "percent profit mean: 0.4825669023900904\n",
      "percent profit median: 0.24872418149823372 \n",
      "\n",
      "\n",
      "1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df, confusion_dict = create_models(arb_data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export model performance data into csvs and JSON "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need /model_perf and /cm directories to store the performance csv's and JSON if you split up running the models on several notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting model performance to csv\n",
    "df.to_csv('model_perf/perf1.csv', index=False)\n",
    "\n",
    "# exporting confusion matrices to json\n",
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj, 'to_json'):\n",
    "            return obj.to_json(orient='records')\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "with open('cm/confusion1.json', 'w') as fp:\n",
    "    json.dump(confusion_dict, fp, cls=JSONEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate performance and confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_perf_dfs(filepaths):\n",
    "    \n",
    "    df_list = []\n",
    "    for path in filepaths:\n",
    "        df = pd.read_csv(path)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df = pd.concat(df_list)\n",
    "    df = df.sort_values(by='pct_profit_mean', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def concat_dicts(filepaths):\n",
    "    \n",
    "    confusion_dict = {}\n",
    "    \n",
    "    for path in filepaths:\n",
    "        confusion = json.load(open(path))\n",
    "        confusion_dict.update(confusion)\n",
    "    \n",
    "    return confusion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check the number of files you have in each folder\n",
    "# if you ran on 4 notebooks you should have 4\n",
    "\n",
    "perf_csv_paths = glob.glob('model_perf/*.csv')\n",
    "confusion_paths = glob.glob('cm/*.json')\n",
    "\n",
    "print(len(perf_csv_paths))\n",
    "print(len(confusion_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# concatenate all confusion matrices\n",
    "confusion_dict = concat_dicts(confusion_paths)\n",
    "\n",
    "# concatenate all performance dataframes\n",
    "perf_df = concat_perf_dfs(perf_csv_paths)\n",
    "\n",
    "print(len(confusion_dict.keys()))\n",
    "print(len(perf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at performance dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ex_tp</th>\n",
       "      <th>max_features</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>pct_profit_mean</th>\n",
       "      <th>pct_profit_median</th>\n",
       "      <th>correct_arb</th>\n",
       "      <th>pct_wrong_0</th>\n",
       "      <th>pct_wrong_1</th>\n",
       "      <th>pct_wrong_neg1</th>\n",
       "      <th>correct_arb_neg1</th>\n",
       "      <th>correct_arb_1</th>\n",
       "      <th>correct_arb_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>150</td>\n",
       "      <td>9.692778</td>\n",
       "      <td>11.240564</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>100</td>\n",
       "      <td>9.583432</td>\n",
       "      <td>11.099366</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>9.572652</td>\n",
       "      <td>11.296690</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>150</td>\n",
       "      <td>9.275228</td>\n",
       "      <td>10.927262</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28972.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>coinbase_pro_hitbtc_bch_btc</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>9.252052</td>\n",
       "      <td>11.141901</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6379</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6381</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6382</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6383</td>\n",
       "      <td>coinbase_pro_kraken_ltc_usd</td>\n",
       "      <td>75</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6384 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ex_tp  max_features  max_depth  n_estimators  \\\n",
       "0     coinbase_pro_hitbtc_bch_btc            60         17           150   \n",
       "1     coinbase_pro_hitbtc_bch_btc            60         17           100   \n",
       "2     coinbase_pro_hitbtc_bch_btc            50         14           150   \n",
       "3     coinbase_pro_hitbtc_bch_btc            60         14           150   \n",
       "4     coinbase_pro_hitbtc_bch_btc            60         14           100   \n",
       "...                           ...           ...        ...           ...   \n",
       "6379  coinbase_pro_kraken_ltc_usd            65         25           150   \n",
       "6380  coinbase_pro_kraken_ltc_usd            70         25           100   \n",
       "6381  coinbase_pro_kraken_ltc_usd            70         25           150   \n",
       "6382  coinbase_pro_kraken_ltc_usd            75         25           100   \n",
       "6383  coinbase_pro_kraken_ltc_usd            75         25           150   \n",
       "\n",
       "      pct_profit_mean  pct_profit_median  correct_arb  pct_wrong_0  \\\n",
       "0            9.692778          11.240564         60.0     0.000862   \n",
       "1            9.583432          11.099366         54.0     0.001069   \n",
       "2            9.572652          11.296690         58.0     0.000931   \n",
       "3            9.275228          10.927262         64.0     0.000724   \n",
       "4            9.252052          11.141901         63.0     0.000759   \n",
       "...               ...                ...          ...          ...   \n",
       "6379              NaN                NaN          0.0     0.000000   \n",
       "6380              NaN                NaN          0.0     0.000000   \n",
       "6381              NaN                NaN          0.0     0.000000   \n",
       "6382              NaN                NaN          0.0     0.000000   \n",
       "6383              NaN                NaN          0.0     0.000000   \n",
       "\n",
       "      pct_wrong_1  pct_wrong_neg1  correct_arb_neg1  correct_arb_1  \\\n",
       "0             NaN        0.062500              60.0            0.0   \n",
       "1             NaN        0.052632              54.0            0.0   \n",
       "2             NaN        0.079365              58.0            0.0   \n",
       "3             NaN        0.085714              64.0            0.0   \n",
       "4             NaN        0.100000              63.0            0.0   \n",
       "...           ...             ...               ...            ...   \n",
       "6379          0.0        0.000000               0.0            0.0   \n",
       "6380          0.0        0.000000               0.0            0.0   \n",
       "6381          0.0        0.000000               0.0            0.0   \n",
       "6382          0.0        0.000000               0.0            0.0   \n",
       "6383          0.0        0.000000               0.0            0.0   \n",
       "\n",
       "      correct_arb_0  \n",
       "0           28974.0  \n",
       "1           28975.0  \n",
       "2           28973.0  \n",
       "3           28972.0  \n",
       "4           28971.0  \n",
       "...             ...  \n",
       "6379            0.0  \n",
       "6380            0.0  \n",
       "6381            0.0  \n",
       "6382            0.0  \n",
       "6383            0.0  \n",
       "\n",
       "[6384 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### look at confusion dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bitfinex_gemini_bch_50_14_100': '[{\"Predicted -1\":20,\"Predicted 0\":53,\"Predicted 1\":1},{\"Predicted -1\":27,\"Predicted 0\":945,\"Predicted 1\":170},{\"Predicted -1\":0,\"Predicted 0\":205,\"Predicted 1\":175}]'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for creating df with pnl and confusion matrix features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_confusion(df, confusion_dict):\n",
    "    \n",
    "    # create a copy of df to not overwrite original\n",
    "    df = df.copy()\n",
    "    \n",
    "    line = '-------'\n",
    "    feature_dict = {}\n",
    "    model_name_list = []\n",
    "    \n",
    "    # iterate through all models\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        # define model name\n",
    "        model_name = (df.ex_tp.iloc[i] + '_' + str(df.max_features.iloc[i]) \n",
    "                      + '_' + str(df.max_depth.iloc[i]) + '_' + str(df.n_estimators.iloc[i]))\n",
    "        model_name_list.append(model_name)\n",
    "        \n",
    "        # create confusion matrix for specific model\n",
    "        conf_mat = pd.read_json(confusion_dict[model_name])\n",
    "\n",
    "        #########################################################\n",
    "        ############## create confusion features ################\n",
    "        #########################################################\n",
    "        \n",
    "        # confusion matrix has -1, 0, 1 predictions\n",
    "        if 'Predicted 1' in conf_mat.columns and 'Predicted -1' in conf_mat.columns:\n",
    "\n",
    "            # % incorrect predictions for 0, 1, -1\n",
    "            pct_wrong_0 = (conf_mat['Predicted 0'].loc[0] + \n",
    "                           conf_mat['Predicted 0'].loc[2])/conf_mat['Predicted 0'].sum()\n",
    "            pct_wrong_1 = (conf_mat['Predicted 1'].loc[0] + \n",
    "                           conf_mat['Predicted 1'].loc[1])/conf_mat['Predicted 1'].sum()\n",
    "            pct_wrong_neg1 = (conf_mat['Predicted -1'].loc[1] + \n",
    "                               conf_mat['Predicted -1'].loc[2])/conf_mat['Predicted -1'].sum()\n",
    "\n",
    "            # total number correct arbitrage preds (-1)\n",
    "            correct_arb_neg1 = conf_mat['Predicted -1'].loc[0]\n",
    "\n",
    "            # total number correct arbitrage preds (1)\n",
    "            correct_arb_1 = conf_mat['Predicted 1'].loc[2]\n",
    "            \n",
    "            # total number correct arbitrage preds (-1) + (1)\n",
    "            correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "\n",
    "            # total number correct no arbitrage preds (0)\n",
    "            correct_arb_0 = conf_mat['Predicted 0'].loc[1]\n",
    "\n",
    "        # confusion matrix has 0, 1 predictions\n",
    "        elif 'Predicted 1' in conf_mat.columns:\n",
    "\n",
    "            pct_wrong_0 = conf_mat['Predicted 0'].loc[1] / conf_mat['Predicted 0'].sum()\n",
    "            pct_wrong_1 = conf_mat['Predicted 1'].loc[0] / conf_mat['Predicted 1'].sum()\n",
    "            pct_wrong_neg1 = np.nan\n",
    "\n",
    "            # total number correct arbitrage preds (-1)\n",
    "            correct_arb_neg1 = 0\n",
    "\n",
    "            # total number correct arbitrage preds (1)\n",
    "            correct_arb_1 = conf_mat['Predicted 1'].loc[1]\n",
    "            \n",
    "            # total number correct arbitrage preds (-1) + (1)\n",
    "            correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "\n",
    "            # total number correct no arbitrage preds (0)\n",
    "            correct_arb_0 = conf_mat['Predicted 0'].loc[0]\n",
    "        \n",
    "        # confusion matrix has -1, 0 predictions\n",
    "        elif 'Predicted -1' in conf_mat.columns:\n",
    "\n",
    "            pct_wrong_0 = conf_mat['Predicted 0'].loc[0] / conf_mat['Predicted 0'].sum()\n",
    "            pct_wrong_1 = np.nan\n",
    "            pct_wrong_neg1 = conf_mat['Predicted -1'].loc[1] / conf_mat['Predicted -1'].sum()\n",
    "\n",
    "            # total number correct arbitrage preds (-1)\n",
    "            correct_arb_neg1 = conf_mat['Predicted -1'].loc[0]\n",
    "\n",
    "            # total number correct arbitrage preds (1)\n",
    "            correct_arb_1 = 0\n",
    "            \n",
    "            # total number correct arbitrage preds (-1) + (1)\n",
    "            correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "\n",
    "            # total number correct no arbitrage preds (0)\n",
    "            correct_arb_0 = conf_mat['Predicted 0'].loc[1]\n",
    "        \n",
    "        # confusion matrix has only 0\n",
    "        else:\n",
    "            pct_wrong_0 = 0\n",
    "            pct_wrong_1 = 0\n",
    "            pct_wrong_neg1 = 0\n",
    "            correct_arb = 0\n",
    "            correct_arb_neg1 = 0\n",
    "            correct_arb_1 = 0\n",
    "            correct_arb_0 = 0\n",
    "\n",
    "        \n",
    "        # add confusion features to dict\n",
    "        feature_list = [correct_arb, pct_wrong_0, pct_wrong_1, pct_wrong_neg1, \n",
    "                        correct_arb_neg1, correct_arb_1, correct_arb_0]\n",
    "        feature_dict[model_name] = feature_list\n",
    "\n",
    "    # create a df from the new features\n",
    "    columns = ['correct_arb', 'pct_wrong_0', 'pct_wrong_1', 'pct_wrong_neg1', \n",
    "                'correct_arb_neg1', 'correct_arb_1', 'correct_arb_0']\n",
    "    df2 = pd.DataFrame(feature_dict).transpose().reset_index()\n",
    "    df2 = df2.rename(columns = {'index': 'model_name', 0: 'correct_arb', 1:'pct_wrong_0', \n",
    "                                2: 'pct_wrong_1', 3: 'pct_wrong_neg1', \n",
    "                                4: 'correct_arb_neg1', 5: 'correct_arb_1', \n",
    "                                6: 'correct_arb_0'})\n",
    "    \n",
    "    # merge new features with performance df\n",
    "    df['model_name'] = model_name_list\n",
    "    print(df.shape, df2.shape)\n",
    "    df = df.merge(df2, on='model_name').drop(columns = 'model_name')\n",
    "    print('shape after merge:', df.shape)\n",
    "\n",
    "    # filter for models that are predicting arb when its not happening < 15% of the time\n",
    "    df2 = df[df['pct_wrong_0'] < 0.15]\n",
    "    print('shape after filetering pct_wrong_0:', df.shape)\n",
    "\n",
    "    # filter for models that predict > 25 correct arb \n",
    "    df2 = df2[df2['correct_arb'] > 25]\n",
    "    print('shape after filtering correct_arb:', df.shape)\n",
    "    \n",
    "    # filter for models that make > 0.20% profit\n",
    "    df2 = df2[df2['pct_profit_mean'] > 0.2]\n",
    "    print('shape after filtering pct_profit_mean:', df.shape)\n",
    "    \n",
    "    df2 = df2.sort_values(by=['correct_arb'], ascending=False)\n",
    "    df2 = df2.drop_duplicates(subset='ex_tp')\n",
    "    \n",
    "    print('shape after droping duplicates:', df.shape)\n",
    "    \n",
    "    # return 2 dataframes\n",
    "    # df1 has performance for all models\n",
    "    # df2 has performance for filtered models\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df2 = model_confusion(perf_df, confusion_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for models that are predicting arb when its not happening < 15% of the time\n",
    "df3 = df[df['pct_wrong_0'] < 0.30]\n",
    "print('shape after filetering pct_wrong_0:', df3.shape)\n",
    "\n",
    "# filter for models that predict > 25 correct arb \n",
    "df3 = df3[df3['correct_arb'] > 25]\n",
    "print('shape after filtering correct_arb:', df3.shape)\n",
    "\n",
    "# filter for models that make > 0.20% profit\n",
    "df3 = df3[df3['pct_profit_mean'] > 0.2]\n",
    "print('shape after filtering pct_profit_mean:', df3.shape)\n",
    "\n",
    "df3 = df3.sort_values(by=['correct_arb'], ascending=False)\n",
    "df3 = df3.drop_duplicates(subset='ex_tp')\n",
    "\n",
    "print('shape after droping duplicates:', df3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move best models into a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = glob.glob('arb_pickles/*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info for model names\n",
    "models = df3['ex_tp'].values[1:]\n",
    "max_features = df3['max_features'].values[1:]\n",
    "max_depth = df3['max_depth'].values[1:]\n",
    "n_estimators = df3['n_estimators'].values[1:]\n",
    "\n",
    "print(len(models), len(max_features), len(max_depth), len(n_estimators))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    \n",
    "    # define model name\n",
    "    model_name = models[i] + '_' + str(max_features[i]) + '_' + str(max_depth[i]) + '_' + str(n_estimators[i])\n",
    "    \n",
    "    # rename the filepath to move\n",
    "    os.rename(f'pickles/{model_name}.pkl', f'arb_pickles/{models[i]}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic",
   "language": "python",
   "name": "cryptolytic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
