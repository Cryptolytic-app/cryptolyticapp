{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptolytic Data Processing\n",
    "\n",
    "This notebook contains the code to generate the data that is used to create the arbitrage models in this [notebook](https://github.com/Cryptolytic-app/cryptolyticapp/blob/master/modeling/2_arbitrage_model_training.ipynb).\n",
    "\n",
    "<img src=\"https://github.com/Cryptolytic-app/cryptolyticapp/blob/master/assets/cryptolytic_thumbnail.png?raw=true\"\n",
    "     alt=\"drawing\"\n",
    "     width=\"500\"/>\n",
    "     \n",
    "#### What is arbitrage?\n",
    "Arbitrage occurs when there is a price difference between the same asset in two different markets. So with crypto, it’s possible to have the same coin priced differently on separate exchanges. For example with bitcoin, you might have bitcoin priced at &#0036;8,000 on one exchange, and at the same time that bitcoin can be priced at &#0036;8,100 on another exchange. You can buy the bitcoin on the first exchange for &#0036;8,000, send it to the other exchange, and sell it for &#0036;8,100. Now you’ve made &#0036;100 in profit and you can repeat this process as long as that arbitrage opportunity lasts.\n",
    "\n",
    "#### Background on arbitrage models\n",
    "There are many different combinations of arbitrage that could be occuring at any given moment among all the cryptocurrency exchanges. Our goal was to capture as many of these as possible in order to create an API that provides predictions for any arbitrage opportunities that will occur in the next 10 mins. This API could then serve as the backend for a web application that displays the predictions in a more user-friendly format.\n",
    "\n",
    "The arbitrage models in this notebook predict arbitrage 10 min before it happens, lasting for at least 30 mins. It's important that the arbitrage window lasts long enough because it takes time to move coins from one exchange to the other in order to successfully complete the arbitrage trades. The datasets used for modeling are generated by getting all of the combinations of 2 exchanges that support the same trading pair, engineering technical analysis features, merging that data on 'closing_time', engineering more features, and creating a target that signals an arbitrage opportunity. Arbitrage signals predicted by the models have a direction indicating which direction the arbitrage occurs in.\n",
    "\n",
    "#### What kind of machine learning problem is this?\n",
    "Arbitrage can occur in two directions, one from the the first exchange to the second and vice versa, so there are 3 possible classes for a target which makes this a multiclass classification problem.\n",
    "\n",
    "#### Where does the data come from?\n",
    "Data for this project was obtained through the APIs of each exchange.\n",
    "\n",
    "<img src=\"https://github.com/Cryptolytic-app/cryptolyticapp/blob/master/modeling/assets/exchange_logos.png?raw=true\"\n",
    "     alt= \"drawing\"\n",
    "     width=\"1000\"/>\n",
    "     \n",
    "- [Bitfinex API OHLCV Data Documentation](https://docs.bitfinex.com/reference#rest-public-candles)\n",
    "- [Coinbase Pro API OHLCV Data Documentation](https://docs.pro.coinbase.com/?r=1#get-historic-rates)\n",
    "- [HitBTC OHLCV Data Documentation](https://api.hitbtc.com/#candles)\n",
    "- [Kraken OHLCV Data Documentation](https://www.kraken.com/features/api)\n",
    "- [Gemini OHLCV Data Documentation](https://docs.gemini.com/rest-api/)\n",
    "\n",
    "The functions to collect the 5 min and 1 hour candlestick data can be found [here](https://github.com/Cryptolytic-app/cryptolyticapp/tree/master/data_collection_and_databasing).\n",
    "\n",
    "\n",
    "#### What does the raw data look like?\n",
    "<img src=\"https://github.com/Cryptolytic-app/cryptolyticapp/blob/master/modeling/assets/sample_df.png?raw=true\"\n",
    "     alt=\"drawing\"\n",
    "     width=\"400\"/>\n",
    "     \n",
    "#### Data Dictionary\n",
    "- **closing_time:** the closing time of the candlestick\n",
    "- **open:** the price of the cryptocurrency at the opening of the candlestick\n",
    "- **high:** the highest price of the cryptocurrency during that candlestick\n",
    "- **low:** the lowest price of the cryptocurrency during that candlestick\n",
    "- **close:** the price of the cryptocurrency at the end of the candlestick\n",
    "- **volume:** the volume traded during that candlestick\n",
    "\n",
    "\n",
    "#### Features Engineered\n",
    "67 technical analysis features were engineered with the [Technical Analysis Library](https://github.com/bukosabino/ta). They fall into five categories:\n",
    "- Momentum indicators\n",
    "- Volume indicators\n",
    "- Volatility indicators\n",
    "- Trend indicators\n",
    "- Others indicators\n",
    "\n",
    "#### Merging Datasets\n",
    "Since there are no extensive arbitrage datasets available covering all cryptocurrencies trading at major exchanges, we had to create our own by merging datasets that included the same trading pair at 2 exchanges and then create the target variable with more feature engineering. \n",
    "\n",
    "First, we genererated all of the possible arbitrage combinations between the 80 datasets that we collected through the exchange APIs and this resulted in 95 possible combinations of arbitrage datasets that could be used in modeling.\n",
    "\n",
    "For each possible arbitrage combination, we merged the two datasets on 'closing_time' and created new features that identified arbitrage opportunities. These features included:\n",
    "- **higher_closing_price:** identifies the exchange that has the higher closing price (1 or 2)\n",
    "- **pct_higher:** the percentage by which higher_closing_price is greater\n",
    "- **arbitrage_opportunity:** identfies if there is greater than 0.55% gain (to account for fees)\n",
    "    - 1: arbitrage from exchange 1 to exchange 2\n",
    "    - 0: no arbitrage\n",
    "    - -1: arbitrage from exchange 2 to exchnage 1\n",
    "- **close_exchange_1_shift:** shifts the exchange 1 close price to account for a 30 min trading interval + 10 min advance prediction\n",
    "- **close_exchange_2_shift:** shifts the exchange 2 close price to account for a 30 min trading interval + 10 min advance prediction\n",
    "- **window_length:** gets the window length of the arbitrage opportunity\n",
    "- **window_length_shift:** shifts the window length by the 30 min trading interval - 10 mins\n",
    "- **arbitrage_opportunity_shift:** shifts the arbitrage opportunity the the 30 min trading interval - 10 mins\n",
    "\n",
    "Finally, we created the **target** using those features:\n",
    "\n",
    "- 1: arbitrage from exchange 1 to exchange 2 starting 10 mins after prediction time, lasting 30 mins\n",
    "- 0: no arbitrage 10 mins after prediction time\n",
    "- -1: arbitrage from exchange 2 to exchnage 1 starting 10 mins after prediction time, lasting 30 mins\n",
    "    \n",
    "#### Class distribution\n",
    "Since arbitrage is not always occuring, it's expected that target classes in these datasets will be imbalanced. Only datasets that contain the classes for arbitrage occuring in both directions would be suitable for machine learning so we filtered the 95 possible datasets down to the ones that contain at least 5% of each class. This resulted in 15 datasets that could be used for training models:\n",
    "\n",
    "<img src=\"https://github.com/Cryptolytic-app/cryptolyticapp/blob/master/modeling/assets/class_dist.png?raw=true\"\n",
    "     alt=\"drawing\"\n",
    "     width=\"400\"/>\n",
    "     \n",
    "\n",
    "#### Running this notebook \n",
    "Runtime approx. 12 hours\n",
    "\n",
    "Data exported: 26gb\n",
    "\n",
    "- 80 csv files in `data/raw_data/` (641 MB)\n",
    "- 80 csv files in `data/ta_data/` (12.943 GB)\n",
    "- 95 csv files in `data/arb_data/` (12.861 GB)\n",
    "- 1 csv file in `data/` (4KB)\n",
    "- 1 txt file in `data/` (4KB)\n",
    "\n",
    "\n",
    "Notes:\n",
    "- It is HIGHLY reccommended to use AWS Sagemaker to do feature engineering and split the work onto four notebooks. This will cut down the time to about 3 hours. You can do this by list slicing the filepaths of the datasets you are inputting into the functions (for example: `create_ta_csvs(csv_filepaths[:20]`)\n",
    "- Since feature engineering takes a long time, we export data as csvs along each step to not have to re-engineer features in case the runtime restarts: one export after technical analysis features are added, and another export after datasets are merged and more arbitrage features are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory Structure\n",
    "```\n",
    "├── cryptolytic/                        <-- Root directory   \n",
    "│   ├── modeling/                       <-- Directory for modeling work\n",
    "│   │      │\n",
    "│   │      ├──assets/                   <-- Directory with png assets used in notebooks\n",
    "│   │      │\n",
    "│   │      ├──data/                     <-- Directory containing all data for project\n",
    "│   │      │   ├─ arb_data/             <-- Directory for train data after merging + FE pt.2\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ arb_preds_test_data/  <-- Directory for test data w/ predictions\n",
    "│   │      │   │   └── *.csv \n",
    "│   │      │   │\n",
    "│   │      │   ├─ arb_top_data/         <-- Directory for data from the best models\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ raw_data/             <-- Directory for raw training data\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ ta_data/              <-- Directory for csv files after FE pt.1 \n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ raw_zip_data/         <-- Directory containing zip files of raw data\n",
    "│   │      │   │   └── *.zip\n",
    "│   │      │   │\n",
    "│   │      │   ├─ all_features.txt      <-- All features used in baseline models\n",
    "│   │      │   │\n",
    "│   │      │   ├─ top_features.txt      <-- Most important features for models\n",
    "│   │      │   │\n",
    "│   │      │   ├─ model_perf.csv        <-- Data from training baseline models and tuning\n",
    "│   │      │   │\n",
    "│   │      │   ├─ top_model_perf.csv    <-- Data from retraining and exporting best models\n",
    "│   │      │\n",
    "│   │      ├── models/                  <-- Directory for all pickle models\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├─ 1_arbitrage_data_processing.ipynb  <-- NB for data processing and creating csv\n",
    "│   │      │\n",
    "│   │      ├─ 2_arbitrage_modeling.ipynb         <-- NB for baseline models and tuning\n",
    "│   │      │\n",
    "│   │      ├─ 3_arbitrage_model_evaluation.ipynb <-- NB for model selection, eval, and viz\n",
    "│   │      │\n",
    "│   │      ├─ trade_recommender_models.ipynb     <-- NB for trade recommender models\n",
    "│   │      │\n",
    "│   │      ├─ environment.yml                    <-- Contains project dependencies\n",
    "│   │      │\n",
    "│   │      ├─ utils.py                           <-- All the functions used in modeling\n",
    "│   │      │\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses conda to manage environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to update your conda env from a yml file from terminal\n",
    "# conda env update --file modeling/environment.yml\n",
    "\n",
    "# to export yml from terminal\n",
    "# conda env export > modeling/environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import itertools\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_rows', 100000)\n",
    "\n",
    "from ta import add_all_ta_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this section we'll be unzipping the files that contain the 5 minute candle data, saving them to a new directory `raw_data/`, and renaming the filepaths that contain coinbase_pro to cbpro (the underscore causes problems with naming conventions later). You must start with the zip data in the `data/zip_raw_data/` directory which can be found on [Github](https://github.com/Cryptolytic-app/cryptolyticapp/tree/master/modeling/data/zip_raw_data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_filepaths = glob.glob('data/zip_raw_data/*300.zip')\n",
    "len(zip_filepaths) #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS TO UNZIP\n",
    "\n",
    "# for zip_filepath in zip_filepaths:\n",
    "#     with ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('data/csv_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepaths = glob.glob('data/raw_data/*.csv')\n",
    "len(csv_filepaths) #80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS TO RENAME\n",
    "\n",
    "# for filepath in csv_filepaths:\n",
    "#     new_filepath = filepath.replace('coinbase_pro', 'cbpro')\n",
    "#     os.rename(filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw datasets look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closing_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>base_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1417990200</td>\n",
       "      <td>3.7400</td>\n",
       "      <td>3.7400</td>\n",
       "      <td>3.7380</td>\n",
       "      <td>3.7380</td>\n",
       "      <td>26.698132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1417990500</td>\n",
       "      <td>3.7254</td>\n",
       "      <td>3.7254</td>\n",
       "      <td>3.7250</td>\n",
       "      <td>3.7250</td>\n",
       "      <td>6.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1417990800</td>\n",
       "      <td>3.7221</td>\n",
       "      <td>3.7221</td>\n",
       "      <td>3.7221</td>\n",
       "      <td>3.7221</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1417991100</td>\n",
       "      <td>3.7161</td>\n",
       "      <td>3.7161</td>\n",
       "      <td>3.7161</td>\n",
       "      <td>3.7161</td>\n",
       "      <td>1.061168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1417991700</td>\n",
       "      <td>3.7220</td>\n",
       "      <td>3.7220</td>\n",
       "      <td>3.7220</td>\n",
       "      <td>3.7220</td>\n",
       "      <td>4.650807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   closing_time    open    high     low   close  base_volume\n",
       "0    1417990200  3.7400  3.7400  3.7380  3.7380    26.698132\n",
       "1    1417990500  3.7254  3.7254  3.7250  3.7250     6.469000\n",
       "2    1417990800  3.7221  3.7221  3.7221  3.7221    15.000000\n",
       "3    1417991100  3.7161  3.7161  3.7161  3.7161     1.061168\n",
       "4    1417991700  3.7220  3.7220  3.7220  3.7220     4.650807"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/raw_data/bitfinex_ltc_usd_300.csv', index_col=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And we want to transform it into something that looks like this for modeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_exchange_1</th>\n",
       "      <th>high_exchange_1</th>\n",
       "      <th>low_exchange_1</th>\n",
       "      <th>close_exchange_1</th>\n",
       "      <th>base_volume_exchange_1</th>\n",
       "      <th>nan_ohlcv_exchange_1</th>\n",
       "      <th>volume_adi_exchange_1</th>\n",
       "      <th>volume_obv_exchange_1</th>\n",
       "      <th>volume_cmf_exchange_1</th>\n",
       "      <th>volume_fi_exchange_1</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>higher_closing_price</th>\n",
       "      <th>pct_higher</th>\n",
       "      <th>arbitrage_opportunity</th>\n",
       "      <th>window_length</th>\n",
       "      <th>arbitrage_opportunity_shift</th>\n",
       "      <th>window_length_shift</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.605746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.064067</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.064067</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.064067</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.064067</td>\n",
       "      <td>-1</td>\n",
       "      <td>20</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.064067</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   open_exchange_1  high_exchange_1  low_exchange_1  close_exchange_1  \\\n",
       "0              3.7              3.7             3.7               3.7   \n",
       "1              3.7              3.7             3.7               3.7   \n",
       "2              3.7              3.7             3.7               3.7   \n",
       "3              3.7              3.7             3.7               3.7   \n",
       "4              3.7              3.7             3.7               3.7   \n",
       "\n",
       "   base_volume_exchange_1  nan_ohlcv_exchange_1  volume_adi_exchange_1  \\\n",
       "0                     5.0                   0.0              19.605746   \n",
       "1                     5.0                   0.0               0.000000   \n",
       "2                     0.0                   1.0               0.000000   \n",
       "3                     0.0                   1.0               0.000000   \n",
       "4                     0.0                   1.0               0.000000   \n",
       "\n",
       "   volume_obv_exchange_1  volume_cmf_exchange_1  volume_fi_exchange_1  ...  \\\n",
       "0                    0.0                    0.0                   0.0  ...   \n",
       "1                    0.0                    0.0                   0.0  ...   \n",
       "2                    0.0                    0.0                  -0.0  ...   \n",
       "3                    0.0                    0.0                  -0.0  ...   \n",
       "4                    0.0                    0.0                   0.0  ...   \n",
       "\n",
       "   year  month  day  higher_closing_price  pct_higher  arbitrage_opportunity  \\\n",
       "0  2016      8   17                     1    3.064067                     -1   \n",
       "1  2016      8   17                     1    3.064067                     -1   \n",
       "2  2016      8   17                     1    3.064067                     -1   \n",
       "3  2016      8   17                     1    3.064067                     -1   \n",
       "4  2016      8   17                     1    3.064067                     -1   \n",
       "\n",
       "   window_length  arbitrage_opportunity_shift  window_length_shift  target  \n",
       "0              5                         -1.0                 40.0      -1  \n",
       "1             10                         -1.0                 45.0      -1  \n",
       "2             15                         -1.0                 50.0      -1  \n",
       "3             20                         -1.0                 55.0      -1  \n",
       "4             25                         -1.0                 60.0      -1  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/arb_data/cbpro_bitfinex_ltc_usd.csv', index_col=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arbitrage Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create pairs for arbitrage datasets\n",
    "def get_file_pairs(filenames):\n",
    "    \"\"\"\n",
    "    This function takes in a list of exchanges and looks through data\n",
    "    directories to find all possible combinations for 2 exchanges\n",
    "    with the same trading pair. Returns a list of all lists that\n",
    "    include the file pairs\n",
    "    \"\"\"\n",
    "    # get combinations\n",
    "    combos = list(itertools.combinations(filenames, 2))\n",
    "    \n",
    "    # remove unmatched trading pairs\n",
    "    filtered_combos = []\n",
    "    for combo in combos:\n",
    "        tp1 = '_'.join(combo[0].split('/')[2].split('_')[1:3])\n",
    "        tp2 = '_'.join(combo[1].split('/')[2].split('_')[1:3])\n",
    "        if tp1 == tp2:\n",
    "            filtered_combos.append(combo)\n",
    "                    \n",
    "    return filtered_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "pairs = get_file_pairs(csv_filepaths)\n",
    "print(len(pairs)) # 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHLCV Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ohlcv(df, period):\n",
    "    \"\"\" \n",
    "    Changes the time period on cryptocurrency ohlcv data.\n",
    "    Period is a string denoted by '{time_in_minutes}T'\n",
    "    (ex: '1T', '5T', '60T').\n",
    "    \"\"\"\n",
    "\n",
    "    # set date as index\n",
    "    # needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "\n",
    "    # aggregation function\n",
    "    ohlc_dict = {\n",
    "        'open':'first',                                                                                                    \n",
    "        'high':'max',                                                                                                       \n",
    "        'low':'min',                                                                                                        \n",
    "        'close': 'last',                                                                                                    \n",
    "        'base_volume': 'sum'\n",
    "    }\n",
    "\n",
    "    # resample\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NaNs\n",
    "\n",
    "Resample_ohlcv function will create NaNs in df where there were gaps in the data. The gaps could be caused by exchanges being down, errors from cryptowatch or errors from the exchanges themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(df):\n",
    "    \"\"\"\n",
    "    Iterates through a dataframe and fills NaNs with appropriate \n",
    "    open, high, low, close values.\n",
    "    \"\"\"\n",
    "    # Forward fill close column\n",
    "    df['close'] = df['close'].ffill()\n",
    "\n",
    "    # Backward fill the open, high, low rows with the close value\n",
    "    df = df.bfill(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering - before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, period='5T'):\n",
    "    \"\"\"\n",
    "    Takes a df, engineers ta features, and returns a df\n",
    "    with period=['5T']\n",
    "    \"\"\"\n",
    "    # convert unix closing_time to datetime\n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "    \n",
    "    # time resampling to fill gaps in data\n",
    "    df = resample_ohlcv(df, period)\n",
    "    \n",
    "    # move date off the index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # create closing_time\n",
    "    closing_time = df.date.values\n",
    "    df.drop(columns='date', inplace=True)\n",
    "    \n",
    "    # create feature to indicate where rows were gaps in data\n",
    "    df['nan_ohlcv'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    # fill gaps in data\n",
    "    df = fill_nan(df)\n",
    "\n",
    "    # adding all the technical analysis features...\n",
    "    df = add_all_ta_features(df, 'open', 'high', 'low', 'close','base_volume', fillna=True)\n",
    "    \n",
    "    # add closing time column\n",
    "    df['closing_time'] = closing_time\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - after merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_closing_price(df):\n",
    "    \"\"\"\n",
    "    Returns the exchange with the higher closing price\n",
    "    \"\"\"\n",
    "    \n",
    "    # exchange 1 has higher closing price\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 1\n",
    "    \n",
    "    # exchange 2 has higher closing price\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 2\n",
    "    \n",
    "    # closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_pct_higher(df):\n",
    "    \"\"\"\n",
    "    Returns the percentage of the difference between ex1/ex2 \n",
    "    closing prices\n",
    "    \"\"\"\n",
    "    \n",
    "    # if exchange 1 has a higher closing price than exchange 2\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_1'] / \n",
    "                 df['close_exchange_2'])-1)*100\n",
    "    \n",
    "    # if exchange 2 has a higher closing price than exchange 1\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_2'] / \n",
    "                 df['close_exchange_1'])-1)*100\n",
    "    \n",
    "    # if closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_arb_opportunity(df):\n",
    "    \"\"\"\n",
    "    Return available arbitrage opportunities\n",
    "    \n",
    "    1: arbitrage from exchange 1 to exchange 2\n",
    "    0: no arbitrage\n",
    "    -1: arbitrage from exchange 2 to exchange 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # assuming the total fees are 0.55%, if the higher closing price \n",
    "    # is less than 0.55% higher than the lower closing price\n",
    "    if df['pct_higher'] < .55:\n",
    "        return 0 # no arbitrage\n",
    "    \n",
    "    # if exchange 1 closing price is more than 0.55% higher\n",
    "    # than the exchange 2 closing price\n",
    "    elif df['higher_closing_price'] == 1:\n",
    "        return -1 # arbitrage from exchange 2 to exchange 1\n",
    "    \n",
    "    # if exchange 2 closing price is more than 0.55% higher\n",
    "    # than the exchange 1 closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        return 1 # arbitrage from exchange 1 to exchange 2\n",
    "\n",
    "def get_window_length(df):\n",
    "    \"\"\"\n",
    "    Creates a column 'window_length' to show how long an arbitrage \n",
    "    opportunity has lasted\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert arbitrage_opportunity column to a list\n",
    "    target_list = df['arbitrage_opportunity'].to_list()\n",
    "    \n",
    "    # set initial window length \n",
    "    window_length = 5 # time in minutes\n",
    "    \n",
    "    # list for window_lengths\n",
    "    window_lengths = []\n",
    "    \n",
    "    # iterate through arbitrage_opportunity column\n",
    "    for i in range(len(target_list)):\n",
    "        \n",
    "        # check if a value in the arbitrage_opportunity column is \n",
    "        # equal to the previous value in the arbitrage_opportunity \n",
    "        # column and increase window length\n",
    "        if target_list[i] == target_list[i-1]:\n",
    "            window_length += 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "        # if a value in the arbitrage_opportunity column is\n",
    "        # not equal to the previous value in the arbitrage_opportunity \n",
    "        # column reset the window length to five minutes\n",
    "        else:\n",
    "            window_length = 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "    # create window length column showing how long an arbitrage \n",
    "    # opportunity has lasted\n",
    "    df['window_length'] = window_lengths\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    \"\"\"\n",
    "    Merges two dataframes and adds final features for arbitrage data\n",
    "    \n",
    "    Returns a dataframe with new features:\n",
    "    - year\n",
    "    - month\n",
    "    - day\n",
    "    - higher_closing_price\n",
    "    - pct_higher\n",
    "    - arbitrage_opportunity\n",
    "    - window_length\n",
    "    \"\"\"\n",
    "    df = pd.merge(df1, df2, on='closing_time',\n",
    "                  suffixes=('_exchange_1', '_exchange_2'))\n",
    "\n",
    "    # convert closing_time to datetime\n",
    "    df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "\n",
    "    # Create additional date features.\n",
    "    df['year'] = df['closing_time'].dt.year\n",
    "    df['month'] = df['closing_time'].dt.month\n",
    "    df['day'] = df['closing_time'].dt.day\n",
    "    print('before get higher closing price', df.shape)\n",
    "    \n",
    "    # get higher_closing_price feature to create pct_higher feature\n",
    "    df['higher_closing_price'] = df.apply(get_higher_closing_price, axis=1)\n",
    "    \n",
    "    # get pct_higher feature to create arbitrage_opportunity feature\n",
    "    df['pct_higher'] = df.apply(get_pct_higher, axis=1)\n",
    "    \n",
    "    # create arbitrage_opportunity feature\n",
    "    df['arbitrage_opportunity'] = df.apply(get_arb_opportunity, axis=1)\n",
    "    \n",
    "    # create window_length feature\n",
    "    df = get_window_length(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying arbitrage window length to target, in minutes\n",
    "interval = 30\n",
    "\n",
    "def get_target_value(df, interval=30):\n",
    "    \"\"\"\n",
    "    Checks for arbitrage opportunities and returns a target. \n",
    "    \n",
    "    Classes:\n",
    "    - 1: arbitrage from exchange 1 to exchange 2 starting 10 mins after prediction time, lasting 30 mins\n",
    "    - 0: no arbitrage 10 mins after prediction time\n",
    "    - -1: arbitrage from exchange 2 to exchnage 1 starting 10 mins after prediction time, lasting 30 mins\n",
    "    \"\"\"\n",
    "    \n",
    "    # if the arbitrage window is as long as the targeted interval\n",
    "    if df['window_length_shift'] >= interval:\n",
    "        # if that window is for exchange 1 to 2\n",
    "        if df['arbitrage_opportunity_shift'] == 1:\n",
    "            return 1 # arbitrage from exchange 1 to 2\n",
    "        \n",
    "        # if that window is for exchange 2 to 1\n",
    "        elif df['arbitrage_opportunity_shift'] == -1:\n",
    "            return -1 # arbitrage from exchange 2 to 1\n",
    "        \n",
    "        # if no arbitrage opportunity\n",
    "        elif df['arbitrage_opportunity_shift'] == 0:\n",
    "            return 0 # no arbitrage opportunity\n",
    "        \n",
    "    # if the arbitrage window is less than our targeted interval\n",
    "    else:\n",
    "        return 0 # no arbitrage opportunity\n",
    "    \n",
    "\n",
    "def get_target(df, interval=interval):\n",
    "    \"\"\"\n",
    "    Create new features and target.\n",
    "    \n",
    "    Returns a dataframe with new features:\n",
    "    - arbitrage_opportunity_shift\n",
    "    - window_length_shift\n",
    "    - target\n",
    "    \"\"\"\n",
    "    \n",
    "    # used to shift rows\n",
    "    # assumes candle length is five minutes, interval is 30 mins\n",
    "    rows_to_shift = int(-1*(interval/5)) # -7\n",
    "    \n",
    "    # arbitrage_opportunity feature, shifted by length of targeted interval\n",
    "    # minus one to predict ten minutes in advance rather than five\n",
    "    df['arbitrage_opportunity_shift'] = df['arbitrage_opportunity'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    \n",
    "    # window_length feature, shifted by length of targeted interval minus one\n",
    "    # to predict ten minutes\n",
    "    df['window_length_shift'] = df['window_length'].shift(rows_to_shift - 1)\n",
    "    \n",
    "    # creating target column; this will indicate if an arbitrage opportunity\n",
    "    # that lasts as long as the targeted interval is forthcoming\n",
    "    df['target'] = df.apply(get_target_value, axis=1)\n",
    "    \n",
    "    # dropping rows where target could not be calculated due to shift\n",
    "    df = df[:rows_to_shift - 1] # -7\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_close_shift(df, interval=interval):\n",
    "    \"\"\"\n",
    "    Shifts the closing prices by the selected interval +\n",
    "    10 mins.\n",
    "    \n",
    "    Returns a df with new features:\n",
    "    - close_exchange_1_shift\n",
    "    - close_exchange_2_shift\n",
    "    \"\"\"\n",
    "    \n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    \n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_profit(df):\n",
    "    \"\"\"\n",
    "    Calculates the profit of an arbitrage trade and returns df \n",
    "    with new profit feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if exchange 1 has the higher closing price\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # return how much money you would make if you bought \n",
    "        # on exchange 2, sold on exchange 1, and took account \n",
    "        # of 0.55% fees\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    \n",
    "    # if exchange 2 has the higher closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # return how much money you would make if you bought \n",
    "        # on exchange 1, sold on exchange 2, and took account \n",
    "        # of 0.55% fees\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    \n",
    "    # if the closing prices are the same\n",
    "    else:\n",
    "        return 0 # no arbitrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split names when in the format exchange_trading_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exchange_trading_pair(ex_tp):\n",
    "    \"\"\"\n",
    "    Splits exchange_trading_pair into separate variables.\n",
    "    \"\"\"\n",
    "\n",
    "    ex_tp = ex_tp.split('/')[2]\n",
    "    exchange = ex_tp.split('_')[0]\n",
    "    trading_pair = '_'.join(ex_tp.split('_')[1:3])\n",
    "        \n",
    "    return exchange, trading_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all individual csv's with ta data (~2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- create a `/ta_data` directory before running this function\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take less than an hour if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ta_csvs(csv_filepaths):\n",
    "    \"\"\"\n",
    "    Takes a csv filename, creates a dataframe, engineers \n",
    "    features, and saves it as a new csv in /ta_data.\n",
    "    \"\"\"\n",
    "    # counter\n",
    "    n = 1\n",
    "    \n",
    "    for file in csv_filepaths:\n",
    "        \n",
    "        # create df\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # define period\n",
    "        period = '5T'\n",
    "        \n",
    "        # engineer features\n",
    "        df = engineer_features(df, period)\n",
    "        print('features engineered')\n",
    "        \n",
    "        # generate new filename\n",
    "        filename = 'data/ta_data/' + file.split('/')[2][:-4] + '_ta.csv'\n",
    "        \n",
    "        # export csv\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        # print progress\n",
    "        print(f'csv #{n} saved :)')\n",
    "        \n",
    "        # update counter\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ta_csvs(csv_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all arbitrage training data csv's (~9 hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- create a `/arb_data` directory before running this function\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~2-3 hours if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arb_csvs(pairs):\n",
    "    \"\"\"Takes a list of possible arbitrage combinations, finds the \n",
    "        appropriate datasets in /ta_data, loads datasets, merges them,\n",
    "        engineers more features, creates a target and exports the new\n",
    "        dataset as a csv\"\"\"\n",
    "    \n",
    "    # counter\n",
    "    n = 0\n",
    "    \n",
    "    # iterate through arbitrage combinations\n",
    "    for pair in pairs:\n",
    "        \n",
    "        # define paths for the csv\n",
    "        csv_1 = 'data/ta_data/' + pair[0].split('/')[2][:-4] + '_ta.csv'\n",
    "        csv_2 = 'data/ta_data/' + pair[1].split('/')[2][:-4] + '_ta.csv'\n",
    "        print('csv1, csv2:', csv_1, csv_2)\n",
    "        \n",
    "        # define exchanges and trading_pairs\n",
    "        ex_tp_1, ex_tp_2 = pair[0][:-8], pair[1][:-8]\n",
    "        print(ex_tp_1)\n",
    "        ex1, tp1 = get_exchange_trading_pair(ex_tp_1)\n",
    "        ex2, tp2 = get_exchange_trading_pair(ex_tp_2)\n",
    "        print(ex1, tp1,  ex2, tp2)\n",
    "        \n",
    "        # define model_name for the filename\n",
    "        model_name = ex1 + '_' + ex_tp_2.split('/')[2]\n",
    "        print(model_name)\n",
    "          \n",
    "        # create dfs from csv's that already include ta features\n",
    "        df1, df2 = pd.read_csv(csv_1, index_col=0), pd.read_csv(csv_2, index_col=0)       \n",
    "        print('df 1 shape: ', df1.shape, 'df 2 shape: ', df2.shape)\n",
    "\n",
    "        # merge dfs\n",
    "        df = merge_dfs(df1, df2)\n",
    "        print('dfs merged')\n",
    "        print('merged df shape:' , df.shape)\n",
    "        \n",
    "\n",
    "        # create target \n",
    "        df = get_target(df)\n",
    "        print(model_name, ' ', df.shape)\n",
    "\n",
    "        # export csv\n",
    "        path = 'data/arb_data/'\n",
    "        csv_filename = path + model_name + '.csv'\n",
    "        df.to_csv(csv_filename)\n",
    "\n",
    "        # print progress\n",
    "        print(f'csv #{n} saved :)')\n",
    "\n",
    "        # update counter\n",
    "        n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_arb_csvs(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to make sure all csv's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "arb_data_paths = glob.glob('data/arb_data/*.csv')\n",
    "print(len(arb_data_paths)) # 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Distribution\n",
    "The final arbitrage datasets that will be used during modeling contain 3 classes:\n",
    "- 1: arbitrage from exchange 1 to exchange 2 starting 10 mins after prediction time, lasting 30 mins\n",
    "- 0: no arbitrage 10 mins after prediction time\n",
    "- -1: arbitrage from exchange 2 to exchnage 1 starting 10 mins after prediction time, lasting 30 mins\n",
    "\n",
    "We need to look at the class distribution to understand if these datasets contain enough of the 1 and -1 class to make arbitrage predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distribution(arb_data_paths):\n",
    "    \"\"\"\n",
    "    Returns a df of the class distribution for all arbitrage\n",
    "    datasets\n",
    "    \"\"\"\n",
    "    dist_df = pd.DataFrame()\n",
    "    \n",
    "    for path in arb_data_paths:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        arbitrage_combination = path.split('/')[2][:-4]\n",
    "        value_counts = df.target.value_counts()\n",
    "        \n",
    "        # not every dataset has all 3 classes so a conditional\n",
    "        # is required to deal with each case\n",
    "        if len(value_counts) == 1: # just 0\n",
    "            no_arb = round(value_counts[0] / value_counts.sum(), 2)\n",
    "            ex1_to_ex2_arb = 0\n",
    "            ex2_to_ex1_arb = 0\n",
    "        elif len(value_counts) == 2:\n",
    "            # if 0, -1\n",
    "            if value_counts.index[0] == -1 or value_counts.index[1] == -1:\n",
    "                no_arb = round(value_counts[0] / value_counts.sum(), 2)\n",
    "                ex1_to_ex2_arb = 0\n",
    "                ex2_to_ex1_arb = round(value_counts[-1] / value_counts.sum(), 2)\n",
    "            # if 0, 1\n",
    "            else:\n",
    "                no_arb = round(value_counts[0] / value_counts.sum(), 2)\n",
    "                ex1_to_ex2_arb = round(value_counts[1] / value_counts.sum(), 2)\n",
    "                ex2_to_ex1_arb = 0\n",
    "        else: # has all classes\n",
    "            no_arb = round(value_counts[0] / value_counts.sum(), 2)\n",
    "            ex1_to_ex2_arb = round(value_counts[1] / value_counts.sum(), 2)\n",
    "            ex2_to_ex1_arb = round(value_counts[-1] / value_counts.sum(), 2)\n",
    "\n",
    "        dist_df = dist_df.append({\n",
    "            'arbitrage_combination': arbitrage_combination,\n",
    "            'ex1_to_ex2_arb': ex1_to_ex2_arb,\n",
    "            'ex2_to_ex1_arb': ex2_to_ex1_arb,\n",
    "            'no_arb': no_arb\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    return dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbitrage_combination</th>\n",
       "      <th>ex1_to_ex2_arb</th>\n",
       "      <th>ex2_to_ex1_arb</th>\n",
       "      <th>no_arb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kraken_bitfinex_bch_btc</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kraken_gemini_eth_btc</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bitfinex_gemini_btc_usd</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>cbpro_gemini_bch_btc</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bitfinex_kraken_btc_usd</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arbitrage_combination  ex1_to_ex2_arb  ex2_to_ex1_arb  no_arb\n",
       "0  kraken_bitfinex_bch_btc            0.00            0.00    1.00\n",
       "1    kraken_gemini_eth_btc            0.01            0.01    0.98\n",
       "2  bitfinex_gemini_btc_usd            0.00            0.00    1.00\n",
       "3     cbpro_gemini_bch_btc            0.15            0.16    0.70\n",
       "4  bitfinex_kraken_btc_usd            0.00            0.00    1.00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df = class_distribution(arb_data_paths)\n",
    "dist_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the datasets that contain the most arbitrage, let's sort by no_arb..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbitrage_combination</th>\n",
       "      <th>ex1_to_ex2_arb</th>\n",
       "      <th>ex2_to_ex1_arb</th>\n",
       "      <th>no_arb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>kraken_cbpro_etc_usd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>bitfinex_kraken_etc_usd</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>bitfinex_cbpro_bch_usd</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>bitfinex_cbpro_zrx_usd</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>bitfinex_cbpro_etc_usd</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>cbpro_bitfinex_ltc_usd</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>hitbtc_cbpro_eth_usdc</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>cbpro_bitfinex_eth_usd</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>bitfinex_hitbtc_bch_usdt</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>gemini_bitfinex_bch_btc</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       arbitrage_combination  ex1_to_ex2_arb  ex2_to_ex1_arb  no_arb\n",
       "71      kraken_cbpro_etc_usd            0.01            0.60    0.39\n",
       "7    bitfinex_kraken_etc_usd            0.59            0.01    0.39\n",
       "39    bitfinex_cbpro_bch_usd            0.03            0.43    0.54\n",
       "37    bitfinex_cbpro_zrx_usd            0.01            0.45    0.54\n",
       "46    bitfinex_cbpro_etc_usd            0.07            0.36    0.57\n",
       "60    cbpro_bitfinex_ltc_usd            0.24            0.12    0.64\n",
       "69     hitbtc_cbpro_eth_usdc            0.17            0.17    0.66\n",
       "12    cbpro_bitfinex_eth_usd            0.21            0.12    0.67\n",
       "56  bitfinex_hitbtc_bch_usdt            0.19            0.14    0.67\n",
       "92   gemini_bitfinex_bch_btc            0.16            0.16    0.68"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df = dist_df.sort_values(by='no_arb').head(10)\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the datasets that will be most suitable for training arbitrage models, but we need to also be considerate of creating models that will predict arbitrage accurately in both directions. While the first 4 datasets above contain the most arbitrage occurences to train on, they only contain a high number of arbitrage in one direction. Those datasets aren't suitable for modeling, and neither are the ones at the end of this list because they contain almost no arbitrage at all. We can filter this dataframe for the datasets that contain at least 5% arbitrage in each direction to get our datasets that will be used in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbitrage_combination</th>\n",
       "      <th>ex1_to_ex2_arb</th>\n",
       "      <th>ex2_to_ex1_arb</th>\n",
       "      <th>no_arb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>bitfinex_cbpro_etc_usd</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>cbpro_bitfinex_ltc_usd</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>hitbtc_cbpro_eth_usdc</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>cbpro_bitfinex_eth_usd</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>bitfinex_hitbtc_bch_usdt</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>gemini_bitfinex_bch_btc</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>gemini_hitbtc_bch_btc</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>cbpro_gemini_bch_btc</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>kraken_gemini_bch_btc</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>bitfinex_cbpro_btc_usd</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>gemini_kraken_ltc_btc</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>bitfinex_hitbtc_ltc_usdt</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>gemini_bitfinex_ltc_btc</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>gemini_hitbtc_ltc_btc</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>gemini_cbpro_ltc_btc</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       arbitrage_combination  ex1_to_ex2_arb  ex2_to_ex1_arb  no_arb\n",
       "46    bitfinex_cbpro_etc_usd            0.07            0.36    0.57\n",
       "60    cbpro_bitfinex_ltc_usd            0.24            0.12    0.64\n",
       "69     hitbtc_cbpro_eth_usdc            0.17            0.17    0.66\n",
       "12    cbpro_bitfinex_eth_usd            0.21            0.12    0.67\n",
       "56  bitfinex_hitbtc_bch_usdt            0.19            0.14    0.67\n",
       "92   gemini_bitfinex_bch_btc            0.16            0.16    0.68\n",
       "23     gemini_hitbtc_bch_btc            0.16            0.14    0.69\n",
       "3       cbpro_gemini_bch_btc            0.15            0.16    0.70\n",
       "47     kraken_gemini_bch_btc            0.14            0.14    0.71\n",
       "73    bitfinex_cbpro_btc_usd            0.08            0.19    0.73\n",
       "42     gemini_kraken_ltc_btc            0.11            0.07    0.82\n",
       "63  bitfinex_hitbtc_ltc_usdt            0.09            0.08    0.84\n",
       "94   gemini_bitfinex_ltc_btc            0.07            0.06    0.86\n",
       "13     gemini_hitbtc_ltc_btc            0.07            0.06    0.87\n",
       "75      gemini_cbpro_ltc_btc            0.07            0.06    0.87"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_df_filtered = dist_df[(dist_df['ex1_to_ex2_arb'] > 0.05) & (dist_df['ex2_to_ex1_arb'] > 0.05)]\n",
    "dist_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export final arbitrage training dataset paths to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported data: \n",
      "\n",
      " ['data/arb_data/bitfinex_cbpro_etc_usd.csv', 'data/arb_data/cbpro_bitfinex_ltc_usd.csv', 'data/arb_data/hitbtc_cbpro_eth_usdc.csv', 'data/arb_data/cbpro_bitfinex_eth_usd.csv', 'data/arb_data/bitfinex_hitbtc_bch_usdt.csv', 'data/arb_data/gemini_bitfinex_bch_btc.csv', 'data/arb_data/gemini_hitbtc_bch_btc.csv', 'data/arb_data/cbpro_gemini_bch_btc.csv', 'data/arb_data/kraken_gemini_bch_btc.csv', 'data/arb_data/bitfinex_cbpro_btc_usd.csv', 'data/arb_data/gemini_kraken_ltc_btc.csv', 'data/arb_data/bitfinex_hitbtc_ltc_usdt.csv', 'data/arb_data/gemini_bitfinex_ltc_btc.csv', 'data/arb_data/gemini_hitbtc_ltc_btc.csv', 'data/arb_data/gemini_cbpro_ltc_btc.csv']\n"
     ]
    }
   ],
   "source": [
    "data = dist_df_filtered['arbitrage_combination'].to_list()\n",
    "train_data_paths = [f'data/arb_data/{d}.csv' for d in data]\n",
    "\n",
    "with open('data/train_data_paths.txt', 'wb') as fp:\n",
    "    pickle.dump(train_data_paths, fp)\n",
    "\n",
    "print(f'Exported data: \\n\\n {train_data_paths}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued...\n",
    "Model training will be carried out in the following notebook:\n",
    "- [View on Github](https://github.com/Cryptolytic-app/cryptolyticapp/blob/master/modeling/2_arbitrage_model_training.ipynb)\n",
    "\n",
    "- [Jump to local copy](2_arbitrage_model_training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic-env",
   "language": "python",
   "name": "cryptolytic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
