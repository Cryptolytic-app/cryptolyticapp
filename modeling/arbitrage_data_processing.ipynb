{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptolytic Data Processing\n",
    "\n",
    "This notebook contains the code to generate the data that is used to create the arbitrage models in this [notebook]() (add notebook link).\n",
    "\n",
    "#### Background on arbitrage models\n",
    "Arbitrage models were created with the goal of predicting arbitrage 10 min before it happens in an active crypto market. The models are generated by getting all of the combinations of 2 exchanges that support the same trading pair, engineering technical analysis features, merging that data on 'closing_time', engineering more features, and creating a target that signals an arbitrage opportunity. Arbitrage signals predicted by the model have a direction indicating which direction the arbitrage occurs in. A valid arbitrage signal is when the arbitrage lasts >30 mins because it takes time to move coins from one exchange to the other in order to successfully complete the arbitrage trades.\n",
    "\n",
    "The models predict whether there will be an arbitrage opportunity that starts 10 mins after the prediction time and lasts for at least 30 mins, giving a user enough times to execute trades.\n",
    "\n",
    "#### Where does the data come from?\n",
    "Data for this project was obtained through the APIs of each exchange.\n",
    "\n",
    "<img src=\"assets/exchange_logos.png\"\n",
    "     alt=\" \"\n",
    "     align= \"center\"\n",
    "     style=\"width: 1000px;\" />\n",
    "     \n",
    "- [Bitfinex API OHLCV Data Documentation](https://docs.bitfinex.com/reference#rest-public-candles)\n",
    "- [Coinbase Pro API OHLCV Data Documentation](https://docs.pro.coinbase.com/?r=1#get-historic-rates)\n",
    "- [HitBTC OHLCV Data Documentation](https://api.hitbtc.com/#candles)\n",
    "- [Kraken OHLCV Data Documentation](https://www.kraken.com/features/api)\n",
    "- [Gemini OHLCV Data Documentation](https://docs.gemini.com/rest-api/)\n",
    "\n",
    "The functions to collect this data can be found [here](https://github.com/Cryptolytic-app/cryptolytic/tree/master/data_collection_and_databasing)\n",
    "\n",
    "\n",
    "#### What does the data look like?\n",
    "<img src=\"assets/sample_df.png\"\n",
    "     alt=\" \"\n",
    "     align= \"center\"\n",
    "     style=\"width: 400px;\" />\n",
    "     \n",
    "#### Data Dictionary\n",
    "- **closing_time:** the closing time of the candlestick\n",
    "- **open:** the price of the cryptocurrency at the opening of the candlestick\n",
    "- **high:** the highest price of the cryptocurrency during that candlestick\n",
    "- **low:** the lowest price of the cryptocurrency during that candlestick\n",
    "- **close:** the price of the cryptocurrency at the end of the candlestick\n",
    "- **volume:** the volume traded during that candlestick\n",
    "\n",
    "\n",
    "#### Features Engineered\n",
    "Technical analysis features were engineered with the [Technical Analysis Library](https://github.com/bukosabino/ta). They fall into five categories:\n",
    "- Momentum indicators\n",
    "- Volume indicators\n",
    "- Volatility indicators\n",
    "- Trend indicators\n",
    "- Others indicators\n",
    "\n",
    "#### Merging Datasets\n",
    "Arbitrage could occur between two exchanges that have the same trading pair. We genererated all of the possible arbitrage combinations between the 80 datasets that were available. This resulted in 95 possible combinations of arbitrage datasets that could be used in modeling.\n",
    "\n",
    "For each possible arbitrage combination, we merged the two datasets on 'closing_time' and created new features that identified arbitrage opportunities. These features included:\n",
    "- **higher_closing_price:** identifies the exchange that has the higher closing price (1 or 2)\n",
    "- **pct_higher:** the percentage by which higher_closing_price is greater\n",
    "- **arbitrage_opportunity:** identfies if there is greater than 0.55% gain (to account for fees)\n",
    "    - 1: arbitrage from exchange 1 to exchange 2\n",
    "    - 0: no arbitrage\n",
    "    - -1: arbitrage from exchange 2 to exchnage 1\n",
    "- **close_exchange_1_shift:** shifts the exchange 1 close price to account for a 30 min trading interval + 10 min advance prediction\n",
    "- **close_exchange_2_shift:** shifts the exchange 2 close price to account for a 30 min trading interval + 10 min advance prediction\n",
    "- **window_length:** gets the window length of the arbitrage opportunity\n",
    "- **window_length_shift:** shifts the window length by the 30 min trading interval - 10 mins\n",
    "- **arbitrage_opportunity_shift:** shifts the arbitrage opportunity the the 30 min trading interval - 10 mins\n",
    "- **target:** identifies whether there was an arbitrage opportunity\n",
    "    - 1: arbitrage from exchange 1 to exchange 2\n",
    "    - 0: no arbitrage\n",
    "    - -1: arbitrage from exchange 2 to exchnage 1\n",
    "\n",
    "Notes:\n",
    "- It is HIGHLY reccommended to use AWS Sagemaker to do feature engineering and split the work onto four notebooks. This will cut down the time to process data drastically. You can do this by list slicing the filepaths of the datasets you are inputting into the functions (for example: `create_ta_csvs(csv_filepaths[:20]`)\n",
    "- Since feature engineering takes a long time, we export data as csvs along each step to not have to re-engineer features in case the runtime restarts: export after technical analysis features are added, and export after datasets are merged and more arbitrage features are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Folder organization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── cryptolytic/                        <-- The top-level directory for all arbitrage work\n",
    "│   ├── modeling_nb/                    <-- Directory for modeling work\n",
    "│   │      ├──data/                     <-- Directory with subdirectories containing 5 min candle data\n",
    "│   │      │   ├─ arb_data/             <-- Directory for csv files of arbitrage model training data\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ csv_data/             <-- Directory for csv files after combining datasets and FE pt.2\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ ta_data/              <-- Directory for csv files after FE pt.1 \n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ *.zip                 <-- ZIP files of all of the data\n",
    "│   │      │   \n",
    "│   │      ├──final_models/             <-- Directory for final models after model selection\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├──model_perf/               <-- Directory for performance csvs after training models\n",
    "│   │      │      └── *.json\n",
    "│   │      │\n",
    "│   │      ├──models/                   <-- Directory for all pickle models\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_data_processing.ipynb      <-- Notebook for data processing and creating csvs\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_modeling.ipynb             <-- Notebook for baseline models and hyperparam tuning\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_model_selection.ipynb      <-- Notebook for model selection\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_model_evaluation.ipynb     <-- Notebook for final model evaluation\n",
    "│   │      │\n",
    "│   │      ├─environment.yml                      <-- yml file to create conda environment\n",
    "│   │      │\n",
    "│   │      ├─trade_recommender_models.ipynb       <-- Notebook for trade recommender models\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses conda to manage environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to update your conda env from a yml file from terminal\n",
    "# conda env update --file cryptolytic/modeling_nb/environment.yml\n",
    "\n",
    "# to export yml from terminal\n",
    "# conda env export > cryptolytic/finalized_notebooks/environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import itertools\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "pd.set_option('display.max_rows', 100000)\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In this section we'll be unzipping the files that contain the 5 minute candle data, saving them to a new directory `csv_data/`, and renaming the filepaths that contain coinbase_pro to cbpro (the underscore causes problems with naming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_filepaths = glob.glob('data/*300.zip')\n",
    "len(zip_filepaths) #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS TO UNZIP\n",
    "\n",
    "# for zip_filepath in zip_filepaths:\n",
    "#     with ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('data/csv_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filepaths = glob.glob('data/csv_data/*.csv')\n",
    "len(csv_filepaths) #80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS TO RENAME\n",
    "\n",
    "# for filepath in csv_filepaths:\n",
    "#     new_filepath = filepath.replace('coinbase_pro', 'cbpro')\n",
    "#     os.rename(filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw datasets look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closing_time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>base_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1496358900</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1496370000</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1496377500</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1496393700</td>\n",
       "      <td>249.99</td>\n",
       "      <td>249.99</td>\n",
       "      <td>249.99</td>\n",
       "      <td>249.99</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1496407500</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1496408700</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1496422200</td>\n",
       "      <td>239.26</td>\n",
       "      <td>239.26</td>\n",
       "      <td>233.89</td>\n",
       "      <td>233.89</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1496422500</td>\n",
       "      <td>232.90</td>\n",
       "      <td>232.95</td>\n",
       "      <td>232.55</td>\n",
       "      <td>232.55</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1496423100</td>\n",
       "      <td>234.04</td>\n",
       "      <td>234.04</td>\n",
       "      <td>234.04</td>\n",
       "      <td>234.04</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1496423400</td>\n",
       "      <td>230.34</td>\n",
       "      <td>233.26</td>\n",
       "      <td>230.34</td>\n",
       "      <td>232.74</td>\n",
       "      <td>0.749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   closing_time    open    high     low   close  base_volume\n",
       "0    1496358900    1.00    1.00    1.00    1.00        0.100\n",
       "1    1496370000    1.03    1.03    1.03    1.03        0.002\n",
       "2    1496377500    1.02    1.02    1.02    1.02        0.530\n",
       "3    1496393700  249.99  249.99  249.99  249.99        0.001\n",
       "4    1496407500    1.05    1.05    1.05    1.05        0.010\n",
       "5    1496408700    1.99    1.99    1.99    1.99        0.003\n",
       "6    1496422200  239.26  239.26  233.89  233.89        0.036\n",
       "7    1496422500  232.90  232.95  232.55  232.55        0.056\n",
       "8    1496423100  234.04  234.04  234.04  234.04        0.321\n",
       "9    1496423400  230.34  233.26  230.34  232.74        0.749"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(csv_filepaths[1], index_col=0).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And we want to transform it into something that looks like this for modeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('bitfinex_cbpro_btc_usd.csv')[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all combinations of exchanges with the same trading pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five supported exchanges\n",
    "exchanges = ['bitfinex', 'coinbase_pro', 'gemini', 'hitbtc', 'kraken']\n",
    "\n",
    "# function to create pairs for arbitrage datasets\n",
    "def get_file_pairs(filenames, exchanges):\n",
    "    \"\"\"\n",
    "    This function takes in a list of exchanges and looks through data\n",
    "    directories to find all possible combinations for 2 exchanges\n",
    "    with the same trading pair. Returns a list of all lists that\n",
    "    include the file pairs\n",
    "    \"\"\"\n",
    "    # get combinations\n",
    "    combos = list(itertools.combinations(filenames, 2))\n",
    "    \n",
    "    # remove unmatched trading pairs\n",
    "    filtered_combos = []\n",
    "    for combo in combos:\n",
    "        tp1 = '_'.join(combo[0].split('/')[2].split('_')[1:3])\n",
    "        tp2 = '_'.join(combo[1].split('/')[2].split('_')[1:3])\n",
    "        if tp1 == tp2:\n",
    "            filtered_combos.append(combo)\n",
    "                    \n",
    "    return filtered_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "pairs = get_file_pairs(csv_filepaths, exchanges)\n",
    "print(len(pairs)) # 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHLCV Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ohlcv(df, period):\n",
    "    \"\"\" \n",
    "    Changes the time period on cryptocurrency ohlcv data.\n",
    "    Period is a string denoted by '{time_in_minutes}T'\n",
    "    (ex: '1T', '5T', '60T').\n",
    "    \"\"\"\n",
    "\n",
    "    # set date as index\n",
    "    # needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "\n",
    "    # aggregation function\n",
    "    ohlc_dict = {\n",
    "        'open':'first',                                                                                                    \n",
    "        'high':'max',                                                                                                       \n",
    "        'low':'min',                                                                                                        \n",
    "        'close': 'last',                                                                                                    \n",
    "        'base_volume': 'sum'\n",
    "    }\n",
    "\n",
    "    # resample\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NaNs\n",
    "\n",
    "Resample_ohlcv function will create NaNs in df where there were gaps in the data. The gaps could be caused by exchanges being down, errors from cryptowatch or errors from the exchanges themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(df):\n",
    "    \"\"\"\n",
    "    Iterates through a dataframe and fills NaNs with appropriate \n",
    "    open, high, low, close values.\n",
    "    \"\"\"\n",
    "    # Forward fill close column\n",
    "    df['close'] = df['close'].ffill()\n",
    "\n",
    "    # Backward fill the open, high, low rows with the close value\n",
    "    df = df.bfill(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering - before merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, period='5T'):\n",
    "    \"\"\"\n",
    "    Takes a df, engineers ta features, and returns a df\n",
    "    with period=['5T']\n",
    "    \"\"\"\n",
    "    # convert unix closing_time to datetime\n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "    \n",
    "    # time resampling to fill gaps in data\n",
    "    df = resample_ohlcv(df, period)\n",
    "    \n",
    "    # move date off the index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # create closing_time\n",
    "    closing_time = df.date.values\n",
    "    df.drop(columns='date', inplace=True)\n",
    "    \n",
    "    # create feature to indicate where rows were gaps in data\n",
    "    df['nan_ohlcv'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    # fill gaps in data\n",
    "    df = fill_nan(df)\n",
    "\n",
    "    # adding all the technical analysis features...\n",
    "    df = add_all_ta_features(df, 'open', 'high', 'low', 'close','base_volume', fillna=True)\n",
    "    \n",
    "    # add closing time column\n",
    "    df['closing_time'] = closing_time\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - after merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_closing_price(df):\n",
    "    \"\"\"\n",
    "    Returns the exchange with the higher closing price\n",
    "    \"\"\"\n",
    "    \n",
    "    # exchange 1 has higher closing price\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 1\n",
    "    \n",
    "    # exchange 2 has higher closing price\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 2\n",
    "    \n",
    "    # closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_pct_higher(df):\n",
    "    \"\"\"\n",
    "    Returns the percentage of the difference between ex1/ex2 \n",
    "    closing prices\n",
    "    \"\"\"\n",
    "    \n",
    "    # if exchange 1 has a higher closing price than exchange 2\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_1'] / \n",
    "                 df['close_exchange_2'])-1)*100\n",
    "    \n",
    "    # if exchange 2 has a higher closing price than exchange 1\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # % difference\n",
    "        return ((df['close_exchange_2'] / \n",
    "                 df['close_exchange_1'])-1)*100\n",
    "    \n",
    "    # if closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_arbitrage_opportunity(df):\n",
    "    \"\"\"\n",
    "    Return available arbitrage opportunities\n",
    "    \n",
    "    1: arbitrage from exchange 1 to exchange 2\n",
    "    0: no arbitrage\n",
    "    -1: arbitrage from exchange 2 to exchange 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # assuming the total fees are 0.55%, if the higher closing price \n",
    "    # is less than 0.55% higher than the lower closing price\n",
    "    if df['pct_higher'] < .55:\n",
    "        return 0 # no arbitrage\n",
    "    \n",
    "    # if exchange 1 closing price is more than 0.55% higher\n",
    "    # than the exchange 2 closing price\n",
    "    elif df['higher_closing_price'] == 1:\n",
    "        return -1 # arbitrage from exchange 2 to exchange 1\n",
    "    \n",
    "    # if exchange 2 closing price is more than 0.55% higher\n",
    "    # than the exchange 1 closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        return 1 # arbitrage from exchange 1 to exchange 2\n",
    "\n",
    "def get_window_length(df):\n",
    "    \"\"\"\n",
    "    Creates a column 'window_length' to show how long an arbitrage \n",
    "    opportunity has lasted\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert arbitrage_opportunity column to a list\n",
    "    target_list = df['arbitrage_opportunity'].to_list()\n",
    "    \n",
    "    # set initial window length \n",
    "    window_length = 5 # time in minutes\n",
    "    \n",
    "    # list for window_lengths\n",
    "    window_lengths = []\n",
    "    \n",
    "    # iterate through arbitrage_opportunity column\n",
    "    for i in range(len(target_list)):\n",
    "        \n",
    "        # check if a value in the arbitrage_opportunity column is \n",
    "        # equal to the previous value in the arbitrage_opportunity \n",
    "        # column and increase window length\n",
    "        if target_list[i] == target_list[i-1]:\n",
    "            window_length += 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "        # if a value in the arbitrage_opportunity column is\n",
    "        # not equal to the previous value in the arbitrage_opportunity \n",
    "        # column reset the window length to five minutes\n",
    "        else:\n",
    "            window_length = 5\n",
    "            window_lengths.append(window_length)\n",
    "            \n",
    "    # create window length column showing how long an arbitrage \n",
    "    # opportunity has lasted\n",
    "    df['window_length'] = window_lengths\n",
    "\n",
    "    return df\n",
    "        \n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    \"\"\"\n",
    "    Merges two dataframes and adds final features for arbitrage data\n",
    "    \n",
    "    Returns a dataframe with new features:\n",
    "    - year\n",
    "    - month\n",
    "    - day\n",
    "    - higher_closing_price\n",
    "    - pct_higher\n",
    "    - arbitrage_opportunity\n",
    "    - window_length\n",
    "    \"\"\"\n",
    "    # merge\n",
    "    df = pd.merge(df1, df2, on='closing_time',\n",
    "                  suffixes=('_exchange_1', '_exchange_2'))\n",
    "\n",
    "    # convert closing_time to datetime\n",
    "    df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "\n",
    "    # Create additional date features.\n",
    "    df['year'] = df['closing_time'].dt.year\n",
    "    df['month'] = df['closing_time'].dt.month\n",
    "    df['day'] = df['closing_time'].dt.day\n",
    "    print('before get higher closing price', df.shape)\n",
    "    \n",
    "    # get higher_closing_price feature to create pct_higher feature\n",
    "    df['higher_closing_price'] = df.apply(get_higher_closing_price, axis=1)\n",
    "    \n",
    "    # get pct_higher feature to create arbitrage_opportunity feature\n",
    "    df['pct_higher'] = df.apply(get_pct_higher, axis=1)\n",
    "    \n",
    "    # create arbitrage_opportunity feature\n",
    "    df['arbitrage_opportunity'] = df.apply(get_arbitrage_opportunity, axis=1)\n",
    "    \n",
    "    # create window_length feature\n",
    "    df = get_window_length(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying arbitrage window length to target, in minutes\n",
    "interval = 30\n",
    "\n",
    "def get_target_value(df, interval=30):\n",
    "    \"\"\"\n",
    "    Checks for arbitrage opportunities and returns a target. \n",
    "    \n",
    "    1: arbitrage from exchange 1 to exchange 2\n",
    "    0: no arbitrage\n",
    "    -1: arbitrage from exchange 2 to exchange 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # if the arbitrage window is as long as the targeted interval\n",
    "    if df['window_length_shift'] >= interval:\n",
    "        # if that window is for exchange 1 to 2\n",
    "        if df['arbitrage_opportunity_shift'] == 1:\n",
    "            return 1 # arbitrage from exchange 1 to 2\n",
    "        \n",
    "        # if that window is for exchange 2 to 1\n",
    "        elif df['arbitrage_opportunity_shift'] == -1:\n",
    "            return -1 # arbitrage from exchange 2 to 1\n",
    "        \n",
    "        # if no arbitrage opportunity\n",
    "        elif df['arbitrage_opportunity_shift'] == 0:\n",
    "            return 0 # no arbitrage opportunity\n",
    "        \n",
    "    # if the arbitrage window is less than our targeted interval\n",
    "    else:\n",
    "        return 0 # no arbitrage opportunity\n",
    "    \n",
    "\n",
    "def get_target(df, interval=interval):\n",
    "    \"\"\"\n",
    "    Create new features and target.\n",
    "    \n",
    "    Returns a dataframe with new features:\n",
    "    - arbitrage_opportunity_shift\n",
    "    - window_length_shift\n",
    "    - target\n",
    "    \"\"\"\n",
    "    \n",
    "    # used to shift rows\n",
    "    # assumes candle length is five minutes, interval is 30 mins\n",
    "    rows_to_shift = int(-1*(interval/5)) # -7\n",
    "    \n",
    "    # arbitrage_opportunity feature, shifted by length of targeted interval\n",
    "    # minus one to predict ten minutes in advance rather than five\n",
    "    df['arbitrage_opportunity_shift'] = df['arbitrage_opportunity'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    \n",
    "    # window_length feature, shifted by length of targeted interval minus one\n",
    "    # to predict ten minutes\n",
    "    df['window_length_shift'] = df['window_length'].shift(rows_to_shift - 1)\n",
    "    \n",
    "    # creating target column; this will indicate if an arbitrage opportunity\n",
    "    # that lasts as long as the targeted interval is forthcoming\n",
    "    df['target'] = df.apply(get_target_value, axis=1)\n",
    "    \n",
    "    # dropping rows where target could not be calculated due to shift\n",
    "    df = df[:rows_to_shift - 1] # -7\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_close_shift(df, interval=interval):\n",
    "    \"\"\"\n",
    "    Shifts the closing prices by the selected interval +\n",
    "    10 mins.\n",
    "    \n",
    "    Returns a df with new features:\n",
    "    - close_exchange_1_shift\n",
    "    - close_exchange_2_shift\n",
    "    \"\"\"\n",
    "    \n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    \n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_profit(df):\n",
    "    \"\"\"\n",
    "    Calculates the profit of an arbitrage trade.\n",
    "    \n",
    "    Returns df with new profit feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if exchange 1 has the higher closing price\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # return how much money you would make if you bought \n",
    "        # on exchange 2, sold on exchange 1, and took account \n",
    "        # of 0.55% fees\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    \n",
    "    # if exchange 2 has the higher closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # return how much money you would make if you bought \n",
    "        # on exchange 1, sold on exchange 2, and took account \n",
    "        # of 0.55% fees\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    \n",
    "    # if the closing prices are the same\n",
    "    else:\n",
    "        return 0 # no arbitrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split names when in the format exchange_trading_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exchange_trading_pair(ex_tp):\n",
    "    \"\"\"\n",
    "    Splits exchange_trading_pair into separate variables\n",
    "    \"\"\"\n",
    "\n",
    "    ex_tp = ex_tp.split('/')[2]\n",
    "    exchange = ex_tp.split('_')[0]\n",
    "    trading_pair = '_'.join(ex_tp.split('_')[1:3])\n",
    "        \n",
    "    return exchange, trading_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all individual csv's with ta data (~1-2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- create a `/ta_data` directory before running this function\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~1 hour if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ta_csvs(csv_filepaths):\n",
    "    \"\"\"\n",
    "    Takes a csv filename, creates a dataframe, engineers \n",
    "    features, and saves it as a new csv in /ta_data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # counter\n",
    "    n = 1\n",
    "    \n",
    "    for file in csv_filepaths:\n",
    "        \n",
    "        # create df\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # define period\n",
    "        period = '5T'\n",
    "        \n",
    "        # engineer features\n",
    "        df = engineer_features(df, period)\n",
    "        print('features engineered')\n",
    "        \n",
    "        # generate new filename\n",
    "        filename = 'data/ta_data/' + file.split('/')[2][:-4] + '_ta.csv'\n",
    "        \n",
    "        # export csv\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        # print progress\n",
    "        print(f'csv #{n} saved :)')\n",
    "        \n",
    "        # update counter\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ta_csvs(csv_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all arbitrage training data csv's (~9 hrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- create a `/arb_data` directory before running this function\n",
    "- this function takes a really long time to run so it's recommended to run in sagemaker and divide the pairs in to 4 notebooks so you're running about 20 pairs in each notebook. Should take ~2-3 hours if split up on 4 notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arb_csvs(pairs):\n",
    "    \"\"\"Takes a list of possible arbitrage combinations, finds the \n",
    "        appropriate datasets in /ta_data, loads datasets, merges them,\n",
    "        engineers more features, creates a target and exports the new\n",
    "        dataset as a csv\"\"\"\n",
    "    \n",
    "    # counter\n",
    "    n = 0\n",
    "    \n",
    "    # iterate through arbitrage combinations\n",
    "    for pair in pairs:\n",
    "        \n",
    "        # define paths for the csv\n",
    "        csv_1 = 'data/ta_data/' + pair[0].split('/')[2][:-4] + '_ta.csv'\n",
    "        csv_2 = 'data/ta_data/' + pair[1].split('/')[2][:-4] + '_ta.csv'\n",
    "        print('csv1, csv2:', csv_1, csv_2)\n",
    "        \n",
    "        # define exchanges and trading_pairs\n",
    "        ex_tp_1, ex_tp_2 = pair[0][:-8], pair[1][:-8]\n",
    "        print(ex_tp_1)\n",
    "        ex1, tp1 = get_exchange_trading_pair(ex_tp_1)\n",
    "        ex2, tp2 = get_exchange_trading_pair(ex_tp_2)\n",
    "        print(ex1, tp1,  ex2, tp2)\n",
    "        \n",
    "        # define model_name for the filename\n",
    "        model_name = ex1 + '_' + ex_tp_2.split('/')[2]\n",
    "        print(model_name)\n",
    "          \n",
    "        # create dfs from csv's that already include ta features\n",
    "        df1, df2 = pd.read_csv(csv_1, index_col=0), pd.read_csv(csv_2, index_col=0)       \n",
    "        print('df 1 shape: ', df1.shape, 'df 2 shape: ', df2.shape)\n",
    "\n",
    "        # merge dfs\n",
    "        df = merge_dfs(df1, df2)\n",
    "        print('dfs merged')\n",
    "        print('merged df shape:' , df.shape)\n",
    "        \n",
    "\n",
    "        # create target \n",
    "        df = get_target(df)\n",
    "        print(model_name, ' ', df.shape)\n",
    "\n",
    "        # export csv\n",
    "        path = 'data/arb_data/'\n",
    "        csv_filename = path + model_name + '.csv'\n",
    "        df.to_csv(csv_filename)\n",
    "\n",
    "        # print progress\n",
    "        print(f'csv #{n} saved :)')\n",
    "\n",
    "        # update counter\n",
    "        n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_arb_csvs(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to make sure all csv's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "arb_data_paths = glob.glob('arb_data/*.csv')\n",
    "print(len(arb_data_paths)) # 95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic-env",
   "language": "python",
   "name": "cryptolytic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
