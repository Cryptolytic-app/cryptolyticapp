{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptolytic Arbitrage Modeling\n",
    "\n",
    "This notebook contains the code to create the arbitrage models used in the Cryptolytic project. You can find more information on data processing in this [notebook](link) and model evaluation in this [notebook](link).\n",
    "\n",
    "#### Background on Arbitrage Models\n",
    "Arbitrage models were created with the goal of predicting arbitrage 10 min before it happens in an active crypto market. The models are generated by getting all of the combinations of 2 exchanges that support the same trading pair, engineering technical analysis features, merging that data on 'closing_time', engineering more features, and creating a target that signals an arbitrage opportunity. Arbitrage signals predicted by the model have a direction indicating which direction the arbitrage occurs in. A valid arbitrage signal is when the arbitrage lasts >30 mins because it takes time to move coins from one exchange to the other in order to successfully complete the arbitrage trades.\n",
    "\n",
    "The models predict whether there will be an arbitrage opportunity that starts 10 mins after the prediction time and lasts for at least 30 mins, giving a user enough times to execute trades.\n",
    "\n",
    "#### Baseline Logistic Regression\n",
    "\n",
    "#### Baseline Random Forest with default parameters\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "#### Random Forest with hyperparameter tuning\n",
    "\n",
    "More than 6000+ iterations of models were generated in this notebook and the best ones were selected from each possible arbitrage combination based on model selection criteria outlined later in this section. The models were Random Forest Classifier and the best model parameters varied for each dataset. The data was obtained from the respective exchanges via their api, and we did a 70/30 train/test split on 5 min candlestick data that fell anywhere in the range from Jun 2015 - Oct 2019. There was a 2 week gap left between the train and test sets to prevent data leakage. The models return 0 (no arbitrage), 1 (arbitrage from exchange 1 to exchange 2) and -1 (arbitrage from exchange 2 to exchange 1). \n",
    "\n",
    "The profit calculation incorporated fees like in the real world. We used mean percent profit as the profitability metric which represented the average percent profit per arbitrage trade if one were to act on all trades predicted by the model in the testing period, whether those predictions were correct or not.\n",
    "\n",
    "From the 6000+ iterations of models trained, the best models were narrowed down based on the following criteria:\n",
    "- How often the models predicted arbitrage when it didn't exist (False positives)\n",
    "- How many times the models predicted arbitrage correctly (True positives)\n",
    "- How profitable the model was in the real world over the period of the test set. \n",
    "\n",
    "There were 21 models that met the thresholds for model selection critera (details of these models can be found at the end of this nb). The final models were all profitable with gains anywhere from 0.2% - 2.3% within the varied testing time periods (Note: the model with >9% mean percent profit was an outlier). Visualizations for how these models performed can be viewed at https://github.com/Lambda-School-Labs/cryptolytic-ds/blob/master/finalized_notebooks/visualization/arb_performance_visualization.ipynb\n",
    "\n",
    "\\* It is HIGHLY recommended to run this on sagemaker and split the training work onto 4 notebooks. These functions will take over a day to run if not split up. There are 95 total options for models, 75 of those options have enough data to train models, and with different options for parameters around ~6K models will be trained. After selecting for the best models, there were 21 that met the criteria to be included in this project.\n",
    "\n",
    "\\*** There has been some feature selection done in this process where we removed highly correlated features, but not enough. There should be more exploration into whether removing features improves accuracy. \n",
    "\n",
    "\\**** We haven't tried normalizing the dataset to see if it will improve accuracy, but that should be a top priority to anyone continuing this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── cryptolytic/                        <-- The top-level directory for all arbitrage work\n",
    "│   ├── modeling/                       <-- Directory for modeling work\n",
    "│   │      ├──data/                     <-- Directory with subdirectories containing 5 min candle data\n",
    "│   │      │   ├─ arb_data/             <-- Directory for csv files of arbitrage model training data\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ csv_data/             <-- Directory for csv files after combining datasets and FE pt.2\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ ta_data/              <-- Directory for csv files after FE pt.1 \n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ *.zip                 <-- ZIP files of all of the data\n",
    "│   │      │   \n",
    "│   │      ├──final_models/             <-- Directory for final models after model selection\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├──model_perf/               <-- Directory for performance csvs after training models\n",
    "│   │      │      └── *.json\n",
    "│   │      │\n",
    "│   │      ├──models/                   <-- Directory for all pickle models\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_data_processing.ipynb      <-- Notebook for data processing and creating csvs\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_modeling.ipynb             <-- Notebook for baseline models and hyperparam tuning\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_model_selection.ipynb      <-- Notebook for model selection\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_model_evaluation.ipynb     <-- Notebook for final model evaluation\n",
    "│   │      │\n",
    "│   │      ├─environment.yml                      <-- yml file to create conda environment\n",
    "│   │      │\n",
    "│   │      ├─trade_recommender_models.ipynb       <-- Notebook for trade recommender models\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import itertools\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "All the arbitrage datasets that will be used in modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "arb_data_paths = glob.glob('data/arb_data/*.csv')\n",
    "print(len(arb_data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e568906d97ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marb_data_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pd.read_csv(arb_data_paths[0], index_col=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "Note: closing_time feature is being removed before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['close_exchange_1','base_volume_exchange_1', \n",
    "            'nan_ohlcv_exchange_1','volume_adi_exchange_1', 'volume_obv_exchange_1',\n",
    "            'volume_cmf_exchange_1', 'volume_fi_exchange_1','volume_em_exchange_1', \n",
    "            'volume_vpt_exchange_1','volume_nvi_exchange_1', 'volatility_atr_exchange_1',\n",
    "            'volatility_bbhi_exchange_1','volatility_bbli_exchange_1', \n",
    "            'volatility_kchi_exchange_1', 'volatility_kcli_exchange_1',\n",
    "            'volatility_dchi_exchange_1','volatility_dcli_exchange_1',\n",
    "            'trend_macd_signal_exchange_1', 'trend_macd_diff_exchange_1', \n",
    "            'trend_adx_exchange_1', 'trend_adx_pos_exchange_1', \n",
    "            'trend_adx_neg_exchange_1', 'trend_vortex_ind_pos_exchange_1', \n",
    "            'trend_vortex_ind_neg_exchange_1', 'trend_vortex_diff_exchange_1', \n",
    "            'trend_trix_exchange_1', 'trend_mass_index_exchange_1', \n",
    "            'trend_cci_exchange_1', 'trend_dpo_exchange_1', 'trend_kst_sig_exchange_1',\n",
    "            'trend_kst_diff_exchange_1', 'trend_aroon_up_exchange_1',\n",
    "            'trend_aroon_down_exchange_1', 'trend_aroon_ind_exchange_1',\n",
    "            'momentum_rsi_exchange_1', 'momentum_mfi_exchange_1',\n",
    "            'momentum_tsi_exchange_1', 'momentum_uo_exchange_1',\n",
    "            'momentum_stoch_signal_exchange_1', 'momentum_wr_exchange_1', \n",
    "            'momentum_ao_exchange_1', 'others_dr_exchange_1', 'close_exchange_2',\n",
    "            'base_volume_exchange_2', 'nan_ohlcv_exchange_2',\n",
    "            'volume_adi_exchange_2', 'volume_obv_exchange_2',\n",
    "            'volume_cmf_exchange_2', 'volume_fi_exchange_2',\n",
    "            'volume_em_exchange_2', 'volume_vpt_exchange_2',\n",
    "            'volume_nvi_exchange_2', 'volatility_atr_exchange_2',\n",
    "            'volatility_bbhi_exchange_2', 'volatility_bbli_exchange_2',\n",
    "            'volatility_kchi_exchange_2', 'volatility_kcli_exchange_2',\n",
    "            'volatility_dchi_exchange_2', 'volatility_dcli_exchange_2',\n",
    "            'trend_macd_signal_exchange_2',\n",
    "            'trend_macd_diff_exchange_2', 'trend_adx_exchange_2',\n",
    "            'trend_adx_pos_exchange_2', 'trend_adx_neg_exchange_2',\n",
    "            'trend_vortex_ind_pos_exchange_2',\n",
    "            'trend_vortex_ind_neg_exchange_2',\n",
    "            'trend_vortex_diff_exchange_2', 'trend_trix_exchange_2',\n",
    "            'trend_mass_index_exchange_2', 'trend_cci_exchange_2',\n",
    "            'trend_dpo_exchange_2', 'trend_kst_sig_exchange_2',\n",
    "            'trend_kst_diff_exchange_2', 'trend_aroon_up_exchange_2',\n",
    "            'trend_aroon_down_exchange_2',\n",
    "            'trend_aroon_ind_exchange_2',\n",
    "            'momentum_rsi_exchange_2', 'momentum_mfi_exchange_2',\n",
    "            'momentum_tsi_exchange_2', 'momentum_uo_exchange_2',\n",
    "            'momentum_stoch_signal_exchange_2',\n",
    "            'momentum_wr_exchange_2', 'momentum_ao_exchange_2',\n",
    "            'others_dr_exchange_2', 'year', 'month', 'day',\n",
    "            'higher_closing_price', 'pct_higher', \n",
    "            'arbitrage_opportunity', 'window_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = '-------------'\n",
    "sp = '      '\n",
    "\n",
    "def tbl_stats_headings():\n",
    "    \"\"\"Prints the headings for the stats table\"\"\"\n",
    "    print(sp*2, line*9, '\\n', \n",
    "          sp*3, 'Accuracy Score', \n",
    "#           sp, 'True Positive Rate',\n",
    "          sp, 'False Postitive Rate', \n",
    "          sp, 'Precision',\n",
    "          sp, 'Recall',\n",
    "          sp, 'F1', '\\n',\n",
    "          sp*2, line*9, '\\n', \n",
    "    )\n",
    "    \n",
    "def tbl_stats_row(test_accuracy, fpr, precision, recall, f1):\n",
    "    \"\"\"Prints the row of model stats after each param set fold\"\"\"\n",
    "    print(\n",
    "        sp*4, f'{test_accuracy:.4f}',     # accuracy\n",
    "#         sp*3, f'{tpr:.4f}',           # roc auc\n",
    "        sp*3, f'{fpr:.4f}',      # p/r auc\n",
    "        sp*2, f'{precision:.4f}',      # p/r auc\n",
    "        sp*1, f'{recall:.4f}',      # p/r auc\n",
    "        sp*1, f'{f1:.4f}',     # p/r auc\n",
    "        sp*2, line*9\n",
    "    )\n",
    "\n",
    "def print_model_name(name, i, arb_data_paths):\n",
    "    print(\n",
    "    line*9, '\\n\\n', \n",
    "    f'Model {i+1}/{len(arb_data_paths)}: {name}', '\\n', \n",
    "    line*9\n",
    "    )\n",
    "\n",
    "def print_model_params(i, params, pg_list):  \n",
    "    print(\n",
    "        line*9, '\\n', \n",
    "        f'Model {i+1} / {len(pg_list)}', '\\n',  \n",
    "        f'params={params if params else None}', '\\n', \n",
    "        line*9\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(arb_data_paths, model_type, features, param_grid):\n",
    "    \"\"\"\n",
    "    This function takes in a list of all the arbitrage data paths, \n",
    "    does train/test split, feature selection, trains models, \n",
    "    saves the pickle file, gets performance stats for the model, \n",
    "    and returns a dataframe of performance stats and a dictionary\n",
    "    of confusion matrices for each model \n",
    "\n",
    "    Predictions\n",
    "    ___________\n",
    "    \n",
    "    Models predict whether arbitrage is occuring at a given time:\n",
    "    1: Active\n",
    "    0: Inactive\n",
    "    -1: \n",
    "    \n",
    "    Evaluation\n",
    "    __________\n",
    "    \n",
    "    - Cross Validation\n",
    "    - Accuracy Score\n",
    "    - ROC AUC\n",
    "    - Precision/Recall AUC\n",
    "    - ROC curves\n",
    "    - Precision/Recall curves\n",
    "    - Mean Percent Profit\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    \n",
    "    df: a dataframe with columns=['CANONICAL_SMILES', 'MOD', \n",
    "                                  'DOF_IC50_uM', 'Class', 'Source']\n",
    "    param_grid: a dict of hyperparameters for RandomForestClassifier\n",
    "    pc: number of principal components to use in training\n",
    "    n_splits: number of cross validation folds (default=5)\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0\n",
    "    line = '---------------'\n",
    "    \n",
    "    base_model_name = str(model_type).split('(')[0]\n",
    "    model_name_dict = {\n",
    "        'LogisticRegression':'lr',\n",
    "        'RandomForestClassifier':'rf'\n",
    "    }\n",
    "    \n",
    "    # this is in case the function stops running you can pick up where you left off\n",
    "    # get all model paths into a variable\n",
    "    model_paths = glob.glob('models2/*.pkl')\n",
    "    \n",
    "    # pick target\n",
    "    target = 'target'\n",
    "    \n",
    "    # iterate through the arbitrage csvs\n",
    "    for i, file in enumerate(arb_data_paths[:3]):\n",
    "        \n",
    "        # define model name\n",
    "        name = file.split('/')[2][:-8]\n",
    "        \n",
    "        # print status\n",
    "        print_model_name(name, i, arb_data_paths)\n",
    "\n",
    "        # read csv\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # convert str closing_time to datetime\n",
    "        df['closing_time'] = pd.to_datetime(df['closing_time']) \n",
    "        \n",
    "#         X = df[features]\n",
    "#         y = df[target]\n",
    "        \n",
    "        # baseline\n",
    "        if not param_grid:\n",
    "            pg_list = [param_grid]\n",
    "        # create parameter grid\n",
    "        else:\n",
    "            pg_list = list(ParameterGrid(param_grid))\n",
    "                \n",
    "        # cv = TimeSeriesSplit(n_splits)\n",
    "        \n",
    "        for i, params in enumerate(pg_list):    \n",
    "            # define model \n",
    "            # need if else\n",
    "            if param_grid:\n",
    "                model_name = name + '_' + str(max_features) + '_' + str(max_depth) + '_' + str(n_estimators)\n",
    "            else:\n",
    "                model_name = name + '_' + model_name_dict[base_model_name]\n",
    "\n",
    "            # define model filename to check if it exists\n",
    "            model_path = f'models/{model_name}.pkl'\n",
    "\n",
    "            # if the model does not exist\n",
    "            if model_path not in model_paths:\n",
    "                \n",
    "                # print status\n",
    "                print_model_params(i, params, pg_list)\n",
    "                \n",
    "                # 70/30 train/test split\n",
    "                test_train_split_row = round(len(df)*.7)\n",
    "\n",
    "                # get closing_time for t/t split\n",
    "                test_train_split_time = df['closing_time'][test_train_split_row]\n",
    "\n",
    "                # remove 2 weeks from train datasets to create a  \n",
    "                # two week gap between the data - prevents data leakage\n",
    "                train_cutoff_time = test_train_split_time - dt.timedelta(days=14)\n",
    "                print('cutoff time:', train_cutoff_time)\n",
    "\n",
    "                # train and test subsets\n",
    "                train = df[df['closing_time'] < train_cutoff_time]\n",
    "                test = df[df['closing_time'] > train_cutoff_time]\n",
    "        \n",
    "\n",
    "                # get closing_time for t/t split\n",
    "                # remove 2 weeks from train datasets to create a \n",
    "                # two week gap between the data - prevents data leakage\n",
    "#                 train_cutoff_time = X_train['closing_time'].iloc[-1] - dt.timedelta(days=14)\n",
    "#                 print('cutoff time:', train_cutoff_time)\n",
    "\n",
    "#                 # train and test subsets\n",
    "#                 X_train = X_train[X_train['closing_time'] < train_cutoff_time]\n",
    "#                 y_train = y_train[:X_train.shape[0]]\n",
    "\n",
    "                # X, y matrix\n",
    "                X_train = train[features]\n",
    "                X_test = test[features]\n",
    "                y_train = train[target]\n",
    "                y_test = test[target]\n",
    "\n",
    "#                 X_train = X_train.drop(columns='closing_time')\n",
    "#                 X_test = X_test.drop(columns='closing_time')\n",
    "\n",
    "                # printing shapes to track progress\n",
    "                print('train and test shape: ', train.shape, test.shape)\n",
    "\n",
    "                # filter out datasets that are too small\n",
    "                if (X_train.shape[0] > 1000) and (X_test.shape[0] > 100):\n",
    "\n",
    "                    # instantiate model\n",
    "                    model = model_type.set_params(**params)\n",
    "\n",
    "                    # there was a weird error caused by two of the datasets which\n",
    "                    # is why this try/except is needed to keep the function running\n",
    "#                         try:\n",
    "\n",
    "                    # fit model\n",
    "                    model = model.fit(X_train, y_train)\n",
    "                    print('model fitted!')\n",
    "\n",
    "                    # make predictions\n",
    "                    y_preds = model.predict(X_test)\n",
    "                    print('predictions made!')\n",
    "\n",
    "                    # test accuracy\n",
    "                    score = accuracy_score(y_test, y_preds)\n",
    "                    print('test accuracy:', score)\n",
    "\n",
    "#                     print(classification_report(y_test, y_preds))\n",
    "                    # fpr\n",
    "#                     print(confusion_matrix(y_test, y_preds))\n",
    "                    fpr = confusion_matrix(y_test, y_preds)[0][2]\n",
    "                    \n",
    "                    \n",
    "                    # precision, recall, f1 score, supp\n",
    "                    precision, recall, f1, supp = precision_recall_fscore_support(y_test, y_preds, average='weighted')\n",
    "\n",
    "                    print(roc_auc_score(y_test, y_preds))\n",
    "                    \n",
    "                    # save model\n",
    "                    pickle.dump(model, open('models/{model_name}.pkl'.format(\n",
    "                                model_name=model_name), 'wb'))\n",
    "                    print('pickle saved!'.format(model_name=model_name))\n",
    "\n",
    "#                         except:\n",
    "#                             print(line*3 + '\\n' + line + 'ERROR' + line + '\\n' + line*3)\n",
    "#                             break # break out of for loop if there is an error with modeling\n",
    "\n",
    "\n",
    "                # dataset is too small\n",
    "                else:\n",
    "                    fpr, precision, recall, f1 = .00001, .00001, .00001, .00001\n",
    "                    print(f'ERROR: dataset too small for {name}')\n",
    "                \n",
    "                # print status\n",
    "                tbl_stats_headings()\n",
    "                tbl_stats_row(score, fpr, precision, recall, f1)\n",
    "\n",
    "            # if the model exists\n",
    "            else:\n",
    "                print(f'Model {i}/{len(arb_data_paths)} already exists.')\n",
    "       \n",
    "        \n",
    "        # update count\n",
    "        # TODO: make a better counter that is actually useful in \n",
    "        # showing how much is left \n",
    "        counter += 1\n",
    "        print(counter, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      " Model 1/95: kraken_bitfinex_bch \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      " Model 1 / 1 \n",
      " params=None \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "cutoff time: 2019-10-11 07:00:00\n",
      "train and test shape:  (2938, 141) (7018, 141)\n",
      "model fitted!\n",
      "predictions made!\n",
      "test accuracy: 0.9955827871188373\n",
      "0.37692472198460225\n",
      "pickle saved!\n",
      "             --------------------------------------------------------------------------------------------------------------------- \n",
      "                    Accuracy Score        False Postitive Rate        Precision        Recall        F1 \n",
      "              --------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "                         0.9956                    1.0000              0.9990        0.9956        0.9973              ---------------------------------------------------------------------------------------------------------------------\n",
      "1 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      " Model 2/95: kraken_gemini_eth \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      " Model 1 / 1 \n",
      " params=None \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "cutoff time: 2019-10-21 06:05:00\n",
      "train and test shape:  (1103, 141) (6232, 141)\n",
      "model fitted!\n",
      "predictions made!\n",
      "test accuracy: 0.9221758664955071\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-92dea61943ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marb_data_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marb_data_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-312-6d035d6c4e35>\u001b[0m in \u001b[0;36mcreate_models\u001b[0;34m(arb_data_paths, model_type, features, param_grid)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    353\u001b[0m     return _average_binary_score(\n\u001b[1;32m    354\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preformance_metric():\n",
    "    ############## Performance metrics ###############\n",
    "    # TODO: put this all in a function and just return the \n",
    "    # metrics we want\n",
    "\n",
    "    performance_list = []\n",
    "    confusion_dict = {}\n",
    "    \n",
    "    \n",
    "    # labels for confusion matrix\n",
    "    unique_y_test = y_test.unique().tolist()\n",
    "    unique_y_preds = list(set(y_preds))\n",
    "    labels = list(set(unique_y_test + unique_y_preds))\n",
    "    labels.sort()\n",
    "    columns = [f'Predicted {label}' for label in labels]\n",
    "    index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "    # create confusion matrix\n",
    "    confusion = pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "                             columns=columns, index=index)\n",
    "    print(model_name + ' confusion matrix:')\n",
    "    print(confusion, '\\n')\n",
    "\n",
    "    # append to confusion list\n",
    "    confusion_dict[model_name] = confusion\n",
    "\n",
    "    # creating dataframe from test set to calculate profitability\n",
    "    test_with_preds = X_test.copy()\n",
    "\n",
    "    # add column with higher closing price\n",
    "    test_with_preds['higher_closing_price'] = test_with_preds.apply(\n",
    "            get_higher_closing_price, axis=1)\n",
    "\n",
    "    # add column with shifted closing price\n",
    "    test_with_preds = get_close_shift(test_with_preds)\n",
    "\n",
    "    # adding column with predictions\n",
    "    test_with_preds['pred'] = y_preds\n",
    "\n",
    "    # adding column with profitability of predictions\n",
    "    test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "            get_profit, axis=1).shift(-2)\n",
    "\n",
    "    # filtering out rows where no arbitrage is predicted\n",
    "    test_with_preds = test_with_preds[test_with_preds['pred'] != 0]\n",
    "\n",
    "    # calculating mean profit where arbitrage predicted...\n",
    "    pct_profit_mean = test_with_preds['pct_profit'].mean()\n",
    "\n",
    "    # calculating median profit where arbitrage predicted...\n",
    "    pct_profit_median = test_with_preds['pct_profit'].median()\n",
    "    print('percent profit mean:', pct_profit_mean)\n",
    "    print('percent profit median:', pct_profit_median, '\\n\\n')\n",
    "\n",
    "    # save net performance to list\n",
    "    performance_list.append([name, max_features, max_depth, n_estimators,\n",
    "                             pct_profit_mean, pct_profit_median])\n",
    "    ######################## END OF TODO ###########################\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Models w/ default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = pd.Series(xg.feature_importances_, X_train.columns)\n",
    "n = 25\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_features': ['auto', 42, 44, 46],\n",
    "    'n_estimators': [250, 300],\n",
    "    'max_depth': [30, 35, 45, 50]\n",
    "}\n",
    "\n",
    "create_models(\n",
    "    arb_data_paths=arb_data_paths, \n",
    "    model_type=LogisticRegression(), \n",
    "    features=features, param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic-env",
   "language": "python",
   "name": "cryptolytic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
