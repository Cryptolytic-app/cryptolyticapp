{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptolytic Arbitrage Modeling\n",
    "\n",
    "This notebook contains the code to create the arbitrage models used in the Cryptolytic project. You can find more information on data processing in this [notebook](link) and model evaluation in this [notebook](link).\n",
    "\n",
    "#### Background on Arbitrage Models\n",
    "Arbitrage models were created with the goal of predicting arbitrage 10 min before it happens in an active crypto market. The models are generated by getting all of the combinations of 2 exchanges that support the same trading pair, engineering technical analysis features, merging that data on 'closing_time', engineering more features, and creating a target that signals an arbitrage opportunity. Arbitrage signals predicted by the model have a direction indicating which direction the arbitrage occurs in. A valid arbitrage signal is when the arbitrage lasts >30 mins because it takes time to move coins from one exchange to the other in order to successfully complete the arbitrage trades.\n",
    "\n",
    "The models predict whether there will be an arbitrage opportunity that starts 10 mins after the prediction time and lasts for at least 30 mins, giving a user enough times to execute trades.\n",
    "\n",
    "#### Baseline Logistic Regression\n",
    "\n",
    "#### Baseline Random Forest with default parameters\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "#### Random Forest with hyperparameter tuning\n",
    "\n",
    "More than 6000+ iterations of models were generated in this notebook and the best ones were selected from each possible arbitrage combination based on model selection criteria outlined later in this section. The models were Random Forest Classifier and the best model parameters varied for each dataset. The data was obtained from the respective exchanges via their api, and we did a 70/30 train/test split on 5 min candlestick data that fell anywhere in the range from Jun 2015 - Oct 2019. There was a 2 week gap left between the train and test sets to prevent data leakage. The models return 0 (no arbitrage), 1 (arbitrage from exchange 1 to exchange 2) and -1 (arbitrage from exchange 2 to exchange 1). \n",
    "\n",
    "The profit calculation incorporated fees like in the real world. We used mean percent profit as the profitability metric which represented the average percent profit per arbitrage trade if one were to act on all trades predicted by the model in the testing period, whether those predictions were correct or not.\n",
    "\n",
    "From the 6000+ iterations of models trained, the best models were narrowed down based on the following criteria:\n",
    "- How often the models predicted arbitrage when it didn't exist (False positives)\n",
    "- How many times the models predicted arbitrage correctly (True positives)\n",
    "- How profitable the model was in the real world over the period of the test set. \n",
    "\n",
    "There were 21 models that met the thresholds for model selection critera (details of these models can be found at the end of this nb). The final models were all profitable with gains anywhere from 0.2% - 2.3% within the varied testing time periods (Note: the model with >9% mean percent profit was an outlier). Visualizations for how these models performed can be viewed at https://github.com/Lambda-School-Labs/cryptolytic-ds/blob/master/finalized_notebooks/visualization/arb_performance_visualization.ipynb\n",
    "\n",
    "\\* It is HIGHLY recommended to run this on sagemaker and split the training work onto 4 notebooks. These functions will take over a day to run if not split up. There are 95 total options for models, 75 of those options have enough data to train models, and with different options for parameters around ~6K models will be trained. After selecting for the best models, there were 21 that met the criteria to be included in this project.\n",
    "\n",
    "\\*** There has been some feature selection done in this process where we removed highly correlated features, but not enough. There should be more exploration into whether removing features improves accuracy. \n",
    "\n",
    "\\**** We haven't tried normalizing the dataset to see if it will improve accuracy, but that should be a top priority to anyone continuing this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directory Structure\n",
    "```\n",
    "├── cryptolytic/                        <-- The top-level directory for all arbitrage work\n",
    "│   ├── modeling/                       <-- Directory for modeling work\n",
    "│   │      ├──data/                     <-- Directory with subdirectories containing 5 min candle data\n",
    "│   │      │   ├─ arb_data/             <-- Directory for csv files of arbitrage model training data\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ csv_data/             <-- Directory for csv files after combining datasets and FE pt.2\n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ ta_data/              <-- Directory for csv files after FE pt.1 \n",
    "│   │      │   │   └── *.csv\n",
    "│   │      │   │\n",
    "│   │      │   ├─ *.zip                 <-- ZIP files of all of the data\n",
    "│   │      │   \n",
    "│   │      ├──final_models/             <-- Directory for final models after model selection\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├──model_perf/               <-- Directory for performance csvs after training models\n",
    "│   │      │      └── *.json\n",
    "│   │      │\n",
    "│   │      ├──models/                   <-- Directory for all pickle models\n",
    "│   │      │      └── *.pkl\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_data_processing.ipynb      <-- Notebook for data processing and creating csvs\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_modeling.ipynb             <-- Notebook for baseline models and hyperparam tuning\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_model_selection.ipynb      <-- Notebook for model selection\n",
    "│   │      │\n",
    "│   │      ├─arbitrage_model_evaluation.ipynb     <-- Notebook for final model evaluation\n",
    "│   │      │\n",
    "│   │      ├─environment.yml                      <-- yml file to create conda environment\n",
    "│   │      │\n",
    "│   │      ├─trade_recommender_models.ipynb       <-- Notebook for trade recommender models\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This project uses conda to manage environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to update your conda env from a yml file from terminal\n",
    "# conda env update --file modeling/environment.yml\n",
    "\n",
    "# to export yml from terminal\n",
    "# conda env export > modeling/environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import itertools\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Store all the arbitrage datasets that will be used in modeling into a list variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "arb_data_paths = glob.glob('data/arb_data/*.csv')\n",
    "print(len(arb_data_paths)) #95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset looks like this and should have 141 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(451520, 141)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_exchange_1</th>\n",
       "      <th>high_exchange_1</th>\n",
       "      <th>low_exchange_1</th>\n",
       "      <th>close_exchange_1</th>\n",
       "      <th>base_volume_exchange_1</th>\n",
       "      <th>nan_ohlcv_exchange_1</th>\n",
       "      <th>volume_adi_exchange_1</th>\n",
       "      <th>volume_obv_exchange_1</th>\n",
       "      <th>volume_cmf_exchange_1</th>\n",
       "      <th>volume_fi_exchange_1</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>higher_closing_price</th>\n",
       "      <th>pct_higher</th>\n",
       "      <th>arbitrage_opportunity</th>\n",
       "      <th>window_length</th>\n",
       "      <th>arbitrage_opportunity_shift</th>\n",
       "      <th>window_length_shift</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>278.04</td>\n",
       "      <td>278.05</td>\n",
       "      <td>277.99</td>\n",
       "      <td>277.99</td>\n",
       "      <td>9.115813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.656990</td>\n",
       "      <td>-98329.562325</td>\n",
       "      <td>-0.394465</td>\n",
       "      <td>2.499380</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>277.96</td>\n",
       "      <td>278.25</td>\n",
       "      <td>277.95</td>\n",
       "      <td>278.19</td>\n",
       "      <td>16.981369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.073009</td>\n",
       "      <td>-98312.580956</td>\n",
       "      <td>-0.341762</td>\n",
       "      <td>2.166029</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071945</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>278.15</td>\n",
       "      <td>278.25</td>\n",
       "      <td>278.15</td>\n",
       "      <td>278.23</td>\n",
       "      <td>8.926014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.544430</td>\n",
       "      <td>-98303.654941</td>\n",
       "      <td>-0.322681</td>\n",
       "      <td>-0.045552</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086334</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>278.21</td>\n",
       "      <td>278.23</td>\n",
       "      <td>278.21</td>\n",
       "      <td>278.23</td>\n",
       "      <td>1.613097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.968705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.331044</td>\n",
       "      <td>-0.614731</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.104339</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>278.22</td>\n",
       "      <td>278.22</td>\n",
       "      <td>278.22</td>\n",
       "      <td>278.22</td>\n",
       "      <td>0.059885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.613097</td>\n",
       "      <td>-98303.714827</td>\n",
       "      <td>-0.504553</td>\n",
       "      <td>0.088661</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079137</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   open_exchange_1  high_exchange_1  low_exchange_1  close_exchange_1  \\\n",
       "0           278.04           278.05          277.99            277.99   \n",
       "1           277.96           278.25          277.95            278.19   \n",
       "2           278.15           278.25          278.15            278.23   \n",
       "3           278.21           278.23          278.21            278.23   \n",
       "4           278.22           278.22          278.22            278.22   \n",
       "\n",
       "   base_volume_exchange_1  nan_ohlcv_exchange_1  volume_adi_exchange_1  \\\n",
       "0                9.115813                   0.0             -11.656990   \n",
       "1               16.981369                   0.0               1.073009   \n",
       "2                8.926014                   0.0              15.544430   \n",
       "3                1.613097                   0.0               6.968705   \n",
       "4                0.059885                   0.0               1.613097   \n",
       "\n",
       "   volume_obv_exchange_1  volume_cmf_exchange_1  volume_fi_exchange_1  ...  \\\n",
       "0          -98329.562325              -0.394465              2.499380  ...   \n",
       "1          -98312.580956              -0.341762              2.166029  ...   \n",
       "2          -98303.654941              -0.322681             -0.045552  ...   \n",
       "3               0.000000              -0.331044             -0.614731  ...   \n",
       "4          -98303.714827              -0.504553              0.088661  ...   \n",
       "\n",
       "   year  month  day  higher_closing_price  pct_higher  arbitrage_opportunity  \\\n",
       "0  2015      7   20                     1    0.007195                      0   \n",
       "1  2015      7   20                     1    0.071945                      0   \n",
       "2  2015      7   20                     1    0.086334                      0   \n",
       "3  2015      7   20                     1    0.104339                      0   \n",
       "4  2015      7   20                     1    0.079137                      0   \n",
       "\n",
       "   window_length  arbitrage_opportunity_shift  window_length_shift  target  \n",
       "0             10                          0.0                 45.0       0  \n",
       "1             15                          0.0                 50.0       0  \n",
       "2             20                          0.0                 55.0       0  \n",
       "3             25                          0.0                 60.0       0  \n",
       "4             30                          0.0                 65.0       0  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(arb_data_paths[0], index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "Note: closing_time feature is being removed before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['close_exchange_1','base_volume_exchange_1', \n",
    "            'nan_ohlcv_exchange_1','volume_adi_exchange_1', 'volume_obv_exchange_1',\n",
    "            'volume_cmf_exchange_1', 'volume_fi_exchange_1','volume_em_exchange_1', \n",
    "            'volume_vpt_exchange_1','volume_nvi_exchange_1', 'volatility_atr_exchange_1',\n",
    "            'volatility_bbhi_exchange_1','volatility_bbli_exchange_1', \n",
    "            'volatility_kchi_exchange_1', 'volatility_kcli_exchange_1',\n",
    "            'volatility_dchi_exchange_1','volatility_dcli_exchange_1',\n",
    "            'trend_macd_signal_exchange_1', 'trend_macd_diff_exchange_1', \n",
    "            'trend_adx_exchange_1', 'trend_adx_pos_exchange_1', \n",
    "            'trend_adx_neg_exchange_1', 'trend_vortex_ind_pos_exchange_1', \n",
    "            'trend_vortex_ind_neg_exchange_1', 'trend_vortex_diff_exchange_1', \n",
    "            'trend_trix_exchange_1', 'trend_mass_index_exchange_1', \n",
    "            'trend_cci_exchange_1', 'trend_dpo_exchange_1', 'trend_kst_sig_exchange_1',\n",
    "            'trend_kst_diff_exchange_1', 'trend_aroon_up_exchange_1',\n",
    "            'trend_aroon_down_exchange_1', 'trend_aroon_ind_exchange_1',\n",
    "            'momentum_rsi_exchange_1', 'momentum_mfi_exchange_1',\n",
    "            'momentum_tsi_exchange_1', 'momentum_uo_exchange_1',\n",
    "            'momentum_stoch_signal_exchange_1', 'momentum_wr_exchange_1', \n",
    "            'momentum_ao_exchange_1', 'others_dr_exchange_1', 'close_exchange_2',\n",
    "            'base_volume_exchange_2', 'nan_ohlcv_exchange_2',\n",
    "            'volume_adi_exchange_2', 'volume_obv_exchange_2',\n",
    "            'volume_cmf_exchange_2', 'volume_fi_exchange_2',\n",
    "            'volume_em_exchange_2', 'volume_vpt_exchange_2',\n",
    "            'volume_nvi_exchange_2', 'volatility_atr_exchange_2',\n",
    "            'volatility_bbhi_exchange_2', 'volatility_bbli_exchange_2',\n",
    "            'volatility_kchi_exchange_2', 'volatility_kcli_exchange_2',\n",
    "            'volatility_dchi_exchange_2', 'volatility_dcli_exchange_2',\n",
    "            'trend_macd_signal_exchange_2',\n",
    "            'trend_macd_diff_exchange_2', 'trend_adx_exchange_2',\n",
    "            'trend_adx_pos_exchange_2', 'trend_adx_neg_exchange_2',\n",
    "            'trend_vortex_ind_pos_exchange_2',\n",
    "            'trend_vortex_ind_neg_exchange_2',\n",
    "            'trend_vortex_diff_exchange_2', 'trend_trix_exchange_2',\n",
    "            'trend_mass_index_exchange_2', 'trend_cci_exchange_2',\n",
    "            'trend_dpo_exchange_2', 'trend_kst_sig_exchange_2',\n",
    "            'trend_kst_diff_exchange_2', 'trend_aroon_up_exchange_2',\n",
    "            'trend_aroon_down_exchange_2',\n",
    "            'trend_aroon_ind_exchange_2',\n",
    "            'momentum_rsi_exchange_2', 'momentum_mfi_exchange_2',\n",
    "            'momentum_tsi_exchange_2', 'momentum_uo_exchange_2',\n",
    "            'momentum_stoch_signal_exchange_2',\n",
    "            'momentum_wr_exchange_2', 'momentum_ao_exchange_2',\n",
    "            'others_dr_exchange_2', 'year', 'month', 'day',\n",
    "            'higher_closing_price', 'pct_higher', \n",
    "            'arbitrage_opportunity', 'window_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = '-------------'\n",
    "sp = '      '\n",
    "\n",
    "def tbl_stats_headings():\n",
    "    \"\"\"Prints the headings for the stats table\"\"\"\n",
    "    print(sp*2, line*9, '\\n', \n",
    "          sp*3, 'Accuracy Score', \n",
    "#           sp, 'True Positive Rate',\n",
    "#           sp, 'False Postitive Rate', \n",
    "          sp, 'Precision',\n",
    "          sp, 'Recall',\n",
    "          sp, 'F1', '\\n',\n",
    "          sp*2, line*9, '\\n', \n",
    "    )\n",
    "    \n",
    "def tbl_stats_row(test_accuracy, precision, recall, f1):\n",
    "    \"\"\"Prints the row of model stats after each param set fold\"\"\"\n",
    "    print(\n",
    "        sp*4, f'{test_accuracy:.4f}',     # accuracy\n",
    "#         sp*3, f'{tpr:.4f}',           # roc auc\n",
    "#         sp*3, f'{fpr:.4f}',      # p/r auc\n",
    "        sp*2, f'{precision:.4f}',      # p/r auc\n",
    "        sp*1, f'{recall:.4f}',      # p/r auc\n",
    "        sp*1, f'{f1:.4f}',     # p/r auc\n",
    "        sp*2, line*9\n",
    "    )\n",
    "\n",
    "def print_model_name(name, i, arb_data_paths):\n",
    "    print(\n",
    "    '\\n\\n', line*9, '\\n\\n', \n",
    "    f'Model {i+1}/{len(arb_data_paths)}: {name}', '\\n', \n",
    "    line*9\n",
    "    )\n",
    "\n",
    "def print_model_params(i, params, pg_list):  \n",
    "    print(\n",
    "        sp*2, line*5, '\\n', \n",
    "        sp*2, f'Model {i+1}/{len(pg_list)}', '\\n',  \n",
    "        sp*2, f'params={params if params else None}', '\\n', \n",
    "        sp*2, line*5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for calculating profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying arbitrage window length to target, in minutes\n",
    "interval = 30\n",
    "\n",
    "def get_higher_closing_price(df):\n",
    "    \"\"\"\n",
    "    Returns the exchange with the higher closing price\n",
    "    \"\"\"\n",
    "    # exchange 1 has higher closing price\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 1\n",
    "    \n",
    "    # exchange 2 has higher closing price\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 2\n",
    "    \n",
    "    # closing prices are equivalent\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_close_shift(df, interval=interval):\n",
    "    \"\"\"\n",
    "    Shifts the closing prices by the selected interval +\n",
    "    10 mins.\n",
    "    \n",
    "    Returns a df with new features:\n",
    "    - close_exchange_1_shift\n",
    "    - close_exchange_2_shift\n",
    "    \"\"\"\n",
    "    \n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    \n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(\n",
    "        rows_to_shift - 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_profit(df):\n",
    "    \"\"\"\n",
    "    Calculates the profit of an arbitrage trade.\n",
    "    \n",
    "    Returns df with new profit feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if exchange 1 has the higher closing price\n",
    "    if df['higher_closing_price'] == 1:\n",
    "        \n",
    "        # return how much money you would make if you bought \n",
    "        # on exchange 2, sold on exchange 1, and took account \n",
    "        # of 0.55% fees\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    \n",
    "    # if exchange 2 has the higher closing price\n",
    "    elif df['higher_closing_price'] == 2:\n",
    "        \n",
    "        # return how much money you would make if you bought \n",
    "        # on exchange 1, sold on exchange 2, and took account \n",
    "        # of 0.55% fees\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    \n",
    "    # if the closing prices are the same\n",
    "    else:\n",
    "        return 0 # no arbitrage\n",
    "\n",
    "def profit(X_test, y_preds):  \n",
    "    # creating dataframe from test set to calculate profitability\n",
    "    test_with_preds = X_test.copy()\n",
    "\n",
    "    # add column with higher closing price\n",
    "    test_with_preds['higher_closing_price'] = test_with_preds.apply(\n",
    "            get_higher_closing_price, axis=1)\n",
    "\n",
    "    # add column with shifted closing price\n",
    "    test_with_preds = get_close_shift(test_with_preds)\n",
    "\n",
    "    # adding column with predictions\n",
    "    test_with_preds['pred'] = y_preds\n",
    "\n",
    "    # adding column with profitability of predictions\n",
    "    test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "            get_profit, axis=1).shift(-2)\n",
    "\n",
    "    # filtering out rows where no arbitrage is predicted\n",
    "    test_with_preds = test_with_preds[test_with_preds['pred'] != 0]\n",
    "\n",
    "    # calculating mean profit where arbitrage predicted...\n",
    "    pct_profit_mean = round(test_with_preds['pct_profit'].mean(), 2)\n",
    "\n",
    "    # calculating median profit where arbitrage predicted...\n",
    "    pct_profit_median = round(test_with_preds['pct_profit'].median(), 2)\n",
    "    \n",
    "    return pct_profit_mean, pct_profit_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(arb_data_paths, model_type, features, param_grid):\n",
    "    \"\"\"\n",
    "    This function takes in a list of all the arbitrage data paths, \n",
    "    does train/test split, feature selection, trains models, \n",
    "    saves the pickle file, and prints performance stats for each model\n",
    "\n",
    "    Predictions\n",
    "    ___________\n",
    "    \n",
    "    Models predict whether arbitrage will in 10 mins from the \n",
    "    prediction time, and last for at least 30 mins:\n",
    "    1: arbitrage from exchange 1 to exchange 2\n",
    "    0: no arbitrage\n",
    "    -1: arbitrage from exchange 2 to exchange 1\n",
    "    \n",
    "    Evaluation\n",
    "    __________\n",
    "    \n",
    "    - Accuracy Score\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 score\n",
    "    - Mean Percent Profit\n",
    "    - Median Percent Profit\n",
    "\n",
    "    Parameters\n",
    "    __________\n",
    "    \n",
    "    arb_data_paths: filepaths for all the datasets used in modeling\n",
    "    model_type: scikit-learn model (LogisticRegression() or \n",
    "        RandomForestClassifier())\n",
    "    features: the features for training or empty [] for all features\n",
    "    param_grid: the params used for hyperparameter tuning or empty {} \n",
    "    \"\"\"\n",
    "    \n",
    "    base_model_name = str(model_type).split('(')[0]\n",
    "    model_name_dict = {\n",
    "        'LogisticRegression':'lr',\n",
    "        'RandomForestClassifier':'rf'\n",
    "    }\n",
    "    \n",
    "    # this is part of a check put in the code to allow the function\n",
    "    # to pick up where it previously left off in case of errors\n",
    "    model_paths = glob.glob('models2/*.pkl')\n",
    "    \n",
    "    # pick target\n",
    "    target = 'target'\n",
    "\n",
    "    # iterate through the arbitrage csvs\n",
    "    for i, file in enumerate(arb_data_paths):\n",
    "        \n",
    "        # define model name\n",
    "        name = file.split('/')[2][:-8]\n",
    "        \n",
    "        # print status\n",
    "        print_model_name(name, i, arb_data_paths)\n",
    "\n",
    "        # read csv\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # convert str closing_time to datetime\n",
    "        df['closing_time'] = pd.to_datetime(df['closing_time'])\n",
    "        \n",
    "        # this makes the function dynamic for whether you want\n",
    "        # to select features or hyperparameters or not\n",
    "        # selected features and hyperparameters\n",
    "        if features and param_grid:\n",
    "            pg_list = list(ParameterGrid(param_grid))\n",
    "        # if theres features and no parms\n",
    "        elif features and not param_grid:\n",
    "            pg_list = [param_grid]\n",
    "        # if theres params and no features\n",
    "        elif not features and param_grid:\n",
    "            pg_list = [param_grid]\n",
    "        # baseline: no features and no params\n",
    "        else:\n",
    "            features = df.drop(\n",
    "                labels=['target', 'closing_time'], \n",
    "                axis=1\n",
    "            ).columns.to_list()\n",
    "            pg_list = [param_grid]\n",
    "\n",
    "        # hyperparameter tuning\n",
    "        for i, params in enumerate(pg_list):    \n",
    "            # define model name\n",
    "            if param_grid:\n",
    "                model_name = '_'.join([\n",
    "                    name, \n",
    "                    str(max_features), \n",
    "                    str(max_depth), \n",
    "                    str(n_estimators)\n",
    "                ])\n",
    "            else:\n",
    "                model_name = name + '_' + model_name_dict[base_model_name]\n",
    "\n",
    "            # define model filename to check if it exists\n",
    "            model_path = f'models/{model_name}.pkl'\n",
    "\n",
    "            # if the model does not exist\n",
    "            if model_path not in model_paths:\n",
    "                \n",
    "                # print status\n",
    "                print_model_params(i, params, pg_list)\n",
    "\n",
    "                # remove 2 weeks from train datasets to create a  \n",
    "                # two week gap between the data - prevents data leakage\n",
    "                tt_split_row = round(len(df)*.7)\n",
    "                tt_split_time = df['closing_time'][tt_split_row]\n",
    "                cutoff_time = tt_split_time - dt.timedelta(days=14)\n",
    "\n",
    "                # train and test subsets\n",
    "                train = df[df['closing_time'] < cutoff_time]\n",
    "                test = df[df['closing_time'] > tt_split_time]\n",
    "\n",
    "                # X, y matrix\n",
    "                X_train = train[features]\n",
    "                X_test = test[features]\n",
    "                y_train = train[target]\n",
    "                y_test = test[target]\n",
    "            \n",
    "                # printing shapes to track progress\n",
    "                print(sp*2, 'train and test shape: ', train.shape, test.shape)\n",
    "  \n",
    "                # filter out datasets that are too small\n",
    "                if ((X_train.shape[0] > 1000) \n",
    "                    and (X_test.shape[0] > 100) \n",
    "                    and len(set(y_train)) > 1):\n",
    "\n",
    "                    # instantiate model\n",
    "                    model = model_type.set_params(**params)\n",
    "\n",
    "                    # there was a weird error caused by two of the datasets which\n",
    "                    # is why this try/except is needed to keep the function running\n",
    "#                         try:\n",
    "\n",
    "                    # fit model\n",
    "                    model = model.fit(X_train, y_train)\n",
    "        \n",
    "                    # make predictions\n",
    "                    y_preds = model.predict(X_test)\n",
    "                \n",
    "                    pct_prof_mean, pct_prof_median = profit(X_test, y_preds)\n",
    "                    print(sp*2,'percent profit mean:', pct_prof_mean)\n",
    "                    print(sp*2, 'percent profit median:', pct_prof_median, '\\n\\n')\n",
    "                    \n",
    "                    # classification report\n",
    "                    print(classification_report(y_test, y_preds))\n",
    "                        \n",
    "                    # save model\n",
    "                    pickle.dump(model, open(f'models/{model_name}.pkl', 'wb'))\n",
    "\n",
    "#                         except:\n",
    "#                             print(line*3 + '\\n' + line + 'ERROR' + line + '\\n' + line*3)\n",
    "#                             break # break out of for loop if there is an error with modeling\n",
    "\n",
    "                # dataset is too small\n",
    "                else:\n",
    "                    print(f'{sp*2}ERROR: dataset too small for {name}')\n",
    "\n",
    "            # the model exists\n",
    "            else:\n",
    "                print(f'{sp*2}{model_path} already exists.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      " Model 1/95: kraken_bitfinex_bch \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      " Model 1 / 1 \n",
      " params=None \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "cutoff time: 2019-10-11 07:00:00\n",
      "train and test shape:  (2938, 141) (7018, 141)\n",
      "model fitted!\n",
      "predictions made!\n",
      "test accuracy: 0.9955827871188373\n",
      "0.37692472198460225\n",
      "pickle saved!\n",
      "             --------------------------------------------------------------------------------------------------------------------- \n",
      "                    Accuracy Score        False Postitive Rate        Precision        Recall        F1 \n",
      "              --------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      "                         0.9956                    1.0000              0.9990        0.9956        0.9973              ---------------------------------------------------------------------------------------------------------------------\n",
      "1 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      "\n",
      " Model 2/95: kraken_gemini_eth \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------------- \n",
      " Model 1 / 1 \n",
      " params=None \n",
      " ---------------------------------------------------------------------------------------------------------------------\n",
      "cutoff time: 2019-10-21 06:05:00\n",
      "train and test shape:  (1103, 141) (6232, 141)\n",
      "model fitted!\n",
      "predictions made!\n",
      "test accuracy: 0.9221758664955071\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-92dea61943ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marb_data_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marb_data_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-312-6d035d6c4e35>\u001b[0m in \u001b[0;36mcreate_models\u001b[0;34m(arb_data_paths, model_type, features, param_grid)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    353\u001b[0m     return _average_binary_score(\n\u001b[1;32m    354\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbitrage_combination</th>\n",
       "      <th>ex1_to_ex2_arb</th>\n",
       "      <th>ex2_to_ex1_arb</th>\n",
       "      <th>no_arb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bitfinex_cbpro_btc_usd</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    arbitrage_combination  ex1_to_ex2_arb  ex2_to_ex1_arb  no_arb\n",
       "0  bitfinex_cbpro_btc_usd            0.08            0.19    0.73"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_distribution(arb_data_paths):\n",
    "    \"\"\"\n",
    "    Returns the class distribution for all arbitrage\n",
    "    datasets in a df\n",
    "    \"\"\"\n",
    "    dist_df = pd.DataFrame(columns=[\n",
    "        'arbitrage_combination', \n",
    "        'ex1_to_ex2_arb', \n",
    "        'ex2_to_ex1_arb',\n",
    "        'no_arb'\n",
    "    ])\n",
    "    for path in arb_data_paths:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        arbitrage_combination = path.split('/')[2][:-4]\n",
    "        no_arb = round(df.target.value_counts()[0] / df.target.value_counts().sum(), 2)\n",
    "        ex1_to_ex2_arb = round(df.target.value_counts()[1] / df.target.value_counts().sum(), 2)\n",
    "        ex2_to_ex1_arb = round(df.target.value_counts()[-1] / df.target.value_counts().sum(), 2)\n",
    "        dist_dict = {\n",
    "            'arbitrage_combination': arbitrage_combination,\n",
    "            'ex1_to_ex2_arb': ex1_to_ex2_arb,\n",
    "            'ex2_to_ex1_arb': ex2_to_ex1_arb,\n",
    "            'no_arb': no_arb\n",
    "        }\n",
    "        dist_df = dist_df.append(dist_dict, ignore_index=True)\n",
    "    return dist_df\n",
    "\n",
    "dist_df = class_distribution(arb_data_paths)\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Models w/ default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models(arb_data_paths=arb_data_paths, model_type=LogisticRegression(), features=features, param_grid={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = pd.Series(xg.feature_importances_, X_train.columns)\n",
    "n = 25\n",
    "plt.figure(figsize=(10,n/2))\n",
    "plt.title(f'Top {n} features')\n",
    "importances.sort_values()[-n:].plot.barh(color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_features': ['auto', 42, 44, 46],\n",
    "    'n_estimators': [250, 300],\n",
    "    'max_depth': [30, 35, 45, 50]\n",
    "}\n",
    "\n",
    "create_models(\n",
    "    arb_data_paths=arb_data_paths, \n",
    "    model_type=LogisticRegression(), \n",
    "    features=features, param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preformance_metric():\n",
    "    ############## Performance metrics ###############\n",
    "    # TODO: put this all in a function and just return the \n",
    "    # metrics we want\n",
    "\n",
    "    performance_list = []\n",
    "    confusion_dict = {}\n",
    "    \n",
    "    \n",
    "    # labels for confusion matrix\n",
    "    unique_y_test = y_test.unique().tolist()\n",
    "    unique_y_preds = list(set(y_preds))\n",
    "    labels = list(set(unique_y_test + unique_y_preds))\n",
    "    labels.sort()\n",
    "    columns = [f'Predicted {label}' for label in labels]\n",
    "    index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "    # create confusion matrix\n",
    "    confusion = pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "                             columns=columns, index=index)\n",
    "    print(model_name + ' confusion matrix:')\n",
    "    print(confusion, '\\n')\n",
    "\n",
    "    # append to confusion list\n",
    "    confusion_dict[model_name] = confusion\n",
    "\n",
    "    # creating dataframe from test set to calculate profitability\n",
    "    test_with_preds = X_test.copy()\n",
    "\n",
    "    # add column with higher closing price\n",
    "    test_with_preds['higher_closing_price'] = test_with_preds.apply(\n",
    "            get_higher_closing_price, axis=1)\n",
    "\n",
    "    # add column with shifted closing price\n",
    "    test_with_preds = get_close_shift(test_with_preds)\n",
    "\n",
    "    # adding column with predictions\n",
    "    test_with_preds['pred'] = y_preds\n",
    "\n",
    "    # adding column with profitability of predictions\n",
    "    test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "            get_profit, axis=1).shift(-2)\n",
    "\n",
    "    # filtering out rows where no arbitrage is predicted\n",
    "    test_with_preds = test_with_preds[test_with_preds['pred'] != 0]\n",
    "\n",
    "    # calculating mean profit where arbitrage predicted...\n",
    "    pct_profit_mean = test_with_preds['pct_profit'].mean()\n",
    "\n",
    "    # calculating median profit where arbitrage predicted...\n",
    "    pct_profit_median = test_with_preds['pct_profit'].median()\n",
    "    print('percent profit mean:', pct_profit_mean)\n",
    "    print('percent profit median:', pct_profit_median, '\\n\\n')\n",
    "\n",
    "    # save net performance to list\n",
    "    performance_list.append([name, max_features, max_depth, n_estimators,\n",
    "                             pct_profit_mean, pct_profit_median])\n",
    "    ######################## END OF TODO ###########################\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic-env",
   "language": "python",
   "name": "cryptolytic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
