{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_feat(y_test, y_preds):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # labels for confusion matrix\n",
    "#     unique_y_test = y_test.unique().tolist()\n",
    "#     unique_y_preds = list(set(y_preds))\n",
    "#     labels = list(set(unique_y_test + unique_y_preds))\n",
    "#     labels.sort()\n",
    "#     columns = [f'Predicted {label}' for label in labels]\n",
    "#     index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "#     # create confusion matrix\n",
    "#     conf_mat = pd.DataFrame(\n",
    "#         confusion_matrix(y_test, y_preds),\n",
    "#         columns=columns, index=index\n",
    "#     )\n",
    "#     class_report = classification_report(y_test, y_preds, digits=4)\n",
    "#     print(conf_mat, '\\n')\n",
    "#     print(classification_report(y_test, y_preds, digits=4))\n",
    "#     print(classification_report(y_test, y_preds, digits=4, output_dict=True))\n",
    "    \n",
    "#     # confusion matrix has -1, 0, 1 predictions\n",
    "#     if 'Predicted 1' in conf_mat.columns and 'Predicted -1' in conf_mat.columns:\n",
    "#         # % incorrect predictions for 0\n",
    "#         FPR = (conf_mat['Predicted 0'][0] + \n",
    "#             conf_mat['Predicted 0'][2])/conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = conf_mat['Predicted -1'][0]\n",
    "#         correct_arb_1 = conf_mat['Predicted 1'][2]\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = class_report['-1']['precision']\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = class_report['1']['precision']\n",
    "#         recall_neg1 = class_report['-1']['recall']\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = class_report['1']['recall']\n",
    "#         f1_neg1 = class_report['-1']['f1-score']\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = class_report['1']['f1-score']\n",
    "        \n",
    "#     # confusion matrix has 0, 1 predictions\n",
    "#     elif 'Predicted 1' in conf_mat.columns:\n",
    "#         # % incorrect predictions for 0\n",
    "#         FPR = conf_mat['Predicted 0'][1] / conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = 0\n",
    "#         correct_arb_1 = conf_mat['Predicted 1'][1]\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = np.nan\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = class_report['1']['precision']\n",
    "#         recall_neg1 = np.nan\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = class_report['1']['recall']\n",
    "#         f1_neg1 = np.nan\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = class_report['1']['f1-score']\n",
    "        \n",
    "#     # confusion matrix has -1, 0 predictions\n",
    "#     elif 'Predicted -1' in conf_mat.columns:\n",
    "#         FPR = conf_mat['Predicted 0'][0] / conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = conf_mat['Predicted -1'][0]\n",
    "#         correct_arb_1 = 0\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = class_report['-1']['precision']\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = np.nan\n",
    "#         recall_neg1 = class_report['-1']['recall']\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = np.nan\n",
    "#         f1_neg1 = class_report['-1']['f1-score']\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = np.nan\n",
    "        \n",
    "#     # confusion matrix has only 0 predictions\n",
    "#     else:\n",
    "#         FPR = np.nan\n",
    "        \n",
    "#         correct_arb_neg1 = 0\n",
    "#         correct_arb_1 = 0\n",
    "#         correct_arb = 0\n",
    "        \n",
    "#         precision_neg1 = np.nan\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = np.nan\n",
    "#         recall_neg1 = np.nan\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = np.nan\n",
    "#         f1_neg1 = np.nan\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = np.nan\n",
    "        \n",
    "    \n",
    "#     return FPR, correct_arb_neg1, correct_arb_1, correct_arb, precision_neg1, correct_arb_1, correct_arb_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "# 'weighted':\n",
    "#Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n",
    "#This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "# def tbl_stats_row(test_accuracy, precision, recall, f1):\n",
    "#     \"\"\"Prints the row of model stats after each param set fold\"\"\"\n",
    "#     print(\n",
    "#         sp*4, f'{test_accuracy:.4f}',     # accuracy\n",
    "# #         sp*3, f'{tpr:.4f}',           # roc auc\n",
    "# #         sp*3, f'{fpr:.4f}',      # p/r auc\n",
    "#         sp*2, f'{precision:.4f}',      # p/r auc\n",
    "#         sp*1, f'{recall:.4f}',      # p/r auc\n",
    "#         sp*1, f'{f1:.4f}',     # p/r auc\n",
    "#         sp*2, line*9\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_rep = {'-1': {'precision': 1.0, 'recall': 0.6111111111111112, \n",
    "#     'f1-score': 0.7586206896551725, 'support': 396}, \n",
    "#     '0': {'precision': 0.997034345980612, 'recall': 0.9994054586329264,\n",
    "#     'f1-score': 0.9982184942564996, 'support': 53823}, \n",
    "#     '1': {'precision': 0.9952181709503886, 'recall': 0.9990999099909991, \n",
    "#     'f1-score': 0.9971552627638868, 'support': 6666},\n",
    "#     'accuracy': 0.9968465139196846, 'macro avg': {'precision': 0.9974175056436668,\n",
    "#     'recall': 0.8698721599116789, 'f1-score': 0.9179981488918529, 'support': 60885}, \n",
    "#     'weighted avg': {'precision': 0.9968547906917923, 'recall': 0.9968465139196846, \n",
    "#     'f1-score': 0.9965437265509582, 'support': 60885}}\n",
    "\n",
    "\n",
    "\n",
    "# class_rep['-1']['precision']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
