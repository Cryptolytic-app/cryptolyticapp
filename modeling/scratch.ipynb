{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_feat(y_test, y_preds):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # labels for confusion matrix\n",
    "#     unique_y_test = y_test.unique().tolist()\n",
    "#     unique_y_preds = list(set(y_preds))\n",
    "#     labels = list(set(unique_y_test + unique_y_preds))\n",
    "#     labels.sort()\n",
    "#     columns = [f'Predicted {label}' for label in labels]\n",
    "#     index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "#     # create confusion matrix\n",
    "#     conf_mat = pd.DataFrame(\n",
    "#         confusion_matrix(y_test, y_preds),\n",
    "#         columns=columns, index=index\n",
    "#     )\n",
    "#     class_report = classification_report(y_test, y_preds, digits=4)\n",
    "#     print(conf_mat, '\\n')\n",
    "#     print(classification_report(y_test, y_preds, digits=4))\n",
    "#     print(classification_report(y_test, y_preds, digits=4, output_dict=True))\n",
    "    \n",
    "#     # confusion matrix has -1, 0, 1 predictions\n",
    "#     if 'Predicted 1' in conf_mat.columns and 'Predicted -1' in conf_mat.columns:\n",
    "#         # % incorrect predictions for 0\n",
    "#         FPR = (conf_mat['Predicted 0'][0] + \n",
    "#             conf_mat['Predicted 0'][2])/conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = conf_mat['Predicted -1'][0]\n",
    "#         correct_arb_1 = conf_mat['Predicted 1'][2]\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = class_report['-1']['precision']\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = class_report['1']['precision']\n",
    "#         recall_neg1 = class_report['-1']['recall']\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = class_report['1']['recall']\n",
    "#         f1_neg1 = class_report['-1']['f1-score']\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = class_report['1']['f1-score']\n",
    "        \n",
    "#     # confusion matrix has 0, 1 predictions\n",
    "#     elif 'Predicted 1' in conf_mat.columns:\n",
    "#         # % incorrect predictions for 0\n",
    "#         FPR = conf_mat['Predicted 0'][1] / conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = 0\n",
    "#         correct_arb_1 = conf_mat['Predicted 1'][1]\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = np.nan\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = class_report['1']['precision']\n",
    "#         recall_neg1 = np.nan\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = class_report['1']['recall']\n",
    "#         f1_neg1 = np.nan\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = class_report['1']['f1-score']\n",
    "        \n",
    "#     # confusion matrix has -1, 0 predictions\n",
    "#     elif 'Predicted -1' in conf_mat.columns:\n",
    "#         FPR = conf_mat['Predicted 0'][0] / conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = conf_mat['Predicted -1'][0]\n",
    "#         correct_arb_1 = 0\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = class_report['-1']['precision']\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = np.nan\n",
    "#         recall_neg1 = class_report['-1']['recall']\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = np.nan\n",
    "#         f1_neg1 = class_report['-1']['f1-score']\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = np.nan\n",
    "        \n",
    "#     # confusion matrix has only 0 predictions\n",
    "#     else:\n",
    "#         FPR = np.nan\n",
    "        \n",
    "#         correct_arb_neg1 = 0\n",
    "#         correct_arb_1 = 0\n",
    "#         correct_arb = 0\n",
    "        \n",
    "#         precision_neg1 = np.nan\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = np.nan\n",
    "#         recall_neg1 = np.nan\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = np.nan\n",
    "#         f1_neg1 = np.nan\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = np.nan\n",
    "        \n",
    "    \n",
    "#     return FPR, correct_arb_neg1, correct_arb_1, correct_arb, precision_neg1, correct_arb_1, correct_arb_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "# 'weighted':\n",
    "#Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n",
    "#This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "# def tbl_stats_row(test_accuracy, precision, recall, f1):\n",
    "#     \"\"\"Prints the row of model stats after each param set fold\"\"\"\n",
    "#     print(\n",
    "#         sp*4, f'{test_accuracy:.4f}',     # accuracy\n",
    "# #         sp*3, f'{tpr:.4f}',           # roc auc\n",
    "# #         sp*3, f'{fpr:.4f}',      # p/r auc\n",
    "#         sp*2, f'{precision:.4f}',      # p/r auc\n",
    "#         sp*1, f'{recall:.4f}',      # p/r auc\n",
    "#         sp*1, f'{f1:.4f}',     # p/r auc\n",
    "#         sp*2, line*9\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_rep = {'-1': {'precision': 1.0, 'recall': 0.6111111111111112, \n",
    "#     'f1-score': 0.7586206896551725, 'support': 396}, \n",
    "#     '0': {'precision': 0.997034345980612, 'recall': 0.9994054586329264,\n",
    "#     'f1-score': 0.9982184942564996, 'support': 53823}, \n",
    "#     '1': {'precision': 0.9952181709503886, 'recall': 0.9990999099909991, \n",
    "#     'f1-score': 0.9971552627638868, 'support': 6666},\n",
    "#     'accuracy': 0.9968465139196846, 'macro avg': {'precision': 0.9974175056436668,\n",
    "#     'recall': 0.8698721599116789, 'f1-score': 0.9179981488918529, 'support': 60885}, \n",
    "#     'weighted avg': {'precision': 0.9968547906917923, 'recall': 0.9968465139196846, \n",
    "#     'f1-score': 0.9965437265509582, 'support': 60885}}\n",
    "\n",
    "\n",
    "\n",
    "# class_rep['-1']['precision']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def performance_metrics(pkls, features):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # instantiate performance df\n",
    "#     columns = ['filename', 'model_id', 'parameters',\n",
    "#                 'accuracy_score', 'mean_pct_profit',\n",
    "#                 'precision', 'recall', 'f1_score',\n",
    "#                 'support', 'correct_arb_preds']\n",
    "#     perf_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "#     for pkl in pkls:\n",
    "        \n",
    "#         # naming \n",
    "#         file = '_'.join(pkl.split('/')[1].split('_')[:4])\n",
    "#         filepath = f'data/arb_data/{file}.csv'\n",
    "#         model_id = pkl.split('/')[1].split('.')[0]\n",
    "#         print('model_id:', model_id)\n",
    "        \n",
    "#         # get features and parameters\n",
    "#         df, features, params = feat_n_params(pkl, filepath)\n",
    "        \n",
    "#         # train/test split and predict\n",
    "#         X_train, X_test, y_train, y_test = tts(df, features)\n",
    "#         y_preds = predictions(pkl, X_test, y_test)\n",
    "        \n",
    "#         # calculate stats\n",
    "#         pct_prof_mean, pct_prof_median = profit(X_test, y_preds)\n",
    "#         correct_arb_preds = confusion_feat(y_test, y_preds)\n",
    "#         cl_report = classification_report(y_test, y_preds, output_dict=True)\n",
    "#         print(classification_report(y_test, y_preds, output_dict=True))\n",
    "#         print(classification_report(y_test, y_preds))\n",
    "\n",
    "#         # append to perf_df\n",
    "#         perf_dict = {\n",
    "#             'filename': file,\n",
    "#             'model_id': model_id,\n",
    "#             'parameters': params,\n",
    "#             'accuracy_score': cl_report['accuracy'],\n",
    "#             'mean_pct_profit': pct_prof_mean,\n",
    "#             'precision': 0,\n",
    "#             'recall': 0,\n",
    "#             'f1_score': 0,\n",
    "#             'support': 0,\n",
    "#             'correct_arb_preds': correct_arb_preds\n",
    "#         }\n",
    "#         perf_df = perf_df.append(perf_dict, ignore_index=True)\n",
    "        \n",
    "#     return perf_df, y_preds, y_test\n",
    "\n",
    "# arb_data_paths = glob.glob('data/arb_data/*.csv')\n",
    "# pkls = glob.glob('models/*.pkl')\n",
    "# perf_df, y_preds, y_test = performance_metrics(pkls, features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note 1 \n",
    "# the modeling function should have parameters called\n",
    "# export_preds and export_model that can be set to true or \n",
    "# false (default false) so that we can use that later in the \n",
    "# evaluation notebook to actually export the preds and models\n",
    "\n",
    "# note 2\n",
    "# the modeling function should have a parameter called filename \n",
    "# that takes a filename for performance csv otherwise it'll overwrite\n",
    "# the original when we retrain after model evaluation\n",
    "\n",
    "# with open ('top_features.txt', 'rb') as fp:\n",
    "#     top_features = pickle.load(fp)\n",
    "\n",
    "\n",
    "# Get the best performing models\n",
    "    # import performance df\n",
    "    # filter for:\n",
    "        # - minimum number of arb\n",
    "        # - minimum precision\n",
    "        # - minimum recall\n",
    "        # - minimum profit\n",
    "    # sort by profit\n",
    "    # save as top_models_df\n",
    "    \n",
    "# 2\n",
    "# function to retrain best models and export preds csv \n",
    "#     - uses filename, model_label, and params from filtered perf df\n",
    "#     - for each row in df:\n",
    "#         - pass that info into the original function to retrain\n",
    "#             this part will happen in the modeling function by \n",
    "#                 setting export_preds and export_model to true:\n",
    "#              - merge X_test, y_test, and y_preds into a df\n",
    "#              - export preds csv into a new folder data/arb_preds_test_data/\n",
    "#                  - this needs to have some kind of naming convention \n",
    "#                      w/ model type bc we need to train models for all 3 sets\n",
    "#              - export model into models/\n",
    "\n",
    "# 3\n",
    "# function to duplicate arb csv's of best models into a new folder\n",
    "#      - for row in df, move csv to arb_top_data/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
