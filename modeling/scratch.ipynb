{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confusion_feat(y_test, y_preds):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     # labels for confusion matrix\n",
    "#     unique_y_test = y_test.unique().tolist()\n",
    "#     unique_y_preds = list(set(y_preds))\n",
    "#     labels = list(set(unique_y_test + unique_y_preds))\n",
    "#     labels.sort()\n",
    "#     columns = [f'Predicted {label}' for label in labels]\n",
    "#     index = [f'Actual {label}' for label in labels]\n",
    "\n",
    "#     # create confusion matrix\n",
    "#     conf_mat = pd.DataFrame(\n",
    "#         confusion_matrix(y_test, y_preds),\n",
    "#         columns=columns, index=index\n",
    "#     )\n",
    "#     class_report = classification_report(y_test, y_preds, digits=4)\n",
    "#     print(conf_mat, '\\n')\n",
    "#     print(classification_report(y_test, y_preds, digits=4))\n",
    "#     print(classification_report(y_test, y_preds, digits=4, output_dict=True))\n",
    "    \n",
    "#     # confusion matrix has -1, 0, 1 predictions\n",
    "#     if 'Predicted 1' in conf_mat.columns and 'Predicted -1' in conf_mat.columns:\n",
    "#         # % incorrect predictions for 0\n",
    "#         FPR = (conf_mat['Predicted 0'][0] + \n",
    "#             conf_mat['Predicted 0'][2])/conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = conf_mat['Predicted -1'][0]\n",
    "#         correct_arb_1 = conf_mat['Predicted 1'][2]\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = class_report['-1']['precision']\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = class_report['1']['precision']\n",
    "#         recall_neg1 = class_report['-1']['recall']\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = class_report['1']['recall']\n",
    "#         f1_neg1 = class_report['-1']['f1-score']\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = class_report['1']['f1-score']\n",
    "        \n",
    "#     # confusion matrix has 0, 1 predictions\n",
    "#     elif 'Predicted 1' in conf_mat.columns:\n",
    "#         # % incorrect predictions for 0\n",
    "#         FPR = conf_mat['Predicted 0'][1] / conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = 0\n",
    "#         correct_arb_1 = conf_mat['Predicted 1'][1]\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = np.nan\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = class_report['1']['precision']\n",
    "#         recall_neg1 = np.nan\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = class_report['1']['recall']\n",
    "#         f1_neg1 = np.nan\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = class_report['1']['f1-score']\n",
    "        \n",
    "#     # confusion matrix has -1, 0 predictions\n",
    "#     elif 'Predicted -1' in conf_mat.columns:\n",
    "#         FPR = conf_mat['Predicted 0'][0] / conf_mat['Predicted 0'].sum()\n",
    "\n",
    "#         correct_arb_neg1 = conf_mat['Predicted -1'][0]\n",
    "#         correct_arb_1 = 0\n",
    "#         correct_arb = correct_arb_neg1 + correct_arb_1\n",
    "        \n",
    "#         precision_neg1 = class_report['-1']['precision']\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = np.nan\n",
    "#         recall_neg1 = class_report['-1']['recall']\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = np.nan\n",
    "#         f1_neg1 = class_report['-1']['f1-score']\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = np.nan\n",
    "        \n",
    "#     # confusion matrix has only 0 predictions\n",
    "#     else:\n",
    "#         FPR = np.nan\n",
    "        \n",
    "#         correct_arb_neg1 = 0\n",
    "#         correct_arb_1 = 0\n",
    "#         correct_arb = 0\n",
    "        \n",
    "#         precision_neg1 = np.nan\n",
    "#         precision_0 = class_report['0']['precision']\n",
    "#         precision_1 = np.nan\n",
    "#         recall_neg1 = np.nan\n",
    "#         recall_0 = class_report['0']['recall']\n",
    "#         recall_1 = np.nan\n",
    "#         f1_neg1 = np.nan\n",
    "#         f1_0 = class_report['0']['f1-score']\n",
    "#         f1_1 = np.nan\n",
    "        \n",
    "    \n",
    "#     return FPR, correct_arb_neg1, correct_arb_1, correct_arb, precision_neg1, correct_arb_1, correct_arb_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "# 'weighted':\n",
    "#Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). \n",
    "#This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "# def tbl_stats_row(test_accuracy, precision, recall, f1):\n",
    "#     \"\"\"Prints the row of model stats after each param set fold\"\"\"\n",
    "#     print(\n",
    "#         sp*4, f'{test_accuracy:.4f}',     # accuracy\n",
    "# #         sp*3, f'{tpr:.4f}',           # roc auc\n",
    "# #         sp*3, f'{fpr:.4f}',      # p/r auc\n",
    "#         sp*2, f'{precision:.4f}',      # p/r auc\n",
    "#         sp*1, f'{recall:.4f}',      # p/r auc\n",
    "#         sp*1, f'{f1:.4f}',     # p/r auc\n",
    "#         sp*2, line*9\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_rep = {'-1': {'precision': 1.0, 'recall': 0.6111111111111112, \n",
    "#     'f1-score': 0.7586206896551725, 'support': 396}, \n",
    "#     '0': {'precision': 0.997034345980612, 'recall': 0.9994054586329264,\n",
    "#     'f1-score': 0.9982184942564996, 'support': 53823}, \n",
    "#     '1': {'precision': 0.9952181709503886, 'recall': 0.9990999099909991, \n",
    "#     'f1-score': 0.9971552627638868, 'support': 6666},\n",
    "#     'accuracy': 0.9968465139196846, 'macro avg': {'precision': 0.9974175056436668,\n",
    "#     'recall': 0.8698721599116789, 'f1-score': 0.9179981488918529, 'support': 60885}, \n",
    "#     'weighted avg': {'precision': 0.9968547906917923, 'recall': 0.9968465139196846, \n",
    "#     'f1-score': 0.9965437265509582, 'support': 60885}}\n",
    "\n",
    "\n",
    "\n",
    "# class_rep['-1']['precision']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def performance_metrics(pkls, features):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # instantiate performance df\n",
    "#     columns = ['filename', 'model_id', 'parameters',\n",
    "#                 'accuracy_score', 'mean_pct_profit',\n",
    "#                 'precision', 'recall', 'f1_score',\n",
    "#                 'support', 'correct_arb_preds']\n",
    "#     perf_df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "#     for pkl in pkls:\n",
    "        \n",
    "#         # naming \n",
    "#         file = '_'.join(pkl.split('/')[1].split('_')[:4])\n",
    "#         filepath = f'data/arb_data/{file}.csv'\n",
    "#         model_id = pkl.split('/')[1].split('.')[0]\n",
    "#         print('model_id:', model_id)\n",
    "        \n",
    "#         # get features and parameters\n",
    "#         df, features, params = feat_n_params(pkl, filepath)\n",
    "        \n",
    "#         # train/test split and predict\n",
    "#         X_train, X_test, y_train, y_test = tts(df, features)\n",
    "#         y_preds = predictions(pkl, X_test, y_test)\n",
    "        \n",
    "#         # calculate stats\n",
    "#         pct_prof_mean, pct_prof_median = profit(X_test, y_preds)\n",
    "#         correct_arb_preds = confusion_feat(y_test, y_preds)\n",
    "#         cl_report = classification_report(y_test, y_preds, output_dict=True)\n",
    "#         print(classification_report(y_test, y_preds, output_dict=True))\n",
    "#         print(classification_report(y_test, y_preds))\n",
    "\n",
    "#         # append to perf_df\n",
    "#         perf_dict = {\n",
    "#             'filename': file,\n",
    "#             'model_id': model_id,\n",
    "#             'parameters': params,\n",
    "#             'accuracy_score': cl_report['accuracy'],\n",
    "#             'mean_pct_profit': pct_prof_mean,\n",
    "#             'precision': 0,\n",
    "#             'recall': 0,\n",
    "#             'f1_score': 0,\n",
    "#             'support': 0,\n",
    "#             'correct_arb_preds': correct_arb_preds\n",
    "#         }\n",
    "#         perf_df = perf_df.append(perf_dict, ignore_index=True)\n",
    "        \n",
    "#     return perf_df, y_preds, y_test\n",
    "\n",
    "# arb_data_paths = glob.glob('data/arb_data/*.csv')\n",
    "# pkls = glob.glob('models/*.pkl')\n",
    "# perf_df, y_preds, y_test = performance_metrics(pkls, features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note 1 \n",
    "# the modeling function should have parameters called\n",
    "# export_preds and export_model that can be set to true or \n",
    "# false (default false) so that we can use that later in the \n",
    "# evaluation notebook to actually export the preds and models\n",
    "\n",
    "# note 2\n",
    "# the modeling function should have a parameter called filename \n",
    "# that takes a filename for performance csv otherwise it'll overwrite\n",
    "# the original when we retrain after model evaluation\n",
    "\n",
    "# with open ('top_features.txt', 'rb') as fp:\n",
    "#     top_features = pickle.load(fp)\n",
    "\n",
    "\n",
    "# Get the best performing models\n",
    "    # import performance df\n",
    "    # filter for:\n",
    "        # - minimum number of arb\n",
    "        # - minimum precision\n",
    "        # - minimum recall\n",
    "        # - minimum profit\n",
    "    # sort by profit\n",
    "    # save as top_models_df\n",
    "    \n",
    "# 2\n",
    "# function to retrain best models and export preds csv \n",
    "#     - uses filename, model_label, and params from filtered perf df\n",
    "#     - for each row in df:\n",
    "#         - pass that info into the original function to retrain\n",
    "#             this part will happen in the modeling function by \n",
    "#                 setting export_preds and export_model to true:\n",
    "#              - merge X_test, y_test, and y_preds into a df\n",
    "#              - export preds csv into a new folder data/arb_preds_test_data/\n",
    "#                  - this needs to have some kind of naming convention \n",
    "#                      w/ model type bc we need to train models for all 3 sets\n",
    "#              - export model into models/\n",
    "\n",
    "# 3\n",
    "# function to duplicate arb csv's of best models into a new folder\n",
    "#      - for row in df, move csv to arb_top_data/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importances(data_path, pkls):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # get column names\n",
    "#     df = pd.read_csv(data_path, index_col=0)\n",
    "#     columns = df.drop(labels=['target', 'closing_time'], axis=1).columns.to_list()\n",
    "\n",
    "#     mfi = []\n",
    "#     titles = {}\n",
    "#     n = 0\n",
    "    \n",
    "#     # generate titles and feature importances\n",
    "#     for pkl in pkls:\n",
    "#         with open(pkl, 'rb') as f:\n",
    "#             model = pickle.load(f)\n",
    "#             titles[n] = pkl.split('/')[1].split('.')[0]\n",
    "#             n += 1\n",
    "\n",
    "#         importances = pd.DataFrame(model.feature_importances_, columns).reset_index()\n",
    "#         importances = importances.rename(columns={'index': 'features', 0: 'importance'})\n",
    "#         importances = importances.sort_values(by='importance', ascending=False)[:10][::-1]\n",
    "#         mfi.append(importances)\n",
    "    \n",
    "#     # figure and styles\n",
    "#     fig, ax = plt.subplots(nrows=34, ncols=2, figsize=(15, 200))\n",
    "#     plt.subplots_adjust(wspace=1)\n",
    "#     plt.style.use('dark_background')\n",
    "    \n",
    "#     n = 0\n",
    "#     for row in ax:\n",
    "#         for col in row:\n",
    "#             try:\n",
    "#                 col.barh(mfi[n]['features'], mfi[n]['importance'], color='#4EB9FF')\n",
    "#                 col.set_yticklabels = mfi[n]['features']\n",
    "#                 col.set_title(titles[n])\n",
    "#                 col.set_xlabel('Feature Importance')\n",
    "#                 n += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge performance with the distribution csv\n",
    "# dist_df = pd.read_csv('data/class_distribution.csv')\n",
    "# dist_df = dist_df.rename(columns={\n",
    "#     'arbitrage_combination': 'csv_name',\n",
    "#     'ex1_to_ex2_arb': 'pct_class_1',\n",
    "#     'ex2_to_ex1_arb': 'pct_class_neg1',\n",
    "#     'no_arb': 'pct_class_0',\n",
    "    \n",
    "# })\n",
    "# perf_df = pd.merge(perf_df, right=dist_df)\n",
    "# perf_df.head()\n",
    "\n",
    "# # export\n",
    "# perf_df.to_csv('data/model_perf_dist.csv', index=False)\n",
    "\n",
    "# perf_dist_df = pd.read_csv('data/model_perf_dist.csv')\n",
    "# print(perf_dist_df.shape) # (1290, 23)\n",
    "# perf_dist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             df['bl_accuracy_diff'] = (\n",
    "#                 (label_dict['lr']['accuracy'] - label_dict['rf']['accuracy'])\n",
    "#                 / label_dict['lr']['accuracy']\n",
    "#                 if rf_bl else np.nan\n",
    "#             )\n",
    "#             df['bl_mean_pct_profit_diff'] = (\n",
    "#                 (label_dict['lr']['mean_pct_profit'] - label_dict['rf']['mean_pct_profit'])\n",
    "#                 / label_dict['lr']['mean_pct_profit']\n",
    "#                 if rf_bl else np.nan\n",
    "#             )\n",
    "#             df['bl_precision_neg1_diff'] = (\n",
    "#                 (label_dict['lr']['precision_neg1'] - label_dict['rf']['precision_neg1'])\n",
    "#                 / label_dict['lr']['accuracy']\n",
    "#                 if rf_bl else np.nan\n",
    "#             )\n",
    "#             df['bl_precision_0_diff'] = \n",
    "#             df['bl_precision_1_diff'] = \n",
    "#             df['bl_recall_neg1_diff'] = \n",
    "#             df['bl_recall_0_diff'] = \n",
    "#             df['bl_recall_1_diff'] = \n",
    "                \n",
    "#                 df['ta_feat_accuracy_diff'] = \n",
    "#                 df['ta_feat_mean_pct_profit_diff'] =                 \n",
    "#                 df['ta_feat_precision_neg1_diff'] = \n",
    "#                 df['ta_feat_precision_0_diff'] = \n",
    "#                 df['ta_feat_precision_1_diff'] = \n",
    "#                 df['ta_feat_recall_neg1_diff'] = \n",
    "#                 df['ta_feat_recall_0_diff'] = \n",
    "#                 df['ta_feat_recall_1_diff'] = \n",
    "\n",
    "#                 df['hyper_accuracy_diff'] = \n",
    "#                 df['hyper_mean_pct_profit_diff'] =                 \n",
    "#                 df['hyper_precision_neg1_diff'] = \n",
    "#                 df['hyper_precision_0_diff'] = \n",
    "#                 df['hyper_precision_1_diff'] = \n",
    "#                 df['hyper_recall_neg1_diff'] = \n",
    "#                 df['hyper_recall_0_diff'] = \n",
    "#                 df['hyper_recall_1_diff'] = \n",
    "\n",
    "                \n",
    "#             # if its rf bl, compare with lr\n",
    "#             # change this to rf_bl later\n",
    "#             elif set_df['model_label'].loc[i] == 'rf':\n",
    "\n",
    "#             # if its regular rf, compare with rf bl\n",
    "#             elif set_df['model_label'].loc[i] == 'rf':\n",
    "\n",
    "#             # if its hyper, compare with regular rf\n",
    "#             else:\n",
    "#                 stuff\n",
    "            \n",
    "\n",
    "\n",
    "# perf_df['bl_precision_neg1_diff'] = perf_df.apply(get_lr_rf_bl_change)\n",
    "# perf_df['ta_features_change'] = perf_df.apply(get_ta_features_change)\n",
    "# perf_df['hyper_change'] = perf_df.apply(get_hyper_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # plotting buy and sell marks \n",
    "#     ax3 = sns.regplot(\n",
    "#         x='closing_time', \n",
    "#         y='buy', \n",
    "#         data=preds_df, \n",
    "#         marker=\"|\", \n",
    "#         fit_reg=False, \n",
    "#         scatter_kws={\n",
    "#             \"zorder\":10, \n",
    "#             \"color\":\"green\",\n",
    "#             \"alpha\":1,\n",
    "#             \"s\":200}, \n",
    "#         label='Buy'\n",
    "#     )\n",
    "    \n",
    "#     ax3 = sns.regplot(\n",
    "#         x='closing_time', \n",
    "#         y='sell', \n",
    "#         data=preds_df, \n",
    "#         marker=\"|\", \n",
    "#         fit_reg=False, \n",
    "#         scatter_kws={\n",
    "#             \"zorder\":10, \n",
    "#             \"color\":\"red\",\n",
    "#             \"alpha\":1,\"s\":200}, \n",
    "#         label='Sell'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fe_performance(adf):\n",
    "#     '''\n",
    "#     Feature engineers on preds csv for visualization\n",
    "#     '''\n",
    "    \n",
    "#     # shifted the arb indicater 2 down, to create new column to make a trade\n",
    "#     adf['trade'] = adf['y_preds'].apply(lambda x: 1 if x==1 else (-1 if x==-1 else 0)).shift(2)\n",
    "\n",
    "#     # function to calculate the price percent diff among the exchange only when there is a arb trade \n",
    "#     def perc_diff(adf):\n",
    "#         if adf['trade'] == -1:\n",
    "#             return (adf['close_exchange_1'] - adf['close_exchange_2'])/adf['close_exchange_2']\n",
    "\n",
    "\n",
    "#         elif adf['trade'] == 1:\n",
    "#             return (adf['close_exchange_2'] - adf['close_exchange_1'])/adf['close_exchange_1']\n",
    "\n",
    "#         else:\n",
    "#             return 0\n",
    "        \n",
    "\n",
    "#     # create that column with the function above\n",
    "#     adf['perc_diff'] = adf.apply(perc_diff, axis=1)\n",
    "    \n",
    "#     # create a column to simulate a 10k start and making trades on the predictions\n",
    "#     adf['my_money'] = adf['perc_diff'].cumsum() * 10000\n",
    "#     adf['my_money'] = adf['my_money'] + 10000\n",
    "    \n",
    "#     # trade indicator\n",
    "#     adf['trade_ind'] = adf['my_money'] * abs(adf['trade'])\n",
    "    \n",
    "#     return adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     # we might not need this column \n",
    "#     df['trade_ind']  = df['my_money'] * abs(df['trade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                     print(set_df['model_label'].loc[i])\n",
    "#                     print(t, metric)\n",
    "#                     print(labels_dict[types[t][1]][metric])\n",
    "#                     print('minus', labels_dict[types[t][0]][metric])\n",
    "#                     print('divided by:', labels_dict[types[t][0]][metric])\n",
    "#                     print('if:', types[t][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_types(model_type, features):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # label each model type\n",
    "#     base_model_name = str(model_type).split('(')[0]\n",
    "#     model_name_dict = {\n",
    "#         'LogisticRegression': 'lr',\n",
    "#         'RandomForestClassifier': 'rf'\n",
    "#     }\n",
    "    \n",
    "#     model_label = model_name_dict[base_model_name]\n",
    "\n",
    "#     # model type for the # of features\n",
    "#     if len(features) < 20:\n",
    "#         model_label = model_label + '_bl'\n",
    "        \n",
    "#     elif len(features) == 25:\n",
    "#         model_label = model_label + '_25_feat'\n",
    "        \n",
    "#     elif len(features) == 50:\n",
    "#         model_label = model_label + '_50_feat'\n",
    "        \n",
    "#     elif len(features) == 75:\n",
    "#         model_label = model_label + '_75_feat'\n",
    "        \n",
    "#     elif len(features) == 100:\n",
    "#         model_label = model_label + '_100_feat'\n",
    "        \n",
    "#     return model_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete this later\n",
    "# its for finding the inf\n",
    "\n",
    "# gemini_hitbtc_ltc_btc_rf_bl\n",
    "# inf_value = perf_df[(perf_df['model_id'] == 'gemini_hitbtc_ltc_btc_rf_bl')]['FPR'].values[0]\n",
    "# perf_df[perf_df['FPR'] == inf_value]\n",
    "# perf_df[(perf_df['csv_name'] == 'gemini_cbpro_ltc_btc') & (perf_df['model_label'] == 'rf_bl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### have to concat new hyper and test performance csv YOU CAN DELETE THIS CELL\n",
    "\n",
    "# perf_hyper_df = pd.read_csv('data/model_perf_hyper.csv')\n",
    "# perf_df = pd.read_csv('data/model_perf_test.csv')\n",
    "\n",
    "# def concat_perf_csv(perf_csv_paths):\n",
    "#     perf_df = pd.DataFrame()\n",
    "#     for path in perf_csv_paths:\n",
    "#         df = pd.read_csv(path)\n",
    "#         perf_df = pd.concat([perf_df, df])\n",
    "    \n",
    "#     perf_df.to_csv('data/model_perf_final.csv', index=False)\n",
    "#     return perf_df\n",
    "\n",
    "# perf_csv_paths = ['data/model_perf_hyper.csv', 'data/model_perf_test.csv']\n",
    "# concat_perf_csv(perf_csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # delete this cell later\n",
    "# perf2 = pd.read_csv('data/model_perf_test.csv')\n",
    "# print(perf2.shape)\n",
    "# perf2.head()\n",
    "\n",
    "# perf_df = pd.read_csv('data/model_perf.csv')\n",
    "# perf_df = perf_df.rename(columns={'FPR': 'fpr'})\n",
    "# perf1 = perf_df[perf_df['model_label'] =='rf_hyper']\n",
    "# perf_new = perf2.append(perf1, sort=False)\n",
    "# print(perf_new.shape)\n",
    "\n",
    "# perf_new = pd.merge(perf_new, dist_df, on='csv_name')\n",
    "# print(perf_new.shape) # (1305, 23)\n",
    "# # perf_new.head()\n",
    "\n",
    "# # perf_new.to_csv('data/model_perf_new.csv', index=False)\n",
    "# perf_df = pd.read_csv('data/model_perf_new.csv')\n",
    "# print(perf_df.shape)\n",
    "# perf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         # duplicate csv data to new folder for easy export from sagemaker\n",
    "#         shutil.copyfile(filepath, f'data/arb_top_data/{filename}.csv')\n",
    "#         print('csv_copied!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
