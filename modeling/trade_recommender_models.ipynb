{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from ta import add_all_ta_features\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements\n",
    "- pandas==0.25.1\n",
    "- ta==0.4.7\n",
    "- scikit-learn==21.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background on Trade Recommender Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trade recommender models were created with with the goal of predicting whether the price of a cryptocurrency will go up or down in the next time period (the period is determined by the specific model). If the time period for the model was 6hrs, and if the model predicted that the price will go up, that would mean that if you bought that cryptocurrency 6 hours after the prediction time (this time comes from the data point that the model is predicting off of), the price of the crypto should have gone up after 6 hours from the time that you bought it. \n",
    "\n",
    "100s of iterations of models were generated in this notebook and the best ones were selected from each exchange/trading pair based on which iteration returned the highest net profit. When training the random forest classifier models, performance was highly varied with different periods and parameters so there was no one size fits all model, and that resulted in the models having unique periods and parameters. The data was obtained from the respective exchanges via their api, and models were trained on 1 hour candlestick data from 2015 - Oct 2018. The test set contained data from Jan 2019 - Oct 2019 with a two month gap left between the train and test sets to prevent data leakage. The models' predictions output 0 (sell) and 1 (buy) and profit was calculated by backtesting on the 2019 test set. The profit calculation incorporated fees like in the real world and considered any consecutive \"buy\" prediction as a \"hold\" trade instead so that fees wouldn't have to be paid on those transactions. The final models were all profitable with gains anywhere from 40% - 95% within the Jan 1, 2019 to Oct 30, 2019 time period. Visualizations for how these models performed given a $10K portfolio can be viewed at https://github.com/Lambda-School-Labs/cryptolytic-ds/blob/master/finalized_notebooks/visualization/tr_performance_visualization.ipynb\n",
    "\n",
    "The separate models created for each exchange/trading pair combination were:\n",
    "- Bitfinex BTC/USD\n",
    "- Bitfinex ETH/USD\n",
    "- Bitfinex LTC/USD\n",
    "- Coinbase Pro BTC/USD\n",
    "- Coinbase Pro ETH/USD\n",
    "- Coinbase Pro LTC/USD\n",
    "- HitBTC BTC/USD\n",
    "- HitBTC ETH/USD\n",
    "- HitBTC LTC/USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Folder Structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "├── trade_recommender/                <-- The top-level directory for all trade recommender work\n",
    "│   │\n",
    "│   ├── trade_rec_models.ipynb        <-- Notebook for trade recommender models\n",
    "│   │\n",
    "│   ├── data/                         <-- Directory for csv files of 1 hr candle data\n",
    "│   │     └── data.csv                \n",
    "│   │\n",
    "│   ├── pickles/                      <-- Directory for all trade rec models\n",
    "│   │     └── models.pkl         \n",
    "│   │              \n",
    "│   ├── tr_pickles/                   <-- Directory for best trade rec models\n",
    "          └── models.pkl              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all csv filenames into a variable - 1 hr candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/bitfinex_ltc_usd_3600.csv',\n",
       " 'data/bitfinex_btc_usd_3600.csv',\n",
       " 'data/coinbase_pro_eth_usd_3600.csv',\n",
       " 'data/coinbase_pro_ltc_usd_3600.csv',\n",
       " 'data/hitbtc_eth_usdt_3600.csv',\n",
       " 'data/bitfinex_eth_usd_3600.csv',\n",
       " 'data/coinbase_pro_btc_usd_3600.csv',\n",
       " 'data/hitbtc_ltc_usdt_3600.csv',\n",
       " 'data/hitbtc_btc_usdt_3600.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filenames = glob.glob('data/*.csv') # modify to your filepath for data\n",
    "print(len(csv_filenames))\n",
    "csv_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHLCV Data Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ohlcv(df, period):\n",
    "    \"\"\" Changes the time period on cryptocurrency ohlcv data.\n",
    "        Period is a string denoted by '{time_in_minutes}T'(ex: '1T', '5T', '60T').\"\"\"\n",
    "\n",
    "    # Set date as the index. This is needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "\n",
    "    # Aggregation function\n",
    "    ohlc_dict = {'open':'first',                                                                                                    \n",
    "                 'high':'max',                                                                                                       \n",
    "                 'low':'min',                                                                                                        \n",
    "                 'close': 'last',                                                                                                    \n",
    "                 'base_volume': 'sum'}\n",
    "\n",
    "    # Apply resampling\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample_ohlcv function will create NaNs in df where there were gaps in the data.\n",
    "# The gaps could be caused by exchanges being down, errors from cryptowatch or the \n",
    "# exchanges themselves\n",
    "\n",
    "def fill_nan(df):\n",
    "    \"\"\"Iterates through a dataframe and fills NaNs with appropriate \n",
    "        open, high, low, close values.\"\"\"\n",
    "\n",
    "    # Forward fill close column.\n",
    "    df['close'] = df['close'].ffill()\n",
    "\n",
    "    # Backward fill the open, high, low rows with the close value.\n",
    "    df = df.bfill(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, period):\n",
    "    \"\"\"Takes in a dataframe of 1 hour cryptocurrency trading data\n",
    "        and returns a new dataframe with selected period, new technical analysis features,\n",
    "        and a target.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add a datetime column to df\n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "     \n",
    "    # Convert df to selected period\n",
    "    df = resample_ohlcv(df, period)\n",
    "    \n",
    "    # Add feature to indicate gaps in the data\n",
    "    df['nan_ohlc'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    \n",
    "    # Fill in missing values using fill function\n",
    "    df = fill_nan(df)\n",
    "    \n",
    "    # Reset index \n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Create additional date features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "    # Add technical analysis features\n",
    "    df = add_all_ta_features(df, \"open\", \"high\", \"low\", \"close\", \"base_volume\")\n",
    "      \n",
    "    # Replace infinite values with NaNs\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Drop any features whose mean of missing values is greater than 20%\n",
    "    df = df[df.columns[df.isnull().mean() < .2]]\n",
    "    \n",
    "    # Replace remaining NaN values with the mean of each respective column and reset index\n",
    "    df = df.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    \n",
    "    # Create a feature for close price difference \n",
    "    df['close_diff'] = (df['close'] - df['close'].shift(1))/df['close'].shift(1)    \n",
    "    \n",
    "    # Function to create target\n",
    "    def price_increase(x):\n",
    "        if (x-(.70/100)) > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Create target\n",
    "    target = df['close_diff'].apply(price_increase)\n",
    "    \n",
    "    # To make the prediction before it happens, put target on the next observation\n",
    "    target = target[1:].values\n",
    "    df = df[:-1]\n",
    "    \n",
    "    # Create target column\n",
    "    df['target'] = target\n",
    "    \n",
    "    # Remove first row of dataframe bc it has a null target\n",
    "    df = df[1:]\n",
    "    \n",
    "    # Pick features\n",
    "    features = ['open', 'high', 'low', 'close', 'base_volume', 'nan_ohlc', \n",
    "                'year', 'month', 'day', 'volume_adi', 'volume_obv', 'volume_cmf', \n",
    "                'volume_fi', 'volume_em', 'volume_vpt', 'volume_nvi', 'volatility_atr', \n",
    "                'volatility_bbh', 'volatility_bbl', 'volatility_bbm', 'volatility_bbhi', \n",
    "                'volatility_bbli', 'volatility_kcc', 'volatility_kch', 'volatility_kcl', \n",
    "                'volatility_kchi', 'volatility_kcli', 'volatility_dch', 'volatility_dcl', \n",
    "                'volatility_dchi', 'volatility_dcli', 'trend_macd', 'trend_macd_signal', \n",
    "                'trend_macd_diff', 'trend_ema_fast', 'trend_ema_slow', \n",
    "                'trend_adx_pos', 'trend_adx_neg', 'trend_vortex_ind_pos', \n",
    "                'trend_vortex_ind_neg', 'trend_vortex_diff', 'trend_trix', \n",
    "                'trend_mass_index', 'trend_cci', 'trend_dpo', 'trend_kst', \n",
    "                'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_a', \n",
    "                'trend_ichimoku_b', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', \n",
    "                'trend_aroon_up', 'trend_aroon_down', 'trend_aroon_ind', 'momentum_rsi', \n",
    "                'momentum_mfi', 'momentum_tsi', 'momentum_uo', 'momentum_stoch', \n",
    "                'momentum_stoch_signal', 'momentum_wr', 'momentum_ao',  \n",
    "                'others_dr', 'others_dlr', 'others_cr', 'close_diff', 'date', 'target']\n",
    "\n",
    "    df = df[features]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profit and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(X_test, y_preds):\n",
    "    \"\"\" Takes in a test dataset and a model's predictions, calculates and returns\n",
    "        the profit or loss. When the model generates consecutive buy predictions, \n",
    "        anything after the first one are considered a hold and fees are not added\n",
    "        for the hold trades. \"\"\"\n",
    "    \n",
    "    fee_rate = 0.35 \n",
    "    \n",
    "    # creates dataframe for features and predictions\n",
    "    df_preds = X_test\n",
    "    df_preds['y_preds'] = y_preds\n",
    "    \n",
    "    # creates column with 0s for False predictions and 1s for True predictions\n",
    "    df_preds['binary_y_preds'] = df_preds['y_preds'].shift(1).apply(lambda x: 1 if x == True else 0)\n",
    "    \n",
    "    # performance results from adding the closing difference percentage of the rows where trades were executed\n",
    "    performance = ((10000 * df_preds['binary_y_preds']*df_preds['close_diff']).sum())\n",
    "    \n",
    "    # calculating fees and improve trading strategy\n",
    "    # creates a count list for when trades were triggered\n",
    "    df_preds['preds_count'] = df_preds['binary_y_preds'].cumsum()\n",
    "    \n",
    "    # feature that determines the instance of whether the list increased\n",
    "    df_preds['increase_count'] = df_preds['preds_count'].diff(1)\n",
    "    \n",
    "    # feature that creates signal of when to buy(1), hold(0), or sell(-1)\n",
    "    df_preds['trade_trig'] = df_preds['increase_count'].diff(1)\n",
    "    \n",
    "    # number of total entries(1s)\n",
    "    number_of_entries = (df_preds.trade_trig.values==1).sum()\n",
    "    \n",
    "    # performance takes into account fees given the rate at the beginning of this function\n",
    "    pct_performance = ((df_preds['binary_y_preds']*df_preds['close_diff']).sum())\n",
    "    \n",
    "    # calculate the percentage paid in fees\n",
    "    fees_pct = number_of_entries * 2 * fee_rate/100\n",
    "    \n",
    "    # calculate fees in USD \n",
    "    fees = number_of_entries * 2 * fee_rate / 100 * 10000\n",
    "    \n",
    "    # calculate net profit in USD\n",
    "    performance_net = performance - fees\n",
    "    \n",
    "    # calculate net profit percent\n",
    "    performance_net_pct = performance_net/10000\n",
    "\n",
    "    return pct_performance, performance, fees, performance_net, performance_net_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_pipeline(csv_filenames, periods=['360T','720T','960T','1440T']):\n",
    "    \"\"\"Takes csv file paths of data for modeling, performs feature engineering,\n",
    "        train/test split, creates a model, reports train/test score, and saves\n",
    "        a pickle file of the model in a directory called /pickles. The best models\n",
    "        are moved to a directory called tr_pickles at the end\"\"\"\n",
    "    \n",
    "    line = '------------'\n",
    "    performance_list = []\n",
    "    \n",
    "    for file in csv_filenames:\n",
    "        \n",
    "        # define model name \n",
    "        name = file.split('/')[1][:-9]\n",
    "        \n",
    "        # read csv\n",
    "        csv = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        for period in periods:\n",
    "            \n",
    "            max_depth_list = [17]\n",
    "#             max_depth_list = [17, 20, 25, 27]\n",
    "            for max_depth in max_depth_list:\n",
    "                \n",
    "                max_features_list = [40]\n",
    "#                 max_features_list = [40, 45, 50, 55, 60]\n",
    "                for max_features in max_features_list:\n",
    "                    \n",
    "                    print(line + name + ' ' + period + ' ' + str(max_depth) + ' ' + str(max_features) + line)\n",
    "                    \n",
    "                    # create a copy of the csv\n",
    "                    df = csv.copy()\n",
    "\n",
    "                    # engineer features\n",
    "                    df = feature_engineering(df, period)\n",
    "\n",
    "                    # train test split\n",
    "                    train = df[df['date'] < '2018-10-30 23:00:00'] # cutoff oct 30 2018\n",
    "                    test = df[df['date'] > '2019-01-01 23:00:00'] # cutoff jan 01 2019\n",
    "                    print('train and test shape ({model}):'.format(model=name), train.shape, test.shape)\n",
    "\n",
    "                    # features and target\n",
    "                    features = df.drop(columns=['target', 'date']).columns.tolist()\n",
    "                    target = 'target'\n",
    "\n",
    "                    # define X, y vectors\n",
    "                    X_train = train[features]\n",
    "                    X_test = test[features]\n",
    "                    y_train = train[target]\n",
    "                    y_test = test[target]\n",
    "\n",
    "                    # instantiate model\n",
    "                    model = RandomForestClassifier(max_features=max_features, \n",
    "                                                   max_depth=max_depth, \n",
    "                                                   n_estimators=100, \n",
    "                                                   n_jobs=-1, \n",
    "                                                   random_state=42)\n",
    "                    try:\n",
    "                        # filter out datasets that are too small\n",
    "                        if X_test.shape[0] > 500:\n",
    "                            # fit model\n",
    "                            model.fit(X_train, y_train)\n",
    "                            print('model fitted')\n",
    "\n",
    "                            # train accuracy\n",
    "                            train_score = model.score(X_train, y_train)\n",
    "                            print('train accuracy:', train_score)\n",
    "\n",
    "                            # make predictions\n",
    "                            y_preds = model.predict(X_test)\n",
    "                            print('predictions made')\n",
    "\n",
    "                            # test accuracy\n",
    "                            score = accuracy_score(y_test, y_preds)\n",
    "                            print('test accuracy:', score)\n",
    "\n",
    "                            # get profit and loss\n",
    "                            a, b, c, d, e = performance(X_test, y_preds)\n",
    "                            print(f'net profits: {str(round(d,2))}')\n",
    "\n",
    "                            # formatting for filename\n",
    "                            t = period[:-1]\n",
    "\n",
    "                            # download pickle\n",
    "                            (pickle.dump(model, open('pickles/{model}_{t}_{max_features}_{max_depth}.pkl'\n",
    "                                                    .format(model=name, t=t,\n",
    "                                                            max_features=str(max_features),\n",
    "                                                            max_depth=str(max_depth)), 'wb')))\n",
    "                            print('{model} pickle saved!\\n'.format(model=name))\n",
    "\n",
    "                            # save net performance to list\n",
    "                            performance_list.append([f'{name}', period, max_features, max_depth, a, b, c , d, e])\n",
    "\n",
    "                        else:\n",
    "                            print('{model} does not have enough data!\\n'.format(model=name))\n",
    "                            \n",
    "                    except:\n",
    "                        print('error with model')\n",
    "\n",
    "    # create dataframe for model performance  \n",
    "    df = pd.DataFrame(performance_list, columns = ['ex_tp', 'period', 'max_features',\n",
    "                                                   'max_depth','pct_gain','gain', 'fees', \n",
    "                                                   'net_profit', 'pct_net_profit'])\n",
    "    \n",
    "    # sort by net profit descending and drop duplicates\n",
    "    df2 = df.sort_values(by='net_profit', ascending=False).drop_duplicates(subset='ex_tp')\n",
    "    \n",
    "    # get the names, periods, max_features, max_depth for best models\n",
    "    models = df2['ex_tp'].values\n",
    "    periods = df2['period'].values\n",
    "    max_features = df2['max_features'].values\n",
    "    max_depth = df2['max_depth'].values\n",
    "    \n",
    "    # save the best models in a new directory /tr_pickles\n",
    "    for i in range(len(models)):\n",
    "        model_name = models[i] + '_' + periods[i][:-1] + '_' + str(max_features[i]) + '_' + str(max_depth[i])\n",
    "        os.rename(f'pickles/{model_name}.pkl', f'tr_pickles/{models[i]}.pkl')\n",
    "    \n",
    "    # returning the dataframes for model performance\n",
    "    # df1 contains performance for all models trained\n",
    "    # df2 contains performance for best models\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods=['360T']\n",
    "df, df2 = modeling_pipeline(csv_filenames, periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training models with specific parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is not necessary if you do the above. It's for when you want to only train the best models if you know the parameters so you don't have to train 100s of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_pipeline(csv_filenames, param_dict):\n",
    "    \"\"\"Takes csv file paths of data for modeling and parameters, performs feature engineering,\n",
    "        train/test split, creates a model, reports train/test score, and saves\n",
    "        a pickle file of the model in a directory called /pickles.\"\"\"\n",
    "    \n",
    "    line = '------------'\n",
    "    \n",
    "    performance_list = []\n",
    "    \n",
    "    for file in csv_filenames:\n",
    "        \n",
    "        # define model name \n",
    "        name = file.split('/')[1][:-9]\n",
    "        \n",
    "        # read csv\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        params = param_dict[name]\n",
    "        print(params)\n",
    "        period = params['period']\n",
    "        print(period)\n",
    "        max_features = params['max_features']\n",
    "        max_depth = params['max_depth']\n",
    "\n",
    "        print(line + name + ' ' + period + line)\n",
    "\n",
    "        # engineer features\n",
    "        df = feature_engineering(df, period)\n",
    "\n",
    "        # train test split\n",
    "        train = df[df['date'] < '2018-10-30 23:00:00'] # cutoff oct 30 2018\n",
    "        test = df[df['date'] > '2019-01-01 23:00:00'] # cutoff jan 01 2019\n",
    "        print('train and test shape ({model}):'.format(model=name), train.shape, test.shape)\n",
    "\n",
    "        # features and target\n",
    "        features = df.drop(columns=['target', 'date']).columns.tolist()\n",
    "        target = 'target'\n",
    "\n",
    "        # define X, y vectors\n",
    "        X_train = train[features]\n",
    "        X_test = test[features]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "\n",
    "        # instantiate model\n",
    "        model = RandomForestClassifier(max_features=max_features, \n",
    "                                       max_depth=max_depth, \n",
    "                                       n_estimators=100, \n",
    "                                       n_jobs=-1, \n",
    "                                       random_state=42)\n",
    "\n",
    "        # fit model\n",
    "        if X_train.shape[0] > 500:\n",
    "            model.fit(X_train, y_train)\n",
    "            print('model fitted')\n",
    "\n",
    "            # train accuracy\n",
    "            train_score = model.score(X_train, y_train)\n",
    "            print('train accuracy:', train_score)\n",
    "\n",
    "            # make predictions\n",
    "            y_preds = model.predict(X_test)\n",
    "            print('predictions made')\n",
    "\n",
    "            # test accuracy\n",
    "            score = accuracy_score(y_test, y_preds)\n",
    "            print('test accuracy:', score)\n",
    "\n",
    "            # get profit and loss\n",
    "            a, b, c, d, e = performance(X_test, y_preds)\n",
    "            print(f'net profits: {str(round(d,2))}')\n",
    "\n",
    "            # formatting for filename\n",
    "            t = period[:-1]\n",
    "\n",
    "            # download pickle\n",
    "            pickle.dump(model, open('pickles/{model}_{t}.pkl'.format(model=name, t=t,), 'wb'))\n",
    "            print('{model} pickle saved!\\n'.format(model=name))\n",
    "\n",
    "            # save net performance to list\n",
    "            performance_list.append([f'{name}', period, a, b, c , d, e])\n",
    "\n",
    "        else:\n",
    "            print('{model} does not have enough data!\\n'.format(model=name))\n",
    "\n",
    "    # create df of model performance \n",
    "    df = pd.DataFrame(performance_list, columns = ['ex_tp', 'period', 'pct_gain',\n",
    "                                                   'gain', 'fees', 'net_profit', 'pct_net_profit'])\n",
    "    \n",
    "    # sort performance by net_profit and drop duplicates\n",
    "    df2 = df.sort_values(by='net_profit', ascending=False).drop_duplicates(subset='ex_tp')\n",
    "    models = df2['ex_tp'].values\n",
    "    periods = df2['period'].values\n",
    "    \n",
    "    # move models to new dir tr_pickles\n",
    "    for i in range(len(models)):\n",
    "        model_name = models[i] + '_' + periods[i][:-1]\n",
    "        os.rename(f'pickles/{model_name}.pkl', f'tr_pickles/{models[i]}.pkl')\n",
    "    \n",
    "    # returning the dataframes for model performance\n",
    "    # df1 contains performance for all models trained\n",
    "    # df2 contains performance for best models\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/bitfinex_ltc_usd_3600.csv', 'data/bitfinex_btc_usd_3600.csv', 'data/coinbase_pro_eth_usd_3600.csv', 'data/coinbase_pro_ltc_usd_3600.csv', 'data/bitfinex_eth_usd_3600.csv', 'data/coinbase_pro_btc_usd_3600.csv', 'data/hitbtc_ltc_usdt_3600.csv', 'data/hitbtc_btc_usdt_3600.csv']\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = {'bitfinex_ltc_usd': {'period': '1440T', 'max_features': 50, 'max_depth': 20}, \n",
    "              'hitbtc_ltc_usdt': {'period': '1440T', 'max_features': 45, 'max_depth': 27},\n",
    "              'coinbase_pro_ltc_usd': {'period': '960T', 'max_features': 50, 'max_depth': 17},\n",
    "              'hitbtc_btc_usdt': {'period': '360T', 'max_features': 40, 'max_depth': 17},\n",
    "              'coinbase_pro_btc_usd': {'period': '960T', 'max_features': 55, 'max_depth': 25},\n",
    "              'coinbase_pro_eth_usd': {'period': '960T', 'max_features': 50, 'max_depth': 27},\n",
    "              'bitfinex_btc_usd': {'period': '1200T', 'max_features': 55, 'max_depth': 25},\n",
    "              'bitfinex_eth_usd': {'period': '1200T', 'max_features': 60, 'max_depth': 20}\n",
    "              }\n",
    "\n",
    "# 'hitbtc_eth_usdt': {'period': '1440T', 'max_depth': 50}\n",
    "# ^ this cant go in param dict bc its trained differently\n",
    "\n",
    "csv_paths = csv_filenames.copy()\n",
    "del csv_paths[4]\n",
    "print(csv_paths)\n",
    "print(len(csv_paths))\n",
    "len(csv_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df2 = modeling_pipeline(csv_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train hitbtc eth_usdt model separately - was a special case where it performed better with less parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the hitbtc eth usdt model\n",
    "def modeling_pipeline(csv_filenames):\n",
    "    \"\"\"Takes csv file paths of data for modeling, performs feature engineering,\n",
    "        train/test split, creates a model, reports train/test score, and saves\n",
    "        a pickle file of the model in a directory called /pickles.\"\"\"\n",
    "    \n",
    "    line = '------------'\n",
    "    \n",
    "    performance_list = []\n",
    "    \n",
    "    for file in csv_filenames:\n",
    "        \n",
    "        # define model name \n",
    "        name = file.split('/')[1][:-9]\n",
    "        \n",
    "        # read csv\n",
    "        df = pd.read_csv(file, index_col=0)  \n",
    "\n",
    "        period = '1440T'\n",
    "        print(period)\n",
    "\n",
    "        print(line + name + ' ' + period + line)\n",
    "\n",
    "        # engineer features\n",
    "        df = feature_engineering(df, period)\n",
    "\n",
    "        # train test split\n",
    "        train = df[df['date'] < '2018-10-30 23:00:00'] # cutoff oct 30 2018\n",
    "        test = df[df['date'] > '2019-01-01 23:00:00'] # cutoff jan 01 2019\n",
    "        print('train and test shape ({model}):'.format(model=name), train.shape, test.shape)\n",
    "\n",
    "        # features and target\n",
    "        features = df.drop(columns=['target', 'date']).columns.tolist()\n",
    "        target = 'target'\n",
    "\n",
    "        # define X, y vectors\n",
    "        X_train = train[features]\n",
    "        X_test = test[features]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "\n",
    "        # instantiate model\n",
    "        model = RandomForestClassifier(max_depth=50, \n",
    "                                       n_estimators=100, \n",
    "                                       n_jobs=-1, \n",
    "                                       random_state=42)\n",
    "        \n",
    "        # filter out datasets that are too small\n",
    "        if X_train.shape[0] > 500:\n",
    "            # fit model\n",
    "            model.fit(X_train, y_train)\n",
    "            print('model fitted')\n",
    "\n",
    "            # train accuracy\n",
    "            train_score = model.score(X_train, y_train)\n",
    "            print('train accuracy:', train_score)\n",
    "\n",
    "            # make predictions\n",
    "            y_preds = model.predict(X_test)\n",
    "            print('predictions made')\n",
    "\n",
    "            # test accuracy\n",
    "            score = accuracy_score(y_test, y_preds)\n",
    "            print('test accuracy:', score)\n",
    "\n",
    "            # get profit and loss\n",
    "            a, b, c, d, e = performance(X_test, y_preds)\n",
    "            print(f'net profits: {str(round(d,2))}')\n",
    "\n",
    "            # formatting for filename\n",
    "            t = period[:-1]\n",
    "\n",
    "            # download pickle\n",
    "            pickle.dump(model, open('pickles/{model}_{t}.pkl'.format(model=name, t=t,), 'wb'))\n",
    "            print('{model} pickle saved!\\n'.format(model=name))\n",
    "\n",
    "            # save net performance to list\n",
    "            performance_list.append([f'{name}', period, a, b, c , d, e])\n",
    "\n",
    "        else:\n",
    "            print('{model} does not have enough data!\\n'.format(model=name))\n",
    "\n",
    "    # create df of model performance    \n",
    "    df = pd.DataFrame(performance_list, columns = ['ex_tp', 'period', 'pct_gain',\n",
    "                                                   'gain', 'fees', 'net_profit', 'pct_net_profit'])\n",
    "    \n",
    "    models = df2['ex_tp'].values\n",
    "    periods = df2['period'].values\n",
    "    \n",
    "    # move model to new dir tr_pickles\n",
    "    for i in range(len(models)):\n",
    "        model_name = models[i] + '_' + periods[i][:-1]\n",
    "        os.rename(f'pickles/{model_name}.pkl', f'tr_pickles/{models[i]}.pkl')\n",
    "        \n",
    "    # returning the dataframes for model performance\n",
    "    # df1 contains performance for all models trained\n",
    "    # df2 contains performance for best models\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = ['data/hitbtc_eth_usdt_3600.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440T\n",
      "------------hitbtc_eth_usdt 1440T------------\n",
      "train and test shape (hitbtc_eth_usdt): (543, 69) (272, 69)\n",
      "model fitted\n",
      "train accuracy: 1.0\n",
      "predictions made\n",
      "test accuracy: 0.46691176470588236\n",
      "net profits: 8874.99\n",
      "hitbtc_eth_usdt pickle saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df, df2 = modeling_pipeline(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- neural networks\n",
    "- implement NLP with data scraped from twitter to see how frequency of crypto discussion affects the predictions\n",
    "- more exchange/trading pair support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryptolytic",
   "language": "python",
   "name": "cryptolytic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
