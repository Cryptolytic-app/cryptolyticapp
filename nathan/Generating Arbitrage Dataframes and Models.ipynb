{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bitfinex_eos_usdt_300.csv', 'hitbtc_eos_usdt_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'coinbase_pro_bch_btc_300.csv'],\n",
       " ['bitfinex_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['bitfinex_etc_usd_300.csv', 'coinbase_pro_etc_usd_300.csv'],\n",
       " ['bitfinex_btc_usd_300.csv', 'coinbase_pro_btc_usd_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'coinbase_pro_ltc_btc_300.csv'],\n",
       " ['bitfinex_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['bitfinex_dash_usd_300.csv', 'coinbase_pro_dash_usd_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'coinbase_pro_dash_btc_300.csv'],\n",
       " ['bitfinex_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['bitfinex_ltc_usd_300.csv', 'coinbase_pro_ltc_usd_300.csv'],\n",
       " ['bitfinex_bch_usdt_300.csv', 'hitbtc_bch_usdt_300.csv'],\n",
       " ['bitfinex_bch_usd_300.csv', 'coinbase_pro_bch_usd_300.csv'],\n",
       " ['bitfinex_eos_usd_300.csv', 'coinbase_pro_eos_usd_300.csv'],\n",
       " ['bitfinex_xrp_usd_300.csv', 'coinbase_pro_xrp_usd_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'coinbase_pro_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['bitfinex_eth_usdt_300.csv', 'hitbtc_eth_usdt_300.csv'],\n",
       " ['bitfinex_eth_usd_300.csv', 'coinbase_pro_eth_usd_300.csv'],\n",
       " ['bitfinex_ltc_usdt_300.csv', 'hitbtc_ltc_usdt_300.csv'],\n",
       " ['bitfinex_zrx_usd_300.csv', 'coinbase_pro_zrx_usd_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'coinbase_pro_xrp_btc_300.csv'],\n",
       " ['bitfinex_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'coinbase_pro_eos_btc_300.csv'],\n",
       " ['bitfinex_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['bitfinex_btc_usdt_300.csv', 'hitbtc_btc_usdt_300.csv'],\n",
       " ['coinbase_pro_dash_btc_300.csv', 'hitbtc_dash_btc_300.csv'],\n",
       " ['coinbase_pro_eth_btc_300.csv', 'hitbtc_eth_btc_300.csv'],\n",
       " ['coinbase_pro_xrp_btc_300.csv', 'hitbtc_xrp_btc_300.csv'],\n",
       " ['coinbase_pro_eos_btc_300.csv', 'hitbtc_eos_btc_300.csv'],\n",
       " ['coinbase_pro_eth_usdc_300.csv', 'hitbtc_eth_usdc_300.csv'],\n",
       " ['coinbase_pro_bch_btc_300.csv', 'hitbtc_bch_btc_300.csv'],\n",
       " ['coinbase_pro_ltc_btc_300.csv', 'hitbtc_ltc_btc_300.csv'],\n",
       " ['coinbase_pro_btc_usdc_300.csv', 'hitbtc_btc_usdc_300.csv']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_file_pairs(exchanges):\n",
    "    filenames = []\n",
    "    for directory in os.listdir('ohlcv_data'):\n",
    "        if directory != '.DS_Store':\n",
    "            for filename in os.listdir('ohlcv_data/' + directory):\n",
    "                if filename.endswith('300.csv'):\n",
    "                     filenames.append(filename)\n",
    "    file_pairs = []\n",
    "    for filename_1 in filenames:\n",
    "        remaining_filenames = filenames[filenames.index(filename_1)+1:]\n",
    "        for filename_2 in remaining_filenames:\n",
    "            for exchange in exchanges:\n",
    "                if filename_1.replace(exchange, '') in filename_2:\n",
    "                    file_pairs.append([filename_1, filename_2])\n",
    "    return file_pairs\n",
    "\n",
    "exchanges = ['bitfinex', 'coinbase_pro', 'hitbtc']\n",
    "get_file_pairs(exchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ohlcv(df, period='5T'):\n",
    "\n",
    "    # Set date as the index. This is needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "\n",
    "    # Aggregation function\n",
    "    ohlc_dict = {                                                                                                             \n",
    "    'open':'first',                                                                                                    \n",
    "    'high':'max',                                                                                                       \n",
    "    'low':'min',                                                                                                        \n",
    "    'close': 'last',                                                                                                    \n",
    "    'base_volume': 'sum'\n",
    "    }\n",
    "\n",
    "    # Apply resampling.\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ta import add_all_ta_features\n",
    "\n",
    "def fill_nan(df):\n",
    "    \n",
    "    df['close'] = df['close'].ffill()\n",
    "    df = df.bfill(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "    df = resample_ohlcv(df)\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    df['date'] = df['date'].astype('int64')//1e9\n",
    "    df = df.rename(columns={'date': 'closing_time'})\n",
    "        \n",
    "    df['nan_ohlcv'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    df = fill_nan(df)\n",
    "        \n",
    "    df = add_all_ta_features(df, 'open', 'high', 'low', 'close',\n",
    "                             'base_volume', fillna=True)\n",
    "    \n",
    "    df['closing_time'] = df['closing_time'].astype('int64')\n",
    "    df['nan_ohlcv'] = df['nan_ohlcv'].astype('int64')\n",
    "    \n",
    "    df = df.drop(columns=['open', 'high', 'low', 'momentum_kama',\n",
    "                          'momentum_stoch', 'others_cr', 'others_dlr',\n",
    "                          'trend_ema_fast', 'trend_ema_slow', \n",
    "                          'trend_ichimoku_a', 'trend_ichimoku_b', 'trend_kst',\n",
    "                          'trend_macd', 'trend_visual_ichimoku_a',\n",
    "                          'trend_visual_ichimoku_b', 'volatility_bbh',\n",
    "                          'volatility_bbl', 'volatility_bbm',\n",
    "                          'volatility_dch', 'volatility_dcl',\n",
    "                          'volatility_kcc', 'volatility_kch',\n",
    "                          'volatility_kcl'])\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_closing_price(df):\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 'exchange_1'\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 'exchange_2'\n",
    "    else:\n",
    "        return 'equivalent'\n",
    "\n",
    "def get_pct_higher(df):\n",
    "    if df['higher_closing_price'] == 'exchange_1':\n",
    "        return ((df['close_exchange_1'] / \n",
    "                 df['close_exchange_2'])-1)*100\n",
    "    elif df['higher_closing_price'] == 'exchange_2':\n",
    "        return ((df['close_exchange_2'] / \n",
    "                 df['close_exchange_1'])-1)*100\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_arbitrage_opportunity(df):\n",
    "    if df['pct_higher'] < .55:\n",
    "        return 0 # no arbitrage\n",
    "    elif df['higher_closing_price'] == 'exchange_1':\n",
    "        return -1 # arbitrage exchange 2 to exchange 1\n",
    "    elif df['higher_closing_price'] == 'exchange_2':\n",
    "        return 1 # arbitrage exchange 1 to exchange 2\n",
    "    \n",
    "def get_window_length(df):\n",
    "    target_list = df['arbitrage_opportunity'].to_list()\n",
    "    window_length = 5\n",
    "    window_lengths = []\n",
    "    for i in range(len(target_list)):\n",
    "            if target_list[i] == target_list[i-1]:\n",
    "                window_length += 5\n",
    "                window_lengths.append(window_length)\n",
    "            else:\n",
    "                window_length = 5\n",
    "                window_lengths.append(window_length)      \n",
    "    df['window_length'] = window_lengths\n",
    "    return df\n",
    "        \n",
    "def merge_dfs(df1, df2):\n",
    "    df = pd.merge(df1, df2, on='closing_time',\n",
    "                  suffixes=('_exchange_1', '_exchange_2'))\n",
    "        \n",
    "    df['year'] = pd.to_datetime(df['closing_time'], unit='s').dt.year\n",
    "    df['month'] = pd.to_datetime(df['closing_time'], unit='s').dt.month\n",
    "    df['day'] = pd.to_datetime(df['closing_time'], unit='s').dt.day\n",
    "\n",
    "    df['higher_closing_price'] = df.apply(get_higher_closing_price, axis=1)\n",
    "    df['pct_higher'] = df.apply(get_pct_higher, axis=1)\n",
    "    df['arbitrage_opportunity'] = df.apply(get_arbitrage_opportunity, axis=1)\n",
    "    df = get_window_length(df)\n",
    "    df = df.drop(columns=['higher_closing_price', 'pct_higher'])\n",
    "    df = df[:-8]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval=30\n",
    "\n",
    "def get_target_value(df, interval=interval):\n",
    "\n",
    "    if df['window_length_shift'] >= interval:\n",
    "        if df['arbitrage_opportunity_shift'] == 1:\n",
    "            return 1\n",
    "        elif df['arbitrage_opportunity_shift'] == -1:\n",
    "            return -1\n",
    "        elif df['arbitrage_opportunity_shift'] == 0:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_target(df, interval=interval):\n",
    "    \n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    \n",
    "    df['arbitrage_opportunity_shift'] = df['arbitrage_opportunity'].shift(\n",
    "        rows_to_shift)\n",
    "    df['window_length_shift'] = df['window_length'].shift(rows_to_shift)\n",
    "    \n",
    "    df['target'] = df.apply(get_target_value, axis=1)\n",
    "    \n",
    "    df = df.drop(columns=['window_length_shift',\n",
    "                          'arbitrage_opportunity_shift'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higher_closing_price(df):\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        return 'exchange_1'\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        return 'exchange_2'\n",
    "    else:\n",
    "        return 'equivalent'\n",
    "    \n",
    "def get_close_shift(df, interval=interval):\n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(rows_to_shift)\n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(rows_to_shift)\n",
    "    return df\n",
    "\n",
    "def get_profit(df):\n",
    "    if df['higher_closing_price'] == 'exchange_1':\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    elif df['higher_closing_price'] == 'exchange_2':\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineering df1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...)..apply(<func>)\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.7/site-packages/ta/trend.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[i] = 100 * (dip_mio[i]/trs[i])\n",
      "/anaconda3/lib/python3.7/site-packages/ta/trend.py:174: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[i] = 100 * (din_mio[i]/trs[i])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_all_arbitrage_dfs_and_models(exchanges):\n",
    "    for pair in get_file_pairs(exchanges):\n",
    "        for exchange in exchanges:\n",
    "            if exchange in pair[0]:\n",
    "                exchange_1 = exchange\n",
    "            if exchange in pair[1]:\n",
    "                exchange_2 = exchange\n",
    "        \n",
    "        df1 = get_df('ohlcv_data/' + exchange_1 + '_300/' + pair[0])\n",
    "        print('engineering df1...')\n",
    "        df1 = engineer_features(df1)\n",
    "        print('success!')\n",
    "\n",
    "        df2 = get_df('ohlcv_data/' + exchange_2 + '_300/' + pair[1])\n",
    "        print('engineering df2...')\n",
    "        df2 = engineer_features(df2)\n",
    "        print('success!')\n",
    "\n",
    "        print('merging df1 and df2...')\n",
    "        df = merge_dfs(df1, df2)\n",
    "        print('success!')\n",
    "        \n",
    "        end_of_model_name = '_' + pair[1].replace('_300.csv', '.pkl')\n",
    "        model_name = exchange_1 + end_of_model_name\n",
    "        print(model_name.replace('.pkl', '').upper())\n",
    "        \n",
    "        df = get_target(df)\n",
    "        \n",
    "        test_train_split_row = round(len(df)*.7)\n",
    "        test_train_split_time = df['closing_time'][test_train_split_row]\n",
    "\n",
    "        train_cutoff_time = test_train_split_time - 604800\n",
    "        test_cutoff_time = test_train_split_time + 604800\n",
    "\n",
    "        train = df[df['closing_time'] < train_cutoff_time]\n",
    "        test = df[df['closing_time'] > test_cutoff_time]\n",
    "        print('train and test shape:'.format(model=model_name), \n",
    "              train.shape, test.shape)\n",
    "\n",
    "        features = df.drop(columns=['target']).columns.tolist()\n",
    "        target = 'target'\n",
    "\n",
    "        X_train = train[features]\n",
    "        X_test = test[features]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "        \n",
    "        model = RandomForestClassifier(max_depth=75, n_estimators=100, \n",
    "                                       n_jobs=-1, random_state=42)\n",
    "        \n",
    "        if X_train.shape[0] > 1000:\n",
    "            model.fit(X_train, y_train)\n",
    "            print('model fitted!')\n",
    "\n",
    "            train_score = model.score(X_train, y_train)\n",
    "            print('train accuracy:', train_score)\n",
    "\n",
    "            y_preds = model.predict(X_test)\n",
    "            print('predictions made!')\n",
    "\n",
    "            score = accuracy_score(y_test, y_preds)\n",
    "            print('test accuracy:', score)\n",
    "\n",
    "            pickle.dump(model, open('pickles/{model}.pkl'.format(\n",
    "                model=model_name), 'wb'))\n",
    "            print('pickle saved!'.format(model=model) + '\\n')\n",
    "                \n",
    "            unique_y_test = y_test.unique().tolist()\n",
    "            unique_y_preds = list(set(y_preds))\n",
    "            labels = list(set(unique_y_test + unique_y_preds))\n",
    "            labels.sort()\n",
    "            columns = [f'Predicted {label}' for label in labels]\n",
    "            index = [f'Actual {label}'  for label in labels]\n",
    "                \n",
    "            confusion = pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "                                     columns=columns, index=index)\n",
    "                \n",
    "            print(model_name + ' confusion matrix:')\n",
    "            print(confusion, '\\n')\n",
    "                \n",
    "            test_with_preds = X_test\n",
    "            test_with_preds['higher_closing_price'\n",
    "                           ] = test_with_preds.apply(\n",
    "                get_higher_closing_price, axis=1)\n",
    "            test_with_preds = get_close_shift(test_with_preds)\n",
    "            test_with_preds['pred'] = y_preds\n",
    "            test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "                get_profit, axis=1).shift(-1)\n",
    "                \n",
    "            pct_profit_mean = test_with_preds[test_with_preds[\n",
    "                'pred'] != 0]['pct_profit'].mean()\n",
    "            pct_profit_median = test_with_preds[test_with_preds[\n",
    "                'pred'] != 0]['pct_profit'].median()\n",
    "                \n",
    "            print('percent profit mean:', pct_profit_mean)\n",
    "            print('percent profit median:', pct_profit_median, '\\n\\n')\n",
    "\n",
    "        else:\n",
    "            print('not enough data!'.format(model=model_name))\n",
    "\n",
    "exchanges = ['bitfinex', 'coinbase_pro', 'hitbtc']\n",
    "create_all_arbitrage_dfs_and_models(exchanges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
