{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# the three exchanges we are using...\n",
    "exchanges = ['bitfinex', 'coinbase_pro', 'gemini', 'hitbtc', 'kraken']\n",
    "\n",
    "# function to get pairs of ohlcv csvs from which to create arbitrage data\n",
    "def get_file_pairs(exchanges):\n",
    "    # empty list to fill with filenames of all ohlcv csvs\n",
    "    filenames = []\n",
    "    # i.e., for subdirectory in ohlcv_data directory\n",
    "    for directory in os.listdir('ohlcv_data'):\n",
    "        # .DS_Store files can mess things up, since they aren't directories\n",
    "        if directory != '.DS_Store':\n",
    "            # for each of the files in the subdirectory...\n",
    "            for filename in os.listdir('ohlcv_data/' + directory):\n",
    "                # if the file is a csv...\n",
    "                if filename.endswith('300.csv'):\n",
    "                    # add the filename to the list of filenames\n",
    "                    filenames.append(filename)\n",
    "    # empty list to fill with pairs of csvs from which to make arbitrage data\n",
    "    file_pairs = []\n",
    "    # filename_1, because we will want to compare each filename to another\n",
    "    for filename_1 in filenames:\n",
    "        # these are all the filenames we haven't looped through yet\n",
    "        remaining_filenames = filenames[filenames.index(filename_1)+1:]\n",
    "        # for each of those filenames we haven't looped through yet...\n",
    "        for filename_2 in remaining_filenames:\n",
    "            # exchanges is a list taken as an argument by this function\n",
    "            for exchange in exchanges:\n",
    "                # drop the exchange from the first filename, see if the\n",
    "                # remaining string is contained in the second filename\n",
    "                if filename_1.replace(exchange, '') in filename_2:\n",
    "                    # if so, add the pair of filenames to the list of pairs\n",
    "                    file_pairs.append([filename_1, filename_2])\n",
    "    # return the list of pairs\n",
    "    return file_pairs\n",
    "\n",
    "# getting the list of ohlcv csvs from which to create arbitrage data\n",
    "get_file_pairs(exchanges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# simple function to turn a csv into a dataframe\n",
    "def get_df(filename):\n",
    "    # index_col=0 because csv still has index\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    # returning the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function resamples ohlcv csvs for a specified candle interval; while \n",
    "# this can be used to change the candle interval for the data, it can also be\n",
    "# used to fill in gaps in the ohlcv data without changing the candle interval\n",
    "def resample_ohlcv(df, period='5T'):\n",
    "    # set the date as the index; this is needed for the function to run\n",
    "    df = df.set_index(['date'])\n",
    "    # dictionary specifying which columns to use for resampling\n",
    "    ohlc_dict = {                                                                                                             \n",
    "    'open':'first',                                                                                                    \n",
    "    'high':'max',                                                                                                       \n",
    "    'low':'min',                                                                                                        \n",
    "    'close': 'last',                                                                                                    \n",
    "    'base_volume': 'sum'\n",
    "    }\n",
    "    # overwriting the df taken as input with a resampled df\n",
    "    df = df.resample(period, how=ohlc_dict, closed='left', label='left')\n",
    "    # returning the resampled df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ta import add_all_ta_features\n",
    "\n",
    "# function to handle nans in the data introduced by resampling\n",
    "def fill_nan(df):\n",
    "    # forward filling the closing price where there were gaps in ohlcv csv\n",
    "    df['close'] = df['close'].ffill()\n",
    "    # backfilling the rest of the nans\n",
    "    df = df.bfill(axis=1)\n",
    "    # returning the revised dataframe\n",
    "    return df\n",
    "\n",
    "# function to engineer features that can be engineered pre-merge...\n",
    "def engineer_features(df):\n",
    "    \n",
    "    # turn the closing_time, which is in Unix time, to datetime...\n",
    "    df['date'] = pd.to_datetime(df['closing_time'], unit='s')\n",
    "    # ...which is needed for resampling; resampling fills gaps in data\n",
    "    df = resample_ohlcv(df)\n",
    "    # resetting the index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # now that df has been resampled, converting back to Unix time...\n",
    "    # dividing by 1e9 to get seconds, not nanoseconds\n",
    "    df['date'] = df['date'].astype('int64')//1e9\n",
    "    # also changing name back to closing_time, to be more precise\n",
    "    df = df.rename(columns={'date': 'closing_time'})\n",
    "    \n",
    "    # adding feature to indicate where rows are just filling gaps in data...\n",
    "    df['nan_ohlcv'] = df['close'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "    # now filling in the nan values in those gap-filling rows...\n",
    "    df = fill_nan(df)\n",
    "    \n",
    "    # adding all the technical analysis features...\n",
    "    df = add_all_ta_features(df, 'open', 'high', 'low', 'close',\n",
    "                             'base_volume', fillna=True)\n",
    "    \n",
    "    # technical analysis library converts some ints to floats; changing back\n",
    "    df['closing_time'] = df['closing_time'].astype('int64')\n",
    "    df['nan_ohlcv'] = df['nan_ohlcv'].astype('int64')\n",
    "    \n",
    "    # dropping features that are highly correlated with other features\n",
    "    df = df.drop(columns=['open', 'high', 'low', 'momentum_kama',\n",
    "                          'momentum_stoch', 'others_cr', 'others_dlr',\n",
    "                          'trend_ema_fast', 'trend_ema_slow', \n",
    "                          'trend_ichimoku_a', 'trend_ichimoku_b', 'trend_kst',\n",
    "                          'trend_macd', 'trend_visual_ichimoku_a',\n",
    "                          'trend_visual_ichimoku_b', 'volatility_bbh',\n",
    "                          'volatility_bbl', 'volatility_bbm',\n",
    "                          'volatility_dch', 'volatility_dcl',\n",
    "                          'volatility_kcc', 'volatility_kch',\n",
    "                          'volatility_kcl'])\n",
    "    \n",
    "    # returning resulting dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following functions are used in engineering features post-merge...\n",
    "\n",
    "# function to create column showing which exchange has a higher closing price\n",
    "def get_higher_closing_price(df):\n",
    "    # i.e., if exchange 1 has the higher closing price...\n",
    "    if (df['close_exchange_1'] - df['close_exchange_2']) > 0:\n",
    "        # return exchange 1\n",
    "        return 'exchange_1'\n",
    "    # otherwise, if exchange 2 has the higher closing price...\n",
    "    elif (df['close_exchange_1'] - df['close_exchange_2']) < 0:\n",
    "        # return exchange 2\n",
    "        return 'exchange_2'\n",
    "    # otherwise, i.e., if neither has a higher closing price...\n",
    "    else:\n",
    "        # return equivalent\n",
    "        return 'equivalent'\n",
    "\n",
    "# function to create column showing percentage by which higher price is higher\n",
    "def get_pct_higher(df):\n",
    "    # i.e., if exchange 1 has a higher closing price than exchange 2...\n",
    "    if df['higher_closing_price'] == 'exchange_1':\n",
    "        # return the percentage by which the exchange 1 closing price is \n",
    "        # greater than the exchange 2 closing price\n",
    "        return ((df['close_exchange_1'] / \n",
    "                 df['close_exchange_2'])-1)*100\n",
    "    # otherwise, if exchange 2 has a higher closing price than exchange 1...\n",
    "    elif df['higher_closing_price'] == 'exchange_2':\n",
    "        # return the percentage by which the exchange 2 closing price is\n",
    "        # greater than the exchange 1 closing price\n",
    "        return ((df['close_exchange_2'] / \n",
    "                 df['close_exchange_1'])-1)*100\n",
    "    # otherwise, i.e., if the closing prices are equivalent...\n",
    "    else:\n",
    "        # return zero\n",
    "        return 0\n",
    "\n",
    "# function to create column showing available arbitrage opportunities\n",
    "def get_arbitrage_opportunity(df):\n",
    "    # assuming the total fees are 0.55%, if the higher closing price is less\n",
    "    # than 0.55% higher than the lower closing price...\n",
    "    if df['pct_higher'] < .55:\n",
    "        # return 0, for no arbitrage\n",
    "        return 0\n",
    "    # otherwise, if the exchange 1 closing price is more than 0.55% higher\n",
    "    # than the exchange 2 closing price...\n",
    "    elif df['higher_closing_price'] == 'exchange_1':\n",
    "        # return -1, for arbitrage from exchange 2 to exchange 1\n",
    "        return -1\n",
    "    # otherwise, if the exchange 2 closing price is more than 0.55% higher\n",
    "    # than the exchange 1 closing price...\n",
    "    elif df['higher_closing_price'] == 'exchange_2':\n",
    "        # return 1, for arbitrage from exchange 1 to exchange 2\n",
    "        return 1\n",
    "    \n",
    "# function to create column showing how long arbitrage opportunity has lasted\n",
    "def get_window_length(df):\n",
    "    # converting arbitrage_opportunity column to a list...\n",
    "    target_list = df['arbitrage_opportunity'].to_list()\n",
    "    # setting initial window length to 5, for 5 minutes; will be updated...\n",
    "    window_length = 5\n",
    "    # creating empty list to fill with values and ultimately convert to column\n",
    "    window_lengths = []\n",
    "    # for i in the range of the length of the arbitrage_opportunity column...\n",
    "    for i in range(len(target_list)):\n",
    "        # if a value in the arbitrage_opportunity column is equal to the\n",
    "        # previous value in the arbitrage_opportunity column...\n",
    "        if target_list[i] == target_list[i-1]:\n",
    "            # increase the window length by five minutes...\n",
    "            window_length += 5\n",
    "            # and append that window length to the list.\n",
    "            window_lengths.append(window_length)\n",
    "        # otherwise, i.e., if a value in the arbitrage_opportunity column is\n",
    "        # not equal to the previous value in the arbitrage_opportunity column\n",
    "        else:\n",
    "            # reset the window length to five minutes...\n",
    "            window_length = 5\n",
    "            # and append that window length to the list\n",
    "            window_lengths.append(window_length)\n",
    "    # convert the window lengths list to a column, showing how long arbitrage\n",
    "    # window / no_arbitrage window has lasted.\n",
    "    df['window_length'] = window_lengths\n",
    "    # return the dataframe with the new window length column\n",
    "    return df\n",
    "        \n",
    "# function to merge dataframes and create final features for arbitrage data\n",
    "def merge_dfs(df1, df2):\n",
    "    # merging two modified ohlcv dfs on closing time to create arbitrage df\n",
    "    df = pd.merge(df1, df2, on='closing_time',\n",
    "                  suffixes=('_exchange_1', '_exchange_2'))\n",
    "    \n",
    "    # feature engineering year, month, and day columns\n",
    "    df['year'] = pd.to_datetime(df['closing_time'], unit='s').dt.year\n",
    "    df['month'] = pd.to_datetime(df['closing_time'], unit='s').dt.month\n",
    "    df['day'] = pd.to_datetime(df['closing_time'], unit='s').dt.day\n",
    "\n",
    "    # getting higher_closing_price feature to create pct_higher feature\n",
    "    df['higher_closing_price'] = df.apply(get_higher_closing_price, axis=1)\n",
    "    # getting pct_higher feature to create arbitrage_opportunity feature\n",
    "    df['pct_higher'] = df.apply(get_pct_higher, axis=1)\n",
    "    # getting arbitrage_opportunity feature\n",
    "    df['arbitrage_opportunity'] = df.apply(get_arbitrage_opportunity, axis=1)\n",
    "    # getting window_length feature\n",
    "    df = get_window_length(df)\n",
    "    # dropping higher_closing_price and pct_higher features, which were\n",
    "    # only needed to feature engineer arbitrage_opportunity and window_length\n",
    "    df = df.drop(columns=['higher_closing_price', 'pct_higher'])\n",
    "    # returning df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target column...\n",
    "\n",
    "# specifying arbitrage window length to target, in minutes\n",
    "interval=30\n",
    "\n",
    "# function to get target values; takes df and window length to target\n",
    "def get_target_value(df, interval=interval):\n",
    "    # i.e., if the coming arbitrage window is as long as the targeted interval\n",
    "    if df['window_length_shift'] >= interval:\n",
    "        # then if the coming arbitrage window is for exchange 1 to 2...\n",
    "        if df['arbitrage_opportunity_shift'] == 1:\n",
    "            # return 1, which means arbitrage from exchange 1 to 2\n",
    "            return 1\n",
    "        # otherwise, if the coming arbitrage window is for exchange 2 to 1...\n",
    "        elif df['arbitrage_opportunity_shift'] == -1:\n",
    "            # return -1, which means arbitrage from exchange 2 to 1...\n",
    "            return -1\n",
    "        # otherwise, if we are coming up on no arbitrage opportunity...\n",
    "        elif df['arbitrage_opportunity_shift'] == 0:\n",
    "            # return 0, which means no arbitrage opportunity\n",
    "            return 0\n",
    "    # otherwise, i.e., if the coming window is less than our targeted interval\n",
    "    else:\n",
    "        # return 0, which means no arbitrage opportunity\n",
    "        return 0\n",
    "    \n",
    "# function to create target column\n",
    "def get_target(df, interval=interval):\n",
    "    # used to shift rows; assumes candle length is five minutes, interval is\n",
    "    # in minutes\n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    # arbitrage_opportunity feature, shifted by length of targeted interval,\n",
    "    # minus one to predict ten minutes in advance rather than five\n",
    "    df['arbitrage_opportunity_shift'] = df['arbitrage_opportunity'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    # window_length feature, shifted by length of targeted interval, minus one\n",
    "    # to predict ten minutes in advance rather than five\n",
    "    df['window_length_shift'] = df['window_length'].shift(rows_to_shift - 1)\n",
    "    # creating target column; this will indicate if an arbitrage opportunity\n",
    "    # that lasts as long as the targeted interval is forthcoming\n",
    "    df['target'] = df.apply(get_target_value, axis=1)\n",
    "    # dropping unncessary columns, which were only needed to engineer target\n",
    "    df = df.drop(columns=['window_length_shift',\n",
    "                          'arbitrage_opportunity_shift'])\n",
    "    # dropping rows where target could not be calculated due to shift\n",
    "    df = df[:rows_to_shift - 1]\n",
    "    # returning resulting dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions needed to calculate profit...\n",
    "    \n",
    "# function to create new features out of closing prices, shifting those\n",
    "# prices by the targeted interval, minus one to predict ten minutes in advance\n",
    "# rather than five\n",
    "def get_close_shift(df, interval=interval):\n",
    "    rows_to_shift = int(-1*(interval/5))\n",
    "    df['close_exchange_1_shift'] = df['close_exchange_1'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    df['close_exchange_2_shift'] = df['close_exchange_2'].shift(\n",
    "        rows_to_shift - 1)\n",
    "    return df\n",
    "\n",
    "# function to create profit feature\n",
    "def get_profit(df):\n",
    "    # if exchange 1 has the higher closing price...\n",
    "    if df['higher_closing_price'] == 'exchange_1':\n",
    "        # see how much money you would make if you bought on exchange 2, sold\n",
    "        # on exchange 1, and took account of 0.55% fees\n",
    "        return (((df['close_exchange_1_shift'] / \n",
    "                 df['close_exchange_2'])-1)*100)-.55\n",
    "    # otherwise, if exchange 2 has the higher closing price...\n",
    "    elif df['higher_closing_price'] == 'exchange_2':\n",
    "        # see how much money you would make if you bought on exchange 1, sold\n",
    "        # on exchange 2, and took account of 0.55% fees\n",
    "        return (((df['close_exchange_2_shift'] / \n",
    "                 df['close_exchange_1'])-1)*100)-.55\n",
    "    # otherwise, i.e., if the closing prices are the same...\n",
    "    else:\n",
    "        # return zero, because in that case you shouldn't make a trade\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# now the master function that creates models from ohlcv csvs...\n",
    "def create_all_arbitrage_dfs_and_models(exchanges):\n",
    "    # looping through the file pairs used to generate the arbitrage data...\n",
    "    for pair in get_file_pairs(exchanges):\n",
    "        # looping through the specified exchanges...\n",
    "        for exchange in exchanges:\n",
    "            # if one of the specified exchanges is in the first filename...\n",
    "            if exchange in pair[0]:\n",
    "                # that is the first exchange;\n",
    "                exchange_1 = exchange\n",
    "            # if one of the specified exchanges is in the second filename...\n",
    "            if exchange in pair[1]:\n",
    "                # that is the second exchange.\n",
    "                exchange_2 = exchange\n",
    "        \n",
    "        # loading first ohlcv csv in pair...\n",
    "        df1 = get_df('ohlcv_data/' + exchange_1 + '_300/' + pair[0])\n",
    "        # engineering features for first ohlcv csv...\n",
    "        print('engineering df1...')\n",
    "        df1 = engineer_features(df1)\n",
    "        print('success!')\n",
    "\n",
    "        # loading second ohlcv csv in pair...\n",
    "        df2 = get_df('ohlcv_data/' + exchange_2 + '_300/' + pair[1])\n",
    "        # engineering features for second ohlcv csv...\n",
    "        print('engineering df2...')\n",
    "        df2 = engineer_features(df2)\n",
    "        print('success!')\n",
    "\n",
    "        # merging two ohlcv dataframes with their engineered features\n",
    "        print('merging df1 and df2...')\n",
    "        df = merge_dfs(df1, df2)\n",
    "        print('success!')\n",
    "        \n",
    "        # getting the second half of the filename for the eventual model...\n",
    "        end_of_model_name = '_' + pair[1].replace('_300.csv', '.pkl')\n",
    "        # assembling whole of the filename for the eventual model...\n",
    "        model_name = exchange_1 + end_of_model_name\n",
    "        # printing the model name to track progress...\n",
    "        print(model_name.replace('.pkl', '').upper())\n",
    "        \n",
    "        # getting the target feature\n",
    "        df = get_target(df)\n",
    "        \n",
    "        # where to split df for 70/30 test/train split...\n",
    "        test_train_split_row = round(len(df)*.7)\n",
    "        # getting closing time for row at which test/train split is made...\n",
    "        test_train_split_time = df['closing_time'][test_train_split_row]\n",
    "\n",
    "        # subtracting one week from that closing time for training data...\n",
    "        train_cutoff_time = test_train_split_time - 604800\n",
    "        # adding one week to that closing time for test data...\n",
    "        test_cutoff_time = test_train_split_time + 604800\n",
    "        # used to ensure we have a two week gap between test and train data\n",
    "        \n",
    "        # training set will end one week before the 7/10th row in dataframe\n",
    "        train = df[df['closing_time'] < train_cutoff_time]\n",
    "        # test set will begin one week after the 7/10th row in dataframe\n",
    "        test = df[df['closing_time'] > test_cutoff_time]\n",
    "        # printing shapes to track progress\n",
    "        print('train and test shape:'.format(model=model_name), \n",
    "              train.shape, test.shape)\n",
    "\n",
    "        # model uses all features; only dropping target\n",
    "        features = df.drop(columns=['target']).columns.tolist()\n",
    "        # specifying name of target column\n",
    "        target = 'target'\n",
    "\n",
    "        # separating features from target\n",
    "        X_train = train[features]\n",
    "        X_test = test[features]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "        \n",
    "        # defining model\n",
    "        model = RandomForestClassifier(max_depth=75, n_estimators=100, \n",
    "                                       n_jobs=-1, random_state=42)\n",
    "        \n",
    "        # i.e., provided we have enough data to train on, and for testing...\n",
    "        if (X_train.shape[0] > 1000) and (X_test.shape[0] > 0):\n",
    "            # fitting the model...\n",
    "            model.fit(X_train, y_train)\n",
    "            print('model fitted!')\n",
    "            # getting accuracy score for train set...\n",
    "            train_score = model.score(X_train, y_train)\n",
    "            print('train accuracy:', train_score)\n",
    "            # making predictions...\n",
    "            y_preds = model.predict(X_test)\n",
    "            print('predictions made!')\n",
    "            # getting accuracy score for test set...\n",
    "            score = accuracy_score(y_test, y_preds)\n",
    "            print('test accuracy:', score)\n",
    "\n",
    "            # saving the model...\n",
    "            pickle.dump(model, open('pickles/{model}.pkl'.format(\n",
    "                model=model_name), 'wb'))\n",
    "            print('pickle saved!'.format(model=model) + '\\n')\n",
    "                \n",
    "            # getting labels for confusion matrix...\n",
    "            unique_y_test = y_test.unique().tolist()\n",
    "            unique_y_preds = list(set(y_preds))\n",
    "            labels = list(set(unique_y_test + unique_y_preds))\n",
    "            labels.sort()\n",
    "            columns = [f'Predicted {label}' for label in labels]\n",
    "            index = [f'Actual {label}'  for label in labels]\n",
    "            # creating and printing confusion matrix...\n",
    "            confusion = pd.DataFrame(confusion_matrix(y_test, y_preds),\n",
    "                                     columns=columns, index=index)\n",
    "            print(model_name + ' confusion matrix:')\n",
    "            print(confusion, '\\n')\n",
    "                \n",
    "            # creating dataframe from test set to calculate profitability\n",
    "            test_with_preds = X_test\n",
    "            # adding column with higher closing price...\n",
    "            test_with_preds['higher_closing_price'\n",
    "                           ] = test_with_preds.apply(\n",
    "                get_higher_closing_price, axis=1)\n",
    "            # adding column with shifted closing prices...\n",
    "            test_with_preds = get_close_shift(test_with_preds)\n",
    "            # adding column with predictions\n",
    "            test_with_preds['pred'] = y_preds\n",
    "            # adding column with profitability of predictions\n",
    "            test_with_preds['pct_profit'] = test_with_preds.apply(\n",
    "                get_profit, axis=1).shift(-1)\n",
    "            # filtering out rows where no arbitrage is predicted\n",
    "            test_with_preds = test_with_preds[test_with_preds['pred'] != 0]\n",
    "            # calculating mean profit where arbitrage predicted...\n",
    "            pct_profit_mean = test_with_preds['pct_profit'].mean()\n",
    "            # calculating median profit where arbitrage predicted...\n",
    "            pct_profit_median = test_with_preds['pct_profit'].median()\n",
    "            print('percent profit mean:', pct_profit_mean)\n",
    "            print('percent profit median:', pct_profit_median, '\\n\\n')\n",
    "\n",
    "        # i.e., if there are less than 1000 rows on which to train...\n",
    "        else:\n",
    "            print('not enough data!'.format(model=model_name))\n",
    "\n",
    "# creating all the arbitrage dfs and models from the ohlcv data...\n",
    "create_all_arbitrage_dfs_and_models(exchanges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
